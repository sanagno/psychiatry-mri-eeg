{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputations methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to explore different imputation methods\n",
    "\n",
    "For a more theoretical perspective consider http://www.stat.columbia.edu/~gelman/arm/missing.pdf\n",
    "\n",
    "We are dealing with data probably belonging to the category missing by design (some values are too expensive to obtain)\n",
    "\n",
    "Also look at https://www.paultwin.com/wp-content/uploads/Lodder_1140873_Paper_Imputation.pdf\n",
    "\n",
    "TODO: also take into account more complicated methods that directly tackle the problem of classification such as BoostClean https://arxiv.org/pdf/1711.01299.pdf\n",
    "TODO: also try EM imputation\n",
    "\n",
    "For a more detail description check the book \"Flexible imputationn of missing data\" https://stefvanbuuren.name/fimd/\n",
    "\n",
    "\n",
    "For discriminative models it is more elaborate, since that is not possible. There are a number of approaches. Gharamani and Jordan http://mlg.eng.cam.ac.uk/zoubin/papers/nips93.pdf describe a principled approach, where missing values are treated like hidden variables, and a variant of the EM algorithm is used to estimate them. In a similar fashion, Smola et al. http://www.gatsby.ucl.ac.uk/aistats/fullpapers/234.pdf describe a variant of the SVM algorithm which explicitly tackles the problem.\n",
    "\n",
    "In out case the missingness of values is not random but can be an indicator by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define a little clearer the setting exactly. For startes we are going to deal with only the 5 most common diseases. Also for starters we are merely going to deal with classifying based on the first diagnosis given only. We are going to consider the multiclass classification problem first, but we are should also check the binary classification one to compare with established methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour_data = pd.read_csv('DataScience2019_MRI/Behavioral/cleaned/HBNFinalSummaries.csv', low_memory=False)\n",
    "\n",
    "behaviour_data = behaviour_data[behaviour_data['NoDX'].isin(['Yes', 'No'])]\n",
    "\n",
    "keep_most_common_diseases = 5\n",
    "healthy_diagnosis = 'No Diagnosis Given'\n",
    "\n",
    "# these disorders should also include the no diagnosis given option\n",
    "keep_most_common_diseases += 1\n",
    "\n",
    "\n",
    "most_common_disorders = behaviour_data['DX_01_Cat'].value_counts().keys().values[:keep_most_common_diseases]\n",
    "behaviour_data = behaviour_data[behaviour_data['DX_01_Cat'].isin(most_common_disorders)]\n",
    "behaviour_data = behaviour_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neurodevelopmental Disorders', 'No Diagnosis Given',\n",
       "       'Anxiety Disorders', 'Depressive Disorders', 'Disruptive',\n",
       "       'Trauma and Stressor Related Disorders'], dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.zeros((len(most_common_disorders), behaviour_data.shape[0]), dtype=np.int32)\n",
    "\n",
    "category_columns = ['DX_' + str(i).zfill(2) + '_Cat' for i in range(1, 11)]\n",
    "df_disorders = behaviour_data[category_columns]\n",
    "\n",
    "for i, disorder in enumerate(most_common_disorders):\n",
    "    mask = df_disorders.select_dtypes(include=[object]). \\\n",
    "            applymap(lambda x: disorder in x if pd.notnull(x) else False)\n",
    "    \n",
    "    disorder_df = df_disorders[mask.any(axis=1)]\n",
    "    \n",
    "    np.add.at(classes[i], disorder_df.index.values, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can safely remove previous columns describing diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour_data_columns = behaviour_data.columns.values.astype(np.str)\n",
    "\n",
    "columns_to_drop = behaviour_data_columns[\n",
    "    np.flatnonzero(np.core.defchararray.find(behaviour_data_columns, 'DX')!=-1)]\n",
    "\n",
    "behaviour_data = behaviour_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disorder, classification in zip(most_common_disorders, classes):\n",
    "    behaviour_data[disorder] = classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also reasonable to assume that we need to drop columns with too many Nans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.8\n",
    "\n",
    "# columns_mask = pd.isnull(behaviour_data).sum() / behaviour_data.shape[0] > threshold\n",
    "\n",
    "# print('Droping this many columns:', np.sum(columns_mask))\n",
    "\n",
    "# dropped_columns = behaviour_data.columns[columns_mask]\n",
    "\n",
    "# behaviour_data = behaviour_data.drop(columns=dropped_columns)\n",
    "# behaviour_data = behaviour_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (1760, 312)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anonymized.ID</th>\n",
       "      <th>EID</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Study.Site</th>\n",
       "      <th>ACE_Score</th>\n",
       "      <th>APQ_P_OPD</th>\n",
       "      <th>APQ_P_Total</th>\n",
       "      <th>APQ_SR_OPD</th>\n",
       "      <th>APQ_SR_Total</th>\n",
       "      <th>...</th>\n",
       "      <th>YSR_SC</th>\n",
       "      <th>YSR_Ext</th>\n",
       "      <th>YSR_Int</th>\n",
       "      <th>YSR_Total</th>\n",
       "      <th>Neurodevelopmental Disorders</th>\n",
       "      <th>No Diagnosis Given</th>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <th>Disruptive</th>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A00078864</td>\n",
       "      <td>NDARYM832PX3</td>\n",
       "      <td>1</td>\n",
       "      <td>7.048254</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A00078865</td>\n",
       "      <td>NDARNJ687DMC</td>\n",
       "      <td>1</td>\n",
       "      <td>6.348163</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A00078866</td>\n",
       "      <td>NDARRM363BXZ</td>\n",
       "      <td>0</td>\n",
       "      <td>10.052589</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A00078867</td>\n",
       "      <td>NDARUW586LLL</td>\n",
       "      <td>1</td>\n",
       "      <td>12.319415</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A00078868</td>\n",
       "      <td>NDARDC298NW4</td>\n",
       "      <td>0</td>\n",
       "      <td>13.901437</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Anonymized.ID           EID  Sex        Age  Study.Site  ACE_Score  \\\n",
       "0     A00078864  NDARYM832PX3    1   7.048254           1        NaN   \n",
       "1     A00078865  NDARNJ687DMC    1   6.348163           1        NaN   \n",
       "2     A00078866  NDARRM363BXZ    0  10.052589           1        NaN   \n",
       "3     A00078867  NDARUW586LLL    1  12.319415           1        NaN   \n",
       "4     A00078868  NDARDC298NW4    0  13.901437           1        NaN   \n",
       "\n",
       "   APQ_P_OPD  APQ_P_Total  APQ_SR_OPD  APQ_SR_Total  ...  YSR_SC  YSR_Ext  \\\n",
       "0        NaN          NaN         NaN           NaN  ...     NaN      NaN   \n",
       "1        NaN          NaN         NaN           NaN  ...     NaN      NaN   \n",
       "2        NaN          NaN        17.0         118.0  ...     NaN      NaN   \n",
       "3        NaN          NaN         NaN           NaN  ...     9.0     16.0   \n",
       "4        NaN          NaN        33.0         154.0  ...    11.0     10.0   \n",
       "\n",
       "   YSR_Int  YSR_Total  Neurodevelopmental Disorders  No Diagnosis Given  \\\n",
       "0      NaN        NaN                             1                   0   \n",
       "1      NaN        NaN                             0                   0   \n",
       "2      NaN        NaN                             1                   0   \n",
       "3     29.0       85.0                             0                   0   \n",
       "4     26.0       70.0                             1                   0   \n",
       "\n",
       "   Anxiety Disorders  Depressive Disorders  Disruptive  \\\n",
       "0                  1                     0           0   \n",
       "1                  0                     1           0   \n",
       "2                  0                     0           0   \n",
       "3                  0                     1           0   \n",
       "4                  0                     0           0   \n",
       "\n",
       "   Trauma and Stressor Related Disorders  \n",
       "0                                      0  \n",
       "1                                      0  \n",
       "2                                      0  \n",
       "3                                      0  \n",
       "4                                      0  \n",
       "\n",
       "[5 rows x 312 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape', behaviour_data.shape)\n",
    "behaviour_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explore some different imputation methods. More specifically:\n",
    "1. Do nothing\n",
    "2. Fill all missing values with a dummy value\n",
    "3. Imputation Using (Mean/Median/Most Frequent) Values\n",
    "4. Imputation Using k-NN, Randomforest\n",
    "5. MICE (TODO)\n",
    "6. Multiple imputer\n",
    "7. Add features based on whether the value exists or not\n",
    "\n",
    "For a comparison with a more naive baseline check out @gvasilako 's notebook for both multilabel and per class metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xgboost as xgb\n",
    "from missingpy import KNNImputer, MissForest\n",
    "import impyute as impy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import UndefinedMetricWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdx and mdx may contain 'No Diagnosis'\n",
    "# drop them for now but they may be important\n",
    "# they correspond to father's and mother's primary diagnosis\n",
    "columns_to_drop = ['Anonymized.ID', 'EID', 'mdx', 'fdx', 'fcodxm_1', 'fcodxm_2', 'fcodxm_3', 'mcodxm_1',\n",
    "                   'mcodxm_2', 'mcodxm_3', 'mcodxmdt', 'TOWRE_Total_Desc', 'Picture_Vocab_Raw',\n",
    "                   'sib1dx', 'sib1codxm_1', 'sib1codxm_2', 'sib1codxm_3',\n",
    "                   'sib2dx', 'sib2codxm_1', 'sib2codxm_2', 'sib2codxm_3',\n",
    "                   'sib3dx', 'sib3codxm_1', 'sib3codxm_2', 'sib3codxm_3',\n",
    "                   'sib4dx', 'sib4codxm_1', 'sib4codxm_2', 'sib4codxm_3',\n",
    "                   'sib5dx', 'sib5codxm_1', 'sib5codxm_2', 'sib5codxm_3']\n",
    "\n",
    "processed = behaviour_data.drop(columns=columns_to_drop)\n",
    "most_common_disorders = list(most_common_disorders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_classification(dataset, diseases_to_run_for, clf, imputer, imputer_requires_disorder, \n",
    "                              drop_missing_threshold, *args):\n",
    "    for check_disorder in most_common_disorders[:diseases_to_run_for]:\n",
    "        \n",
    "        if check_disorder == no_diagnosis_given:\n",
    "            continue\n",
    "            \n",
    "        # only include patients that have that particular disease vs patients that are healthy\n",
    "        temp = dataset[(dataset[check_disorder] == 1) | (dataset[healthy_diagnosis] == 1)]\n",
    "        \n",
    "        pos = most_common_disorders.index(check_disorder)\n",
    "\n",
    "        columns_to_drop = most_common_disorders[:pos] + most_common_disorders[(pos + 1):]\n",
    "        temp = temp.drop(columns=columns_to_drop)\n",
    "        \n",
    "        if drop_missing_threshold is not None:\n",
    "            # drop features missing in drop_missing_threshold percent of the time or more\n",
    "            # The missingpy algorithms to work require that not columsn have more than 80% missing values\n",
    "\n",
    "            columns_mask = pd.isnull(temp).sum() / temp.shape[0] > threshold\n",
    "\n",
    "            print('Droping this many columns:', np.sum(columns_mask))\n",
    "\n",
    "            dropped_columns = temp.columns[columns_mask]\n",
    "\n",
    "            temp = temp.drop(columns=dropped_columns)\n",
    "#             processed_80 = processed_80.reset_index(drop=True)\n",
    "\n",
    "        train, test = train_test_split(temp, test_size=0.3, random_state=17)\n",
    "        \n",
    "        if imputer_requires_disorder:\n",
    "            train, test = imputer(train, test, check_disorder, *args)\n",
    "        else:\n",
    "            train, test = imputer(train, test, *args)\n",
    "\n",
    "        clf.fit(train.drop(columns=[check_disorder]), train[check_disorder])\n",
    "        preds = clf.predict(test.drop(columns=[check_disorder]))\n",
    "        y_test = test[check_disorder]\n",
    "\n",
    "        print('================================= {0} ================================='.format(check_disorder))\n",
    "\n",
    "        precision, recall, _, _ = precision_recall_fscore_support(y_test, preds)\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "        print('accuracy {:.3f} precision {:.3f} {:.3f} recall {:.3f} {:.3f}' \\\n",
    "              .format(accuracy, precision[0], precision[1], recall[0], recall[1]))\n",
    "\n",
    "        print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually droping columns that cannot be incorporated for the next analysis as they are categorical values, we should check their correlation with the predicted classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "disorder_corr = np.zeros((len(columns_to_drop) - 2, len(most_common_disorders)))\n",
    "\n",
    "# the disorder to find the correlation for\n",
    "for index, disorder in enumerate(most_common_disorders):\n",
    "    dropped_columns_dataset = behaviour_data[columns_to_drop + [most_common_disorders[index]]]\n",
    "\n",
    "    # remove the anonymized id and the EID\n",
    "    for col in dropped_columns_dataset[2:-1]:\n",
    "        dropped_columns_dataset[col] = dropped_columns_dataset[col].astype('category').cat.codes\n",
    "\n",
    "    disorder_corr[:, index] = dropped_columns_dataset[dropped_columns_dataset.columns[2:]].corr()[most_common_disorders[index]][:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6cAAAGoCAYAAAC61ZOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdebwkVX338c+XYVhkF9CAoiggCC6IihJBcYlrFOODAWIexQ3NI+4YTWIQt0SNCzG4BDdcEWI0EjUiyiISBUbZN0FEQBERZF9n7u/5o87Vpu27zL090z1zP29e9ZrqqlO/c6q6btO/PqeqUlVIkiRJkjRKa4y6AZIkSZIkmZxKkiRJkkbO5FSSJEmSNHImp5IkSZKkkTM5lSRJkiSNnMmpJEmSJGnkTE4laUwluSzJU+a47R5JLloBbdo6SSVZc9ixx1Xb323b/MeT/ONKqHP/JD9Y0fUMW++xmkXZQ5J8oc3fL8nNSRat2BZO2545n9ur699FkvOS7DnPGL9/n6dY/zdJrm7v/6bzqUvSqs/kVJKmkOSvkixpX5quSvI/SXYfdbsG6U8Kqurkqtp+lG3qtzp8ga+qV1bVO0fdjtVNVV1eVetX1bJRt2WczOcHqmGoqp2q6sQVFT/JYuCDwFPb+3/tPGKt8p8vkkxOJWmgJG8ADgX+Cbg3cD/go8Bec4j1R1+W/AKlScPuLRxl7+Oqwr+/6a3E43NvYB3gvJVU35TS8XuxNGL+EUpSnyQbAe8AXlVVX62qW6rqrqr676p6UyuzdpJDk/yqTYcmWbut2zPJlUnenOTXwGcGLWtl/zzJmUmuT/K/SR42RZt2TfLDVu6qJIclWaut+34rdlbr5d1nsr6e7R+c5MS2/XlJntOz7ogkH0nyzSQ3JTk1yTYzHKaXtP2+KslBPbHWSPKWJD9Lcm2So5Pcs62ebOf1rZ27JflFkke2bV/Qej52aq9fmuS/ZhGXJI9tx+/6JGf1DkVs+/3OJKe0/ftOks2m2rEkb2r79askL+lbd0SSd7X5zZJ8o9V5XZKTJ7/czuJ4fyzJt5LcAjwxyaZJjklyY5LTgG366t0hyXGtnouS/OUM8Z6Z5Py2v7/sfY/64m6T5Ph2TH+b5ItJNu5Zf1mSg5KcneSGJEclWWc2x2pAXQ9IclJr03HAZj3r7tbrlW5Y86Wt7M+TvKCn7MuTXNDWnZ9kl7Z88vyYXP4XPdvs397/DyW5FjgkyaIk72/7fSnwrL72bpTkU23/fpnkXWmJ/0zbDtj3rZJ8Nck17VgfNtPxT/J5uh/F/jvd38vftuXTnesPSPL9dgy+m+7v+gs965/Tzsfr2/n54L73+s1JzgZuSbJmenpu2z7/fc8x/nGSrdq6f01yRTt/f5xkj+mOR9vmQcDkpQfXJzm+LZ/uXH9WkjNaPVckOaQn5KDPl7sNKR5wnp2Y5N1JTgFuBR44w/u+bTuHb2jv11Ez7aek5VRVTk5OTk49E/B0YCmw5jRl3gH8CLgXsDnwv8A727o92/bvBdYG1p1i2SOA3wCPARYBLwIuA9ZucS4DntLmHwk8FlgT2Bq4AHhdT3sK2Lbn9Z7AlW1+MXAJ8PfAWsCTgJuA7dv6I4BrgV1b/C8CX55iv7dudR0JrAc8FLimp52vbcflvm0//x04sm/bNXvifQ54Y5s/HPgZ8Dc9614/i7j3ae1/Jt2Prn/WXm/e1p/Y4j6oHfcTgfdM895fDTyk7d+Xeo9tO1bvavP/DHy8Hd/FwB5AZnm8bwAe19q7DvBl4OhW50OAXwI/aOXXA64AXtzen0cAvwV2nCbeVcAebf0mwC5T7O+27XitTXcefx84tGf9ZcBpwJbAPenOu1fO5lgNqOuHdEM41wYe347JF/rPjRbrxp7jtQWwU5t/fjs2j27Helvg/j3rtmzHYB/gFmCLtm5/ur+/V7c61gVeCVwIbNX27QR6zk/ga3Tn2Xp0f+enAa9o66bdtm+/FwFnAR9qsdYBdl+O4/+Untcznes/BN5Pd97t3o7j5DF+UDsmf0Z3jv4t3Xm6Vk9dZ7Z9WnfAZ9CbgHOA7duxfziwaVv318Cm7di+Efg1sE5bd8hkG6b5PJk85jOd63vSfeasATyM7vx77jSfL3ere0B9JwKXAzu1+hbP8L4fCfwDf/g7233U/79yclrdppE3wMnJyWncJuAFwK9nKPMz4Jk9r58GXNbm9wTunPxyNs2yj9ES2p5lFwFPaPO//2I4oP7XAV/reT1dcrpH+7K4Rs/6I4FD2vwRwCd71j0TuHCKeie/3O3Qs+x9wKfa/AXAk3vWbQHcxR+S6v4vjy8FjunZ9mW0xBj4BS2pmiHum4HP97XzWOBFbf5E4K096/4f8O0p9u/T9CSudF/op0pO3wF8nb5kbJbH+3M96xa1fek9pv/EH5LTfYCT++r4d+Btg+K1ZZcDrwA2XM5z/7nAGT2vLwP+uu+9/vhsjlVf3PvRJYfr9Sz7ElMnp9cD/4eWJPW9r6+d5b6cCezV5vcHLu9bfzwt0W6vn9rThnsDd/TWD+wHnDDTtgPasRvdDzhT/tg1w/HvTU6nPNd7jvE9etZ9oecY/yNwdM+6NegS/T176npJX+zf10/32bTXLI/974CHt/lDmH1yOu25PmD7Q4EPDYo1qO4B9Z0IvKNn/Uzv++fofkS77/L8XTk5Oc1+clivJP2xa4HNMv11V1vSJU+TftGWTbqmqm7v26Z/2f2BN7YhdtcnuZ6u12LLvu1I8qB0Q0h/neRGuuRlyqGpA9p6RVVN9LX3Pj2vf90zfyuw/gwxr+iLNdnm+wNf69mfC4BldF/6BjkJ2CPJFnRJ2tHA45JsDWxEl2DMFPf+wPP7juPudAns8u7flgP2bSr/Qtfz9J10Q1Df0htjhuPdW8fmdAnRVPXeH3hM3/69APiTKeJBl9g9E/hFG4a426AdSHLvJF9uwxdvpEtm+s+rqY7d8hyrLYHfVdUtM5VvZfah6528Kt1w8x3a6q3ofhgatC8vzB+GyF9P16Pbuy/9x2i69t+frhftqp54/07XkzbTtv22An5RVUsHtHk2x7/XdOf6lsB1VXVrT/neNt7tM6udn1cw9Xk5aD+mOvYHpRtqfUNr00Yz7MdUpj3XkzwmyQnphkffQHeOzKWeXr37PNP7/rd0vcanteHR0w5ll7T8TE4l6Y/9kO7X8+dOU+ZXdF9kJt2vLZtUA7bpX3YF8O6q2rhnukdVHTlg24/RDSPcrqo2pBsymhn2o7etW+XuN/u4H12vyVxt1Rdrct+vAJ7Rt0/rVNUvGXBMquoSuoTn1cD3q+pGumToALqew4lZxL2Crjepd916VfWeOezXVQP2baCquqmq3lhVDwSeA7whyZOZ3fHuPRbX0PV4TVXvFcBJffu3flX9zRTxqKrTq2ovui/V/0WX9A/yT23bh7bz6q+Z/Xk162PVym6SZL3ZlK+qY6vqz+iSrguBT7RVV9B3PS5Akvu3MgfSDTXdGDiXu+9L//k3XfuvoPsM2KznmG9YVTvNYtt+VwD3m+LHrpmO/6DPjKnO9auAeya5R0/53jbe7TMrSdr6qc7LQfsx6NjvQZe0/SWwSTv2NzD786i/junO9S8BxwBbVdVGdMPqJ+sZ1PZbgN7j8ScDyvRuN+37XlW/rqqXV9WWdCMTPppZPjpJ0uyYnEpSn6q6ATgY+EiS5ya5R5LFSZ6R5H2t2JHAW5Nsnu7mOgfT9Xosj08Ar2y9AUmyXrvhxwYDym5Ad/3Yza0X6W/61l8NPHCKek6lSwD/tu3HnsCz6a5znKt/bMdlJ7rrwyZvDPJx4N0tWaAdn8k7HF8DTAxo50l0ScVJ7fWJfa9nivsF4NlJnpbupi3rpLsh1H3nsF9HA/sn2bF9yX/bVAXT3cxq2/Yl/wa6ntwJlvN4V/f4lK/S3aTnHkl2pBumOekbwIOS/N8Wb3GSR6fnZjZ97Vor3c2lNqqqu+jOm4lBZenOq5uBG5Lch+66wtma9bGqql8AS4C3t/btTndMBrX/3kn2aonsHa19k+3/JHBQkke2v5lt2zmxHl2ScU2L8WK6ntOZ2v+aJPdNsgkw2fNNVV0FfAf4QJIN092Qa5skT5hp2wFOo0sc39P+xtdJ8ri2bqbj3/93PeW53nOMD2nHeDfufoyPBp6V5MnpHuHyRrrj+78zHKdJnwTemWS7duwflu65pBvQ/bhyDbBmkoOBDWcZs99M5/oGdL3DtyfZFfirnm0Hfb6cCTw+3XN0NwL+brrKZ3rfkzy/53Pld3Tn3FR/W5LmwORUkgaoqg8AbwDeSvel5wq6hOm/WpF30X0RPJvuJiE/acuWp44lwMuBw+i+6FxCd23cIAfRfRG7iS6p7b9L5CHAZ9tQtL/sXVFVd9J9SX0G3c1FPgq8sKouXJ729jmptfd7wPur6jtt+b/S9Wx8J8lNdDcxekxrx63Au4FTWjsf2xNrA/5wt83+1zPFvYLuET9/zx/eqzcxh//HVdX/0F3Hdnzbv+OnKb4d8F265OKHwEer6oQ5Hu8D6YbL/pruGtLP9LTpJrprGvel6/36NX+4sdZU/i9wWbqhoq+kGxo5yNuBXeiS62/SJcmzspzHCrrz9zHAdXSJ7OemKLcG3d/er1rZJ9B+jKmq/6A7h75E97fwX8A9q+p84AN078PVdDfNOWWG9nyC7nrNs+j+fvv3/YV0NxY6n+7v8yv8Yaj4TNv+Xvvx4dl0Nz+6HLiSbtgyzHz8/5nuR7Drkxw0i3P9BXTXuF5L93l0FF0CSlVdRNcz+2905+WzgWe383U2PkiX4H6H7gePT9HdWOpY4NvAT+mGDd/O9MODpzSLc/3/Ae9onwEH0zMiYNDnS1UdR3cMzgZ+TJf8zmS69/3RwKlJbqb7PHptVV06l32VNFiqphvBIUmSpFVRukedXFhVU/ZqS9I4sedUkiRpNdCGwG7ThqM+na6X9b9m2k6SxsV0d6KUJEnSquNP6IYGb0o3fPhvquqM0TZJkmbPYb2SJEmSpJFzWK8kSZIkaeQc1isNyY2veNpQhyHU7X/0vPZ5+eiJW8xcaDncnOHePX9F3It/n7p5qPHOXjrXpyMMdtdcngI4jX1ftWio8dZ6+Yq5h8rizaZ64s3yu+0bHxxaLIBfveXYoca77dbFQ423yb1vHWo8gA0fvtZQ4/34a4OehDR36y++a6jxbrhrupscL78t1h/u5wzA725Zd6jxfstw3+Nhj7lbmuF+GK5dw/0/yrI5PbJ15dp+g+uHGu+E2+851Hj3v3PZUOOtCM+6+sixfaPv+u2lc/6zW7zZA6fdr3Z9+r8Ci4BP9j8jPMnj6e7M/jBg36r6Slu+M90z2Deke5Tau6uq/0kCy82eU0mSJElaYJIsAj5C9+izHYH92rO2e11O95i7L/Utv5XuMWk7AU8HDk2y8XzbZM+pJEmSJI2riRXW87wrcMnk83qTfJnuLt/nTxaoqsvaursNSaiqn/bM/yrJb4DNgXl149tzKkmSJEnjqibmPCU5IMmSnumAnsj3Aa7oeX1lW7ZckuwKrAX8bH47as+pJEmSJI2viblfR11VhwOHD68xd5dkC+DzwIuq5n/Bt8mpJEmSJI2pIeR8U/klsFXP6/u2ZbOSZEPgm8A/VNWPhtEgh/VKkiRJ0sJzOrBdkgckWQvYFzhmNhu28l8DPjd5B99hMDmVJEmSpHE1MTH3aRpVtRQ4EDgWuAA4uqrOS/KOJM8BSPLoJFcCzwf+Pcl5bfO/BB4P7J/kzDbtPN9ddVivJEmSJI2rFTesl6r6FvCtvmUH98yfTjfct3+7LwBfGHZ7TE4lSZIkaVytuEfJjB2TU0mSJEkaVyuw53TcmJxKkiRJ0riax6NkVjUmp5IkSZI0plbgo2TGjsmpJEmSJI2rBdRz6qNkJEmSJEkjZ8+pJEmSJI0rh/VKkiRJkkbOR8lIkiRJkkbOnlNJkiRJ0sgtoBsimZxKkiRJ0riy51SSJEmSNHILqOd0tXiUTJJK8oGe1wclOWQlt2H/JIeNe8z5SrJ1kr+aZblzp1h+W5IzklyQ5LQk+/esf06Stwy52SQ5Isnew44rSZIkrUhVy+Y8rWpWi+QUuAN4XpLNhhk0ndXlGA3L1sCMyekMflZVj6iqBwP7Aq9L8mKAqjqmqt4zn+BJ5j0iYBgxJEmSpHmriblPq5jVJfFaChwOvL5/RZLNk/xnktPb9Li2/JAkB/WUO7f16m2d5KIknwPOBbZKsl+Sc1qZ9/Zs8+IkP01yGvC46epMskaSy5Js3FPu4iT3nqqNffuxdZLjk5yd5HtJ7teWH5Hk40mWtLb8eVu+f5L/SnJcq/fAJG9oPZY/SnLPVm6bJN9O8uMkJyfZoSfuh5P8b5JLe3od3wPskeTMJK9v7To5yU/a9KfL88ZV1aXAG4DX9LT7sDb//HbMz0ry/bZsnSSfae/HGUme2LPdMUmOB77Xflg4rL2X3wXu1XMsH5nkpLbPxybZoi0/McmhSZYArx1UvyRJkqQVY3XqHfoIcHaS9/Ut/1fgQ1X1g5bQHQs8eIZY2wEvqqofJdkSeC/wSOB3wHeSPBc4FXh7W34DcAJwxlR1VtWDk3wd+AvgM0keA/yiqq5O8qVZtPHfgM9W1WeTvAT4MPDctm5rYFdgG+CEJNu25Q8BHgGsA1wCvLmqHpHkQ8ALgUPpkvpXVtXFrU0fBZ7Utt8C2B3YATgG+ArwFuCgqppMgu8B/FlV3Z5kO+BI4FEzHN9+P2l19DsYeFpV/bInqX8VUFX10JZIfyfJg9q6XYCHVdV1SZ4HbA/sCNwbOB/4dJLF7VjuVVXXJNkHeDfwkhZjrap6VNu3cwbUL0mSJK08C+ia09UmOa2qG1tv52uA23pWPQXYMcnk6w2TrD9DuF9U1Y/a/KOBE6vqGoAkXwQe39b1Lj8KmEySpqrzKLqE6zN0w1mPWo427gY8r81/HuhNwo+uqgng4iSX8odE74Squgm4KckNwH+35ecAD2t1/CnwHz11r90T979a3POT3HvwoWIxcFiSnYFlPcdgeWSK5acARyQ5GvhqW7Y7XXJJVV2Y5Bc9dR5XVde1+ccDR1Y32P5XrUcVuoT1IcBxbZ8XAVf11HlUz/yg+u/e8OQA4ACAQ/fYkRc/+L6z2F1JkiRpllbB4blztdokp82hdL1wn+lZtgbw2Kq6vbdgkqXcfVjzOj3zt8yzHVPV+UNg2ySb0/V6vmuG8rOtr6Z4fUfPsome1xN07/0awPVVtfMUcXu3n6oxrweuBh7e4t0+RbnpPAK4oH9hVb2y9eY+C/hxkkfOEGc271uA86pqt5liDKq/qq7ta+PhdL3P3PiKp/W/D5IkSdL8TKx6Nzaaq9XlmlMAWq/Z0cBLexZ/B3j15IvWwwdwGd0wUJLsAjxgirCnAU9IslmSRcB+wEl0w3qfkGTTNlT0+TPVWVUFfA34IHBBT6IzVRt7/S9dbyvAC4CTe9Y9P901rdsADwQummJf7qaqbgR+nuT5rd4kefgMm90EbNDzeiPgqtbD+n/peiJnLcnWwPtpvaF967apqlOr6mDgGmAruv1+QVv/IOB+DN7f7wP7JFnUril9Ylt+EbB5kt1ajMVJdpqibYPqlyRJklaeBXRDpNWt5xTgA8CBPa9fA3wkydl0+/t94JXAfwIvTHIeXaL500HBquqqdI82OYGu1+2bVfV16G6qBPwQuB44cxZ1Qjds9HRg/1mWn/RqumtV30SXKL24Z93ldEn0hnTXj96+HL2uLwA+luStdEN0vwycNU35s4FlSc4CjqC7RvU/k7wQ+Daz673cJskZdL3VNwEfrqojBpT7l3Yda4DvtXZd2Np7Dt2NsPavqjsG7O/X6K6dPZ/u+PwQoKruTHdzpw8n2YjueB8KnDfL+iVJkqSVZwFdc5quM0+rqiRHAN+oqq+Mui0L3bCH9dbtS4cZjo+euMVQ492c4X5QroiP3X3q5qHGO3vphkONd9esf0OanX1ftVwDF2a01svfNtR4kxZv9sChxbrtGx8cWiyAX73l2KHGu+3WxUONt8m9bx1qPIANH77WUOP9+GsbzFxoOay/+K6hxrvhrrVnLrQctlh/uJ8zAL+7Zd2hxvstw32Ph/3Ncensf1CflbWH3Fu0bMorm8bH9htcP9R4J9x+z6HGu/+d4z8s9VlXHzm2b/TtPzxyzn926+y239ju1yCrY8+pJEmSJK0eFlDPqcnpKq6q9h91GyRJkiRpvkxOJUmSJGlc2XMqSZIkSRq1qvG/ZndYTE4lSZIkaVzZcypJkiRJGrlV8Hmlc2VyKkmSJEnjyp5TSZIkSdLI2XMqSZIkSRo5e04lSZIkSSO3gHpO1xh1AyRJkiRJsudUkiRJksaVw3olSZIkSSNncipJkiRJGrkFdM2pyakkSZIkjSt7TiVJkiRJI2fPqSRJkiRp5Ow5lSRJkiSNnD2nkpbX647dYKjx7sGiocZ73XrXDDXeJtvcPtR4y24dajgAfnLuFkON98xdrhhqvDXWGWo4HnPoHUONd+Hb9hhqvElL7/zl0GJtue9HhxYL4M2bPnao8RbXUMOx6eUbDzcgcP5Vy4Ya7ykMd6cvX7p4qPHWyFDD8Zvbhv+ebFLDfU82y51DjTfsr8n32uiWocabWDbcN/n2O4Z7Dq4Id9w53K/0f/X4Xw013nVnm3JodjxTJEmSJGlcOaxXkiRJkjRyJqeSJEmSpJGrIV8jMsZMTiVJkiRpXC2gntM1Rt0ASZIkSdIUJibmPs0gydOTXJTkkiRvGbB+7SRHtfWnJtm6LV+c5LNJzklyQZK/G8aumpxKkiRJ0riqiblP00iyCPgI8AxgR2C/JDv2FXsp8Luq2hb4EPDetvz5wNpV9VDgkcArJhPX+TA5lSRJkqRxteJ6TncFLqmqS6vqTuDLwF59ZfYCPtvmvwI8OUmAAtZLsiawLnAncON8d9XkVJIkSZJWQ0kOSLKkZzqgZ/V9gN6HuF/ZljGoTFUtBW4ANqVLVG8BrgIuB95fVdfNt73eEEmSJEmSxtU87tZbVYcDhw+vMb+3K7AM2BLYBDg5yXer6tL5BLXnVJIkSZLG1Yob1vtLYKue1/dtywaWaUN4NwKuBf4K+HZV3VVVvwFOAR413101OZUkSZKkcbXiktPTge2SPCDJWsC+wDF9ZY4BXtTm9waOr6qiG8r7JIAk6wGPBS6c7646rFeSJEmSxtUMd92dc9iqpUkOBI4FFgGfrqrzkrwDWFJVxwCfAj6f5BLgOroEFrq7/H4myXlAgM9U1dnzbZPJqSRJkiSNqZqY+zWnM8au+hbwrb5lB/fM30732Jj+7W4etHy+TE4lSZIkaVzNPDx3tWFyKkmSJEnjagUN6x1HJqeSJEmSNK5W4LDecePdeiVJkiRJI2fPqSRJkiSNK685lSRJkiSNnMmpJEmSJGnkauFcc2pyKkmSJEnjagH1nHpDpFVMkkrygZ7XByU5ZDm23z/JNUnOSHJxkmOT/GnP+nckecqQm71clrcNSZ6e5LQkFyY5M8lRSe43l1iSJEnSWJmouU+rGHtOVz13AM9L8s9V9ds5xjiqqg4ESPJE4KtJnlhVF1TVwUNr6RwtTxuSPAT4N+A5VXVBW/YcYGvg8nHYH0mSJGnOFtBzTu05XfUsBQ4HXt+/IsnWSY5PcnaS7032Hk6nqk5o8Q5oMY5IsnebPzjJ6UnOTXJ4krTlj251nJnkX5Kc25bvn+SrSb7demXf19O2/ZKc02K9ty1b1Oo7t617/YA2vCfJ+a2+9w/YhTcD/zSZmLZ9Oqaqvt8bq/Wu/kdPe/ZM8o02/9QkP0zykyT/kWT9tvyyJG9vy89JssNMx1OSJEkaqgXUc2pyumr6CPCCJBv1Lf834LNV9TDgi8CHZxnvJ8CgxOuwqnp0VT0EWBf487b8M8ArqmpnYFnfNjsD+wAPBfZJslWSLYH3Ak9q6x+d5Llt/j5V9ZCqemiL+3tJNgX+Atip7dO7BrRxp9b+mXwXeEyS9drrfYAvJ9kMeCvwlKraBVgCvKFnu9+25R8DDuoPmuSAJEuSLLnopktn0QxJkiRJg5icroKq6kbgc8Br+lbtBnypzX8e2H2WITPF8icmOTXJOXSJ5U5JNgY2qKoftjJf6tvme1V1Q1XdDpwP3B94NHBiVV1TVUvpEufHA5cCD0zyb0meDtzYF+sG4HbgU0meB9w67U4km7be3J8muVsi2er9NvDsJGsCzwK+DjwW2BE4JcmZwItamyd9tf37Y7qhwndTVYdX1aOq6lHbb/DA6ZonSZIkLbeamJjztKoxOV11HQq8FFhvpoKz8Ajggt4FSdYBPgrs3Xo1PwGsM4tYd/TML2Oa65qr6nfAw4ETgVcCn+xbvxTYFfgKXa/ttweEOQ/YpZW/tvXmHg6sP6Dsl4G/pEu0l1TVTXSJ+XFVtXObdqyqlw7Yn2n3RZIkSVohHNarcVdV1wFH0yWok/4X2LfNvwA4eaY4SZ5Ad73pJ/pWTSaiv23XYO7d6r0euCnJY9r6fZnZacATkmyWZBGwH3BSG1K7RlX9J93Q2l362rY+sFFVfYvuGtuHD4j9PuAfkjy4Z9k9pmjHSa2Ol9MlqgA/Ah6XZNtW53pJHjSLfZIkSZJWvJqY+7SKsSdo1fYB4MCe168GPpPkTcA1wIun2G6fJLvTJXE/B/5P7w2FoEtCk3wCOBf4NXB6z+qXAp9IMkGX8N0wXSOr6qokbwFOoOup/GZVfT3Jw1t7J38k+bu+TTcAvt56ccPdrwWdjH1OktcCn0uyIfBb4HLgbQPKLms3QdqfbvguVXVNkv2BI5Os3Yq+FfjpdPskSZIkrRSrYA/oXJmcrmKqav2e+avp6SWsql/QDVmdbvsjgCOmWb9/z/xb6RK1fue1GxTRks4lg2JX1Z/3zB8JHNlX11n09Zb2t4FuWO+0quqbwDenWLd/3+sDuXtCT1UdT3ddbP+2W/fMLwH2nKktkiRJ0lCtgteOzpXJqebiWUn+ju78+QVdT6QkSZKkYbPnVJpaVR0FHDXqdkiSJEmrvVXw2tG58oZIkiRJkqSRs+dUkiRJksaVw3olSZIkSaNW3hBJkiRJkpEWVYgAACAASURBVDRy9pxKkiRJkkbO5FSSJEmSNHIL6G69JqeSJEmSNK7sOZUkSZIkjVqZnEqSJEmSRm4BJadrjLoBkiRJkiTZcypJkiRJ48rnnEqSJEmSRm4BDes1OZUkSZKkcWVyKkmSJEkatSqTU0mSJEnSqNlzKkmSJEkaOZNTScvrnVtcN9R4yXA/iC742b2GGu8Dtw63fWutgCdbbbHOcD/iPnTuPYYab2ktG2q8H+w53GO49nNfNtR4K8LlL9xuqPEO/eZwz+sf5+ahxvsVNw01HsDL7tpyqPE+s85w93mtLBpqvPUYbrxn3D7ceACfXef2ocZbZ8jHcBEZarx7377JUONdyR1DjbfOGuP/5MXd7lp7qPE+ecq6Q4231pDPmRXhg6NuwDRqASWn4//XJkmSJEla7dlzKkmSJEnjagH1nJqcSpIkSdK4mhh1A1Yek1NJkiRJGlML6ZpTk1NJkiRJGlcmp5IkSZKkkXNYryRJkiRp1BzWK0mSJEkavQXUc+pzTiVJkiRpTNVEzXmaSZKnJ7koySVJ3jJg/dpJjmrrT02ydd/6+yW5OclBw9hXk1NJkiRJWmCSLAI+AjwD2BHYL8mOfcVeCvyuqrYFPgS8t2/9B4H/GVabTE4lSZIkaVxNzGOa3q7AJVV1aVXdCXwZ2KuvzF7AZ9v8V4AnJwlAkucCPwfOm/vO3Z3JqSRJkiSNqZqY+5TkgCRLeqYDekLfB7ii5/WVbRmDylTVUuAGYNMk6wNvBt4+zH31hkiSJEmSNK7mcUOkqjocOHxobfmDQ4APVdXNrSN1KExOJUmSJGlM1Yq7W+8vga16Xt+3LRtU5sokawIbAdcCjwH2TvI+YGNgIsntVXXYfBpkcipJkiRJ42rFJaenA9sleQBdErov8Fd9ZY4BXgT8ENgbOL6qCthjskCSQ4Cb55uYgsmpJEmSJI2tFdVzWlVLkxwIHAssAj5dVecleQewpKqOAT4FfD7JJcB1dAnsCmNyKkmSJEljagUO66WqvgV8q2/ZwT3ztwPPnyHGIcNqj3frlSRJkiSNnD2nkiRJkjSmVmTP6bgxOZUkSZKkcVXDe1TLuHNY72ooyXOTVJId5hnnW0k2nqHM388h7rIkZyY5L8lZSd6YZI227lFJPjzXNk9T5yFJDhp2XEmSJGlFqom5T6sak9PV037AD9q/c1ZVz6yq62cottzJKXBbVe1cVTsBfwY8A3hbq3NJVb1mDjF/rz2DaV6GEUOSJEmar5rInKdVjcnpaibJ+sDuwEvpudVzkj2TnJjkK0kuTPLFdDZKclGS7Vu5I5O8vM1flmSzNv/XSU5rPZ7/nmRRkvcA67ZlX0zyjiSv66nz3UleO117q+o3wAHAga09eyb5Rtv+CS32mUnOSLJBK/MvSc5Nck6SfXr27+QkxwDnt2X/kOSnSX4AbN/Trm2SfDvJj9s2O7TlRyT5eJJTgfcNqn+eb48kSZK0XBZSz6m9Q6ufvYBvV9VPk1yb5JFV9eO27hHATsCvgFOAx1XVD9rzjY5I8q/AJlX1id6ASR4M7NPK35Xko8ALquotSQ6sqp1bua2BrwKHtmG6+wK7ztTgqro0ySLgXn2rDgJeVVWntKT7duB5wM7Aw4HNgNOTfL+V3wV4SFX9PMkjW/07053nPwEmj8PhwCur6uIkjwE+Cjyprbsv8KdVtSzJfw+oX5IkSVppymtOtQrbD/hym/8ydx/ae1pVXVlVE8CZwNYAVXUccA7wEeBlA2I+GXgkXSJ4Znv9wP5CVXUZcG2SRwBPBc6oqmvnsS+nAB9M8hpg46paStcrfGRVLauqq4GTgEf37N/P2/wewNeq6taquhE4Bn7fs/ynwH+0ffl3YIueOv+jqpZNU//dJDkgyZIkS75w9a/msauSJEnSH7PnVKukJPek6wF8aJICFgGV5E2tyB09xZfR3v/Wy/lg4FZgE+DK/tDAZ6vq72bRjE8C+wN/Anx6lu1+YGvPb1o7AKiq9yT5JvBM4JQkT5sh1C2zqG4N4PrJ3t7pYgyqv6ou7C1cVYfT9cTyy92eVLOoX5IkSdIA9pyuXvYGPl9V96+qratqK+DndL2I03k9cAHwV8BnkizuW/89YO8k94IuCU5y/7burr7yXwOeTtebeexMDU6yOfBx4LCqqr5121TVOVX1XuB0YAfgZGCfds3r5sDjgdMGhP4+8Nwk67ZrRZ8N0HpRf57k+a2OJHn4FG0bVL8kSZK00nhDJK2q9qNLDnv9J9PctbfdCOllwBur6mS6pO6tvWWq6vy27DtJzgaO4w9DYQ8Hzk7yxVb2TuAE4Oie4bH9Jm+idB7wXeA7wNsHlHtdu/HR2cBdwP+0/TsbOAs4Hvjbqvp1/4ZV9RPgqFbuf+iSy0kvAF6a5CzgPLrrdAcZVL8kSZK00lTNfVrVOKx3NVJVTxywrPeZoSf2LD+wZ3nvUNo39Mxv3TN/FF2y1x//zcCbJ1+3IcKPBZ4/TTsXTbPuxMl2VtWrpyj2pjYN3K5n2buBdw+o4+d0vbv9y/fvez1V/ZIkSdJKsSr2gM6VyamGJsmOwDfobkR08ajbI0mSJK3qTE6lOWjDf//oLr6SJEmS5mZVHJ47VyankiRJkjSm7DmVJEmSJI1clcmpJEmSJGnEamLULVh5fJSMJEmSJGnk7DmVJEmSpDE14bBeSZIkSdKoec2pJEmSJGnkvFuvJEmSJGnkfM6pJEmSJGnk7DmVJEmSJI2cN0SSJEmSJI3cQrohks85lSRJkiSNnD2nkiRJkjSmvCGSJEmSJGnkvOZUkiRJkjRyC+maU5NTSZIkSRpTDuuVtNy2P+vnQ423w0b3HWq89w41Gtyv1hpqvD9ZOtRwAJyx+K6hxtty0T2GGm/YrvrJbUONd8iPzhpqvElH7Te8WI//6o3DCwb87Rr3HGq8nSbWG2q8i9beYKjxAL44cc1Q4z2VzYca7wYmhhrvfkuHey/Iw9e6fqjxAO6b4X7WrDXk+18O+26adzDcb96bM9z/P63D+Pda3TTkN+V+tXio8RYtoORqRXBYryRJkiRp5BzWK0mSJEkaOXtOJUmSJEkjt5BGRQ/7sgFJkiRJkpabPaeSJEmSNKYc1itJkiRJGjlviCRJkiRJGrnhPlBrvJmcSpIkSdKYqlXgWbvDYnIqSZIkSWNqYgHdrtfkVJIkSZLG1IQ9p5IkSZKkUVtIw3p9zqkkSZIkaeTsOZUkSZKkMbWQ7tZrz6kkSZIkjakic55mkuTpSS5KckmStwxYv3aSo9r6U5Ns3bPu79ryi5I8bRj7anIqSZIkSWNqYh7TdJIsAj4CPAPYEdgvyY59xV4K/K6qtgU+BLy3bbsjsC+wE/B04KMt3ryYnEqSJEnSmFpRySmwK3BJVV1aVXcCXwb26iuzF/DZNv8V4MlJ0pZ/uaruqKqfA5e0ePNicipJkiRJY2o+w3qTHJBkSc90QE/o+wBX9Ly+si1jUJmqWgrcAGw6y22XmzdEkiRJkqQxNTGPJ8lU1eHA4UNrzApmcipJkiRJY2pixT3n9JfAVj2v79uWDSpzZZI1gY2Aa2e57XJzWK8kSZIkLTynA9sleUCStehucHRMX5ljgBe1+b2B46uq2vJ92918HwBsB5w23wbZcypJkiRJY6pWVNyqpUkOBI4FFgGfrqrzkrwDWFJVxwCfAj6f5BLgOroEllbuaOB8YCnwqqpaNt82mZyOiSTLgHOAxXRv8OeAD1XVWDx3N8mWwIerau95xtkauAC4EFgHuAn4aFUd0dY/B9ixqt4zn3oG1HsE8I2q+sow40qSJEkr0opMBqrqW8C3+pYd3DN/O/D8KbZ9N/DuYbbH5HR83FZVOwMkuRfwJWBD4G3zDZxk0Xx/yaiqX9F15Q/Dz6rqEQBJHgh8NUmq6jPtF5r+4QTLJcma7W5iI40hSZIkzddEVtg1p2PHa07HUFX9BjgAODCdRUn+JcnpSc5O8gqAJHsm+X6Sbya5KMnHk6zR1t2c5ANJzgJ2S/LIJCcl+XGSY5Ns0cq9Jsn5Le6X27InJDmzTWck2SDJ1knObet/lGSnyfYmOTHJo5Ksl+TTSU5r2/U/J2nQvl4KvAF4TYu1f5LD2vzzk5yb5Kwk32/L1knymSTntDqe2LPdMUmOB77Xjtth7bh8F7hXT3unOhYnJjk0yRLgtYPqlyRJklammse0qrHndExV1aVJFtElVXsBN1TVo5OsDZyS5Dut6K7AjsAvgG8Dz6N7QO56wKlV9cYki4GTgL2q6pok+9B1wb8EeAvwgKq6I8nGLeZBdOPGT0myPnB7X/OOAv4SeFtL7LaoqiVJ/onuIumXtFinJfluVd0yw+7+BNhhwPKDgadV1S972vaq7vDUQ5PsAHwnyYPaul2Ah1XVdUmeB2zfjs296cbDf7odi3+b4lgArFVVjwJIcs6A+u+mPSvqAIC1Fm/K4jU3mGFXJUmSpNkbi2v8VhJ7TlcNTwVemORM4FS6B99u19adVlWXtmG7RwK7t+XLgP9s89sDDwGOazHeSne7Z4CzgS8m+Wu6a10BTgE+mOQ1wMYDhrcezR+G+P4lXTI82c63tDpOpLum9H6z2L+pxiqcAhyR5OV0F2nT9u8LAFV1IV1SPpmcHldV17X5xwNHVtWyNiT5+LZ8umMBXeI9Xf13U1WHV9WjqupRJqaSJEkatonMfVrV2HM6ptq1mMuA39Alb6+uqmP7yuzJH/fYT76+vec60wDnVdVuA6p6Fl0i92zgH5I8tKrek+SbwDPpemmfRk/vaetJvDbJw4B9gFf21PN/quqi5dzdR9DdJOnuO1L1yiSPaW38cZJHzhBnph7ayTZOdSzuFmNQ/VV17SzqkCRJkoZiBT7ndOzYczqGkmwOfBw4rD1H6Fjgb9qQVJI8KMl6rfiu6Z5NtAZdoviDASEvAjZPslvbfnGSndo2W1XVCcCb6R6qu36SbarqnKp6L93zjwYNuT0K+Ftgo6o6uy07Fnh10l21neQRs9jXrYH30w217V+3TVWd2u4Ydg3dg35PBl4weRzoemYHJcPfB/ZJd73uFsATpzsWU7RtUP2SJEnSSuM1pxqFddsw08lHyXwe+GBb90lga+AnLfG7BnhuW3c6cBiwLXAC8LX+wFV1Z5K9gQ8n2YjufT8U+CnwhbYsdI+KuT7JO9uNhiaA84D/AbboC/sV4F+Bd/Yse2eLe3ZLfH8O/PmAfd0myRn84VEyH558lEyff0myXWvb94Cz6B5B87F2PehSYP92vWz/tl8DnkR3renlwA9nOBbnzbJ+SZIkSSuAyemYqKqB1zS2dRPA37fp91pCdmNV/VECWFXr970+k274br/d+xdU1asHlLuM7lrNyTJX03f+VNVtwCum2I3JMpcB606z/gjgiDb/vAFFbgdePN127XUBB05Rx8BjUVV79r0eVL8kSZK00qyK147OlcmpJEmSJI2phXS3XpPTVVhVnUh3V1xJkiRJq6FV8drRuTI5lSRJkqQx5bBeSZIkSdLIOaxXkiRJkjRyJqeSJEmSpJGrBTSsd41RN0CSJEmSJHtOJUmSJGlMOaxXkiRJkjRyJqeSJEmSpJHzOaeSJEmSpJHzOaeSJEmSpJFzWK8kSZIkaeRMTiVJkiRJI7eQrjn1OaeSJEmSpJGz51SSJEmSxpQ3RJIkSZIkjZzXnEqSJEmSRm4hXXNqcioNyWGbPG6o8X64+I6hxtvt9ZsMN9666w41HrfeOtx4wL7bPXio8SZOP3Wo8bLecI/heYcO97fViTXH/7fax65zn6HGe8RavxtqvKVLh3trh922vGWo8QC2v2SLocZ78tOuGmq8Ne+/+VDjsdZw35Nn/ffwP7s2/fvHDzfgRpsNNdyibR891Hg3veLAocZbd69dhhrvzu+dOdR4K8IdVw83fbnHDusMNd5tF9821HgLzcQCSk9NTiVJkiRpTI3/T8XDY3IqSZIkSWNq4fSbmpxKkiRJ0tiy51SSJEmSNHIL6VEyw70rgCRJkiRJc2DPqSRJkiSNKe/WK0mSJEkauYWTmpqcSpIkSdLY8oZIkiRJkqSRc1ivJEmSJGnkFk5qanIqSZIkSWPLYb2SJEmSpJFbSMN6fc6pJEmSJGnk7DmVJEmSpDG1cPpNTU4lSZIkaWx5zakkSZIkaeRqAfWdmpxKkiRJ0phaSD2n3hBJkiRJksbUBDXnaT6S3DPJcUkubv9uMkW5F7UyFyd50YD1xyQ5dzZ1mpxKkiRJ0piqeUzz9Bbge1W1HfC99vpuktwTeBvwGGBX4G29SWyS5wE3z7ZCk1NJkiRJGlOj6jkF9gI+2+Y/Czx3QJmnAcdV1XVV9TvgOODpAEnWB94AvGu2FZqcaqVIsizJmUnOS3JWkjcmWaOte1SSD6/g+vdM8qc9r1+Z5IUrsk5JkiRpvibmMSU5IMmSnumA5aj63lV1VZv/NXDvAWXuA1zR8/rKtgzgncAHgFtnW6E3RNLKcltV7QyQ5F7Al4ANgbdV1RJgyWyCJFmzqpbOof496YYU/C9AVX18DjEkSZKkVUZVHQ4cPtX6JN8F/mTAqn/oi1NJZt0Vm2RnYJuqen2SrWe7nT2nWumq6jfAAcCB6eyZ5BsASZ7QeljPTHJGkg3a+pOTHAOcn2Tr3ouqkxyU5JA2f2KSf23bn5tk1/YH8Urg9W35HkkOadvtkOS0nlhbJzmnzT8yyUlJfpzk2CRbrKxjJEmSJEH3KJm5/jdj7KqnVNVDBkxfB66e/P7b/v3NgBC/BLbqeX3ftmw34FFJLgN+ADwoyYkztcfkVCNRVZcCi4B79a06CHhV62XdA7itLd8FeG1VPWgW4e/Rtv9/wKer6jLg48CHqmrnqjq5px0XAmsleUBbtA9wVJLFwL8Be1fVI4FPA+/ur6h3qMSJt1w8q32XJEmSZms+w3rn6Rhg8u67LwK+PqDMscBTk2zSboT0VODYqvpYVW1ZVVsDuwM/rao9Z6rQ5FTj5hTgg0leA2zcM4T3tKr6+SxjHAlQVd8HNkyy8Qzlj6ZLSmn/HgVsDzwEOC7JmcBb6X4JupuqOryqHlVVj9pzve1m2TxJkiRpdlZkz+kM3gP8WZKLgae015P3i/kkQFVdR3dt6eltekdbNidec6qRSPJAYBnd8IAHTy6vqvck+SbwTOCUJE9rq27p2Xwpd/9hZZ2+8P1/iTP9ZR4F/EeSr3ZNqIuTPBQ4r6p2m9UOSZIkSSvAEHpA56SqrgWePGD5EuBlPa8/TTfKcKo4l9F1+szInlOtdEk2pxtme1hVVd+6barqnKp6L92vLzsMCHE1cK8kmyZZG/jzvvX7tFi7AzdU1Q3ATcAGg9pTVT+jS5T/kS5RBbgI2DzJbi3W4iQ7Lf/eSpIkSXM3UTXnaVVjz6lWlnXb8NjFdD2fnwc+OKDc65I8ke5HovOA/6G7oPr3ququJO8ATqO74PrCvhi3Jzmj1fWStuy/ga8k2Qt49YB6jwL+BXhAq+POJHsDH06yEd3fyqGtTZIkSdJKseqlmHNncqqVoqoWTbPuRODENj8ocfz9+p5tPgxM9WzUL1TV6/rK/xR4WM+ik/vWvx94f9+yM4HHT9VuSZIkaUWbWEDpqcN6JUmSJEkjZ8+pViuzuUW1JEmStKoYwl13Vxkmp5IkSZI0pkZ1t95RMDmVJEmSpDG1kK45NTmVJEmSpDHlsF5JkiRJ0sg5rFeSJEmSNHJV9pxKkiRJkkZsIV1z6nNOJUmSJEkjZ8+pJEmSJI0przmVJEmSJI2cd+uVJEmSJI3cQrrm1ORUkiRJksaUd+uVJEmSJI2c15xKkiRJkkbOa04lSZIkSSPnNaeSJEmSpJFbSNecrjHqBkiSJEmSZM+pNCSf4qqhxpu4a7i/kp3xz8ONd0MN9+Nj7RUwZOXyNZcMNd51ixYNNd6y3DnUeM/b6NahxvvdTUMNt0KsM+TfWK+7ad2hxrvHWncNNd4FF28+1HgA92S4bTzt25sNNd7li4f7WXNXhhqO7e4c/nty0YFnDzXe4iF/vK7B8UON9/M17zPUeBufc81Q4y1iuO1bEX63xnDf5PtcMtw/lFWhN+zlo27ANBzWK0mSJEkaOW+IJEmSJEkauYkFdM2pyakkSZIkjamFk5qanEqSJEnS2PKaU0mSJEnSyJmcSpIkSZJGzuecSpIkSZK0EtlzKkmSJEljymG9kiRJkqSR8zmnkiRJkqSRW0jXnJqcSpIkSdKYclivJEmSJGnk7DmVJEmSJI2cPaeSJEmSpJHzhkiSJEmSpJGbWEDDetcYdQMkSZIkSbLnVJIkSZLGlMN6JUmSJEkjt5CG9ZqcSpIkSdKYsudUkiRJkjRy9pxKkiRJkkZuIfWcTnu33iSbJjmzTb9O8sue12utrEauaEneleR1A5Y/OMlJbX8vSPKxtnyXJE9f+S2dXpKXJbmmtffCJK+Z5TaHzlDmSUkeO4f2XJlk4ymWn5Pk3CTnJXlHkrXbuq2SHLW8dc2iLTPupyRJkjRuJqrmPK1qpk1Oq+raqtq5qnYGPg58aPJ1Vd0JkM7q+kiaw4D3tf3fEfhoW74LMDA5TbLSeqOnqOuLrb17AIck2WIIVT0JWO7kdAZ7VNVDgN2A7WnHtqquqKp95hN4GOfkynwfJUmSpKnUPP6bjyT3THJckovbv5tMUe5FrczFSV7Us3y/1iF1dpJvJ9lspjrn9AU+ybZJzk/yReA8YIskhydZ0nrCDu4p+/vesySPTfLdNv+uJEck+UGSXyR5bpIPtN60b04mB0nenuT0tvzjSTKgPXslOTXJGUm+k+RePXV8qvV+XprkVT3bHJzkp0l+AGw3xa5uAVwJUJ1zkqwLHAy8oPVQ7t3q+VySU4AjkqyZ5INJTmtvxstanfdp+3tm258/bWU/39OT+JpWdpe2T2cn+c8kG7XlP0jyoSRLgAOneo+q/n979x0nSVWvf/zz3V3YXcISDESJKoiSF0ThorKooBcFA9fERUUR8RrQy/0ZUNRrVlDBAIjAEsSIIkoQEQUDcckgSQThCqKoICIs7PP741SzvUPPbHf1qe4z08/79arXdFV3P3Xq9NSZOV1Vp3Q38LtqG4iI1SLilOozuqjTkdBO9RgRGwJvAg6syv3s8bIi4gnVL+41EXEk8JjPqkM57wX2BfaMiJWq363Lq7xNq8/+8qoeNqiW/09VV1dHxNurZZ1+J99UfcYX0da5nqD8Yz/Hjus3MzMzMxsB7wXOkfQU4JxqfgkRsSpwMPBMYFvg4IhYperLfRF4nqTNgCuZoO/S0s/RpY1JR1I3kXQH8F5Jc4HNgedHxCZdZKwPPBd4GfAN4MzqaNoiFh+Z/KKkbYBNgZXofMTyPGA7SVsCpwDvaXvuqcDzSZ2Tj0bE9IjYFnh5VdYXkyqyk0OB8yLi9Ih4V0SsJOkB4KNURyglfbetPuZJeh2ps/UnSdsC2wBvi4h1gNcBp1VHNjcnfUhbA4+XtGm17cdXeScC764+zOuBD7aVa7qkuZLGPU01ItYDpgNXV4sOIx0FngvsCRzdTT1Kurl67Wer7f31BFkfAc6V9HTgdGDN8crXTtLfgVuBJ495an/gc1V9bQP8X0Q8E3htNf8sYP+I2LR6/aO/k6SO8Qer1+wAPKMtd6K6aP8cH7P+sWWPiH2rTu4ld95/Rzeba2ZmZmbWNWlR7alPLwXmV4/nA7t3eM0LgbMl3SPpr8DZpP5aVNPy1cHFOXT4X3qsfk5dvFnSJW3zr46IfarMNUmnwV67lIzTJT0cEVcBSDq7Wn4VsF71eF5EHAjMAh4PXAqcMSZnHeDbEbE6MBO4oe25H1WnIP8pIu4BngDsCHyv6mg+EBGndSqcpKMj4gxSpe8B7BsRW4yzLadK+lf1+AXA0yLiVdX8SqSjsxcDR0bELOAHkq6IiJuAjSLiMODHwE8i4nHALEm/qt4/HzihbV0TXZP52oiYR+pkvaV1+jWwc7We1utWqY4Ct5uoHtuNl7Uj8CIASadGxH0TlHOsTkdZfw0cFBHrAqdIuikidmDxZ0dE/IB0CvNPWPJ3cjvSNz1/qV737Wr7Jio/LPk5Pmb9Ywso6SjgKIB/W2ve5Dux38zMzMyKtqiP03MjYl/SgbOWo6r/X7uxmqQ/Vo/vBFbr8Jq1gD+0zd8OrCVpYUS8ldSvux+4EXhbh/cvoZ8jp/e3HkTEU4B3AjtVR/rOJHUmAR5uW88slvRg9XMR8FDb8kXAjIhYjnTd5x5V7jEdMgC+TDpitinpaFf7ax5se/wIPXbIJd0h6RhJu1Xb8bRxXnp/2+MA9m+7Pnd9SedI+hnpSPEfgeMj4rVV52kz4HzSB3ZkF8W6f4LnTqrqYQfgc1Gd4lyVadu2Mq3V6uC1mage23WT1bXqlOUnkX5pHyXpBNKXAg8CZ0bEjkuJmqhellgl45f/0Ywa6zczMzMzy0pSP9NR1RmXrWmJjmlE/LTtcrn26aVjyiDovpccEcsAbwW2JB24vBJ439Lel2sgoznAfcC9kQbgeWHbc78nnboK6VTaXswmdVT/HBErTvD+lYA7qkPGe4/zmnbnAXtExKyImAP8e6cXRcQusfja1zWBVUiHo+8DVpwg/yzS6aat924UEbOrI3B3Vr8UxwJbRsQTgJD0HdK1rFtVHdYHIuLZVd5ewC+62K5HSboQOBl4e7Xop7R9WzHOEeDx6nHs9o6XdR7wmmrZbkxcR633rgh8FfhOdf1p+3MbSLpJ0heBH7G4E79HVZ8rkE43OL9D9AXATpEu5F4WeEUX5R9btk7rNzMzMzMbmEWo9rQ0knaW9IwO06nAXVXfjurnnzpE3EE6yNSydrVsiyr/5qpj+23g2Y99+5JydU4XkE7h/S3pmslftT33YeArEXExSx4dXaqqkza/yj4DuHCcl34Y+D7ptNm7usi9qHr9laRTaS8a56W7AtdExBWkayjfpTTQN7/OvgAAIABJREFU0M+AzSMNHPSKDu87knQU8PKIuJrU+ZoBzAOuiIjLSNfZHk76MM+rBgE6Fnh/lbEX8PmIuJJ0ivTHlrZdHXwKeFNELE/qjG1fDexzLfDmDq//MJ3r8VTSgEWXVR3m8bIOBnautvnfmfi88vOr07kvAG4mHakd6zWRBle6nHTt8InVZ3dyVcYLgK9KumrsGyXdTqqzC0id1/ZTzLupi47rn2B7zMzMzMyy6+fIaZ9+yOIDVnuT+gRjnQW8oBoEaRXS5Y1nkTqom1QH4iCNAXTd0lYYGQptZuS/5rSf6ws6+cwjq2bN+7vy3m1nZgM3mL5txjJZ8+6ZnjWOR5Y6nnVvXrb83Vnz9uvlqvEe/PQPZ2XLOmC9Vy39RT145YMLs+Ytt2zevHv+Nd7VFvVNy7zvLcz2vXdy2zJ59+OFmfe7pzyU9zMGuH7ZvNu8TObmNff9A2+Z0fegLUtYeVHeEmZu+hvx12l5P+S1Hs67o0yGe06++fYTM7cO+ayx8ia1P+A//u3a2ttVjYPTGrflVmBPSfdExFxgP0mtO5K8kcUH2D4u6dhq+X6kSz8XVu9/fWs8mPH4Xo5mZmZmZmaF6vd+pbXXmzqS8zosv4R0q8nW/DGksYHGvu4I4Ihe1jkZvsgwMzMzMzOzKc5HTs3MzMzMzAo1SpdhunNqZmZmZmZWqNzjkJTMnVMzMzMzM7NC+cipmZmZmZmZDd0id07NzMzMzMxs2Hzk1MzMzMzMzIbO15yamZmZmZnZ0PnIqZmZmZmZmQ3dKF1zOm3YBTAzMzMzMzPzkVMzMzMzM7NCydecmpmZmZmZ2bCN0mm97pyamZmZmZkVygMimZmZmZmZ2dD5tF4zMzMzMzMbOh85NTMzMzMzs6Fz59TMzMzMzMyGbnS6pqSeuCdPngY3AfuWnDcZyjhqeZOhjKOWNxnKOGp5k6GMzisv03nlZZae56nZaVqODq6Z9WTfwvOayHReeZnOKy/TeeVlOq+svCYynVdeZul51iB3Ts3MzMzMzGzo3Dk1MzMzMzOzoXPn1Gzwjio8r4lM55WX6bzyMp1XXqbzysprItN55WWWnmcNiupCYTMzMzMzM7Oh8ZFTMzMzMzMzGzp3Ts3MzMzMzGzo3Dk1MzMzMzOzoXPn1MxsRETEzG6WdZm1X0Ss3Da/SkT4XnJmZmZWmzunZgMQEZt0WPbcUvLGWccb+njvxhExLyJWGLN8l5p520bENtXjTSLi3RHxorrlW8q6am33JNnm33S5rBv7Sfpba0bSX4G31sx6VFWHs/vNqbJ27rBs7xzZYzI/VPN9L4yIfSJivTHL31gjKyJiz4h4ZfV4XkQcFhH7R0Ttv/WDqMO69Ve9dzLUYbHtde52q3pv1rZrEPVXZRZRh020/W4L+9+PbXA8Wq/ZAETE1cAJwGeAWdXPuZKeVULeOOu4TdI6Nd73DuBtwHXAFsA7JZ1aPbdA0lY95h0M7ArMAM4GngmcCzwfOEvSx3st41LW1/N2l77NEbE6sBZwIvAaIKqn5gBHSNq4l7wq8ypJm7bNTwOulPSMXrPG5M4HngXcA5wPnAf8sur89pp1HnAN8N/ACsDRwIOSXtFPGTusp87vzCeAHYAFwG7AFyQdXj1X53fmK8ATgWWBe4GZwA+BFwN3SXpnL3ltuY3XYR9tzWSpwyLb69ztVvW+7O31IOqvWs/Q67Cpv3duC/vfj21w3Dk1G4CIWB74NLA1sCJwEvBpSYuGmRcRV473FPBUST2f8hkRVwHPkvSP6lvQ7wInSPpiRFwmacsaeVuQ/sDcCawt6d5IR9culLRZjTJm3e7St7n6hvz1wFzgkran7gOOk3RKL3lV5qHAGsAR1aL9SH/439Vr1jj5awKvIP0ztaakGTUyAngP8JZq0YcknVyzPPeO9xQwu9fyVZ/xlpIejnR69DeA6yUdUPd3RtKmEbEM6XdmDUkPRcQMYEGd/aTKzVKHueuvypwsdVhke5273WrLzN1eZ/v7WXodNlF/Va7bwj73Yxucnv8YmFktC4EHgNmkb35vqdsxzZy3GvBCYOxRqQB+XbNs0yT9A0DS7yOdfvXdiFiXxUfsevGwpEeAf0bEzZLurbIfiIi6dZh7u4veZknzgfkR8XJJ36tRnk4OJJ3Ge0A1fzZwZL+hEfE64N+ATYE/A18iHUGtYxVgW+BmYG1g3YgI1ftW9m/ANpLu6lDmP9TImyHpYQBJf4uI3YCjIuI7pG/8e9XKWhgRF0t6qJp/uI/9BPLVYe76g8lTh6W217nbLWimvc7597P0Omyi/sBtYY792AbE516bDcbFpD+u25D+8X511fAOO+9HwAqSbh0z/R74ec2y3RURW7Rmqj/c/w48ntTh6NVDEbFc9Xjr1sKIWAmo+4cm93YXvc2Rrll6N+kfknePnWqUD0mPSPqSpN1JR2XPbf2D0acvkI4cfA14h6TPSKp7XewFwJmSdiHtK2sCv6qZdTyw7jjPfaNG3s0R8ZzWTFWf+wDXA0+rkXdnVNe8VdsLPHpK90M18lpy1WHu+oPJU4eltte52y1opr3O+fez9Dpsov7AbWGO/dgGxKf1mg1ARMyVdMmYZXtJOqGEvC7Wt4q6vOYvItYmfft7Z4fntpf0q14yI2KmpAc7LH886XSdq3otY7d6KGPR2xzpOiaAjUj/mPywmt8NuEjS65aW0SHzHGAPYDrpWqF7gJ9JOrDXrA7ZTwd2JF2H9BTSKV571chZR9JtY5btKOm8fss4wTqfLumaLl43G9IRkQ7PrSXpjl7yJljP8sDykv5UJ2/QddhL+SZRHRbZXudut6rXZm+vB11/Vf6UaPvb3u+2sM/92AbHnVOzBkXEqhM9L+meYeb1sN5ag2MMMnMUy9hrXqRBMV4s6b5qfkXgx5J2rLHuyyRtGRH7AOtJ+mBEXKk+r+eJiDnA9sBzSEdJHg9cIKnrkSUjYsI6kbSgnzIuZd1T4ndmWHU4lfbjqdJeD+szGVb9Vesuug5L34+rdU+JOrTB8zWnZs26FBDp2pN1SNe5BLAycCuwwZDzulX3+qNBZo5iGXvNW40lT2t6qFpWx4yIeALwSqD2rUA6+GXb9CVJt9fIOKT6OYs0CNQVpLrajDQgVNZRPscY9mecK29YdTiV9uOp0l4P6zMZVv1B+XVY+n4MU6cObcDcOTVrkKT1ASLia8D3JZ1eze8K7D7svF5WPQkyR7GMveYdD1wUEd+v5ncH5tdc98eBX5Bu83JRRGwA3FIz61H9HnmtMp4HEBGnAFu1nQr3DODD/eYvbfVTIW+IdThl9uMp1F4P5TMZYv1B+XVY+n4MU6QObfA8IJLZYGzX+sMKIOkM4NkF5dkIULpH3htIRyD+CrxB0idqZn1T0iaS9q3mfyfppf2WMSKeEBGfjYjTI+Jnralm3Eatf8aqMl5NvQE2RpnrsH9ur/vj+uuf92ObNHzk1Gww/i8iDgJOrOZfC/xfQXlLM5VOtRtmZgl5t5CG2p8BRERsVee6o4h4MvBlYHVJm0fEZqTrWT9Zo0ztTgK+RRrxcj9gb+DumllXRsTRLLmfjHefw1xyjwY57LxB12ETo2kOuw4ne3s97LZ10PUH5ddhr3luC5vPs0w8IJLZAFQDOxxMGuAF4DzgI72MtlflnCBpr6huC0Ia0RTS6ZUf7WeAiIhYBXgSbV9atTotEbFqnezcmaNYxpx5EfG/pNu+3MziU5okaadeylRl/Rx4P/DlamCkAK6W9PRes8bkXipp6/bBlSLdq26bGlmzSPdibe0n5wFflfSvPsu4GbAeS34mp0zFvCbqMPf2NpGZuQ5b7X97HX6kpPa65La1Q/31/fcuZ/kmQ57bwnxtjTXPnVOzAYiIucAHWLKRVK/X10XEtcDOwBnA80jfnj66E9f9Y52z09JU5iiWsYG864FNVd2UvB+tDmNr1N5q2eWStljae5eSe4Gk7SLiLOAw0hGS70rasN8y5xARx5AGE7mGxfcdlKQ3TsW83Joo3wjWYdHtTFOZVe5qku7qJ6PKKboOm6q/nErf70rfj218Pq3XbDBOAv4buJr+bqR9BHAOaZTC9vu+tTqpdUcv3BPYMEenpcHMUSxj7ryrSSNd/ilD1l8iYn2qf5wiYnfgMff6q+FjkW44/x7gcGAOcECGXAAi4gxJu/YRsZ2kTXKVp9S8SLf0eR+wNnC6pJPbnvuKpP2HWb6GM3PV4eqkI36LSCNavx14GfBb4J2S/lgzuvR2JkvmOLeSuTAitiQdXOnnyGnpdZglLyJ2kXRm9Xgl0ui925L+FhzQZ0e/yP2uwTwbEA+IZDYYd0s6TdItkm5tTb2GSDpM0tOAYyRt0DatL6mfYfVbnZaccmeOYhlz530SuCwizoqIH7ammln/BXwd2DgibgXeS7pGtC+SfiTp75KulvQ8SVuTvpDpWkRsNc60NdDXkV3gNxGR8x+eUvOOJX3p9T3g1RHxvYiYWT23XR+5ube3icxceccB1wJ/AM4FHgBeDJxP+qKxrtLbmVyZfybdTqZ9WgtYwJJfztZReh3mymsf8O4Q0heIuwEXA0f2mV3qftdUng2IT+s1G4CImAe8mvRP9oOt5aVc+1Cddnwq6Q9ie/leUkrmKJaxgbxrSP+QXEXbEXxJv6iTV2WuRPpb8re6GW1ZawFrAFdKeigingi8C3i9pDV7yHmEdF1ap0FDtpM0u48yPgf4IemfvAerdfR8in7peWNP0Y6IDwAvAl4CnK2aN6/Pvb1NZGasw/ZT3m+TtE7bc7VPgS+9ncmVGRHvAZ4PHKjFt0C5RdUtZvpReh3myouIBa19tcM+3ddlGKXud03l2eD4tF6zwXgDsDGwDG3XPgBFdE5J97r8NGM6LYVljmIZc+f9U9Jh/YZEGqn3zaTfaYDrIuJoSTf1kfku0nXZNwEzI+IrpG0/Hti6x7jrgLdIurHDev5Qt4yVrwN7ke8zKTVvZkRMk7QI0m2IIuIO0kAqKxRQviYzc+W1n512/ATP9ar0diZLpqRDIuJbwOer/fZg8t2bsvQ6zJX3xEgDKAYwJyJCi49K9Xv2ZKn7XVN5NiDunJoNxjaSNhp2ISaQpdPScOYoljF33vkR8UnSt8nt38Z3fSuZiHgm6Rv9o0n/cAewJXBeRLxU0sU1y7Yv6V5890TEOsANwPaSLq2R9WHG/8fr7TXL13K3pLqnQk+mvNOAnYCfthZIOi4i7iRdC1xX7u1tIjNX3qkRsYKkf0g6qLWw+nLnhj5yS29nsmVKuh14ZUS8BDgbWK7vkiWl12GuvK8BK1aP5wOPB+6uroe+vM/sUve7pvJsQHxar9kARMSxwGclXTvssnQSEYeSOiu1Oy1NZ45iGRvIO7fDYqmHESAj4nTgEEnnjFn+PNLpdy+qWbZHTz+r5q+QtHmdrCZVR3RXJnXe+j5Fv/S83Joo3wjWYdHtTIOZs4ENJF1TN6Op8pWe14TS97vS92MbnzunZgMQEdcBGwK3UOC1Dzk6LU1njmIZm9jmfkXEDZKeOs5z19c9QyAi/gR8s23Rq9rnJb2jRubjSEdRtyedDvhL0v0R/1KnjFXmsR0WS/Vvd1B6XtY6zF2+JjInQR0W387kzGxoPy66DhvIexzptOgdcFvoW8kUzp1TswGIiHU7LVeNEXvN6oqIE4D/kvT3an5d0sjP83rIuFRpBN1Ozy1x9LPHsu090fOS5tfIPJt0jeSJ1aLXAs+VtHPvJZxwPcsq4204SsobRB3m3t4mMkuvw6nM9dc/t4XN5FlDJHny5GnEJ+AEYKW2+XWBc0rKHMUyNpD3FtI9Fl9EGtDoBmC3HjP+BBzaYfo8cFfO38tx1n94D6+9usOyq/pc/8+B9drmtwGumMJ5Weswd/lGtA6LbmdyZza0Hxddhw3kuS3M0NZ4GszkAZHMDNIpPhdWo/qtBRwIvKewzFEsY9Y8SUdGup3MuaR7CG4p6c4eY943wXPvr1u2Hmzfw2t/EhGvAr5dzb8COKvP9X8SODMiDiN9Ji8ijcY9VfNy12Hu8jWRWXodFt3ONJDZxH5ceh3mznNbmKetsUEYdu/YkydPZUyka1EWAn8EVi8xcxTLmDOPNKz+DaR77n6SdDP7zRv6ffpCQ7kLunjNfcC91c9FVf0trB7fm6EMz838GReX12Qd5t7eEa3DYtuZXJkD2I+LrsMceW4L87c1npqfhl4AT548DX+igU5L7sxRLGMDeT8Antg2vy1weUO/U0vtRJaU28P6P0i6b96zWHya9Iunal7p9TeidVh0O9NU5ijVYen1V5Wx6P2u9P3Y0wSf3bAL4MmTp+FPTXRacmeOYhmb2OYO61i2od+ppjqnl3Xxmo2rn1t1mvpc/xeA2W3z6wJnT7W8puow9/aOaB0W387kyGx4Py66DnPluS3M29Z4Gszk0XrNrKOpPoLmoDJLyIuIw0m3D+hINW7T0sU6a4/cW71/DmnY//vGLH+9pOOW8t6jJO075nYMj26/hngrnsnCddi/QdZhCe1M7sxB/w6WXoc1237vxzbpuHNqNsKa6LTkzhzFMjaQt3f1cHtgE+Bb1fwrgWsl7ddLXpfrvEzSljXetw1wDLAi6X7AfwPeKOnSGll7AmdKujciPkg6WvC/qnEj+4g4jYk/k5dMpby23Cx12ET5RrAOi25nGszMuR8XXYdNfZHotrB+ng3etGEXwMyG6hLgUmAW6Y/VjdW0BbBsIZmjWMaseZLmK90ndDPSve0Ol3Q4MK/KbMKXar7v68D+ktaTtC7wNuDYmlkHVf+M7QDsBBwNfLVm1ueAQ4BbgAeAr1XTP4Cbp2BeS646bKJ8o1aHRbczDWbm3I9Lr8Mm6g/cFubYj21Qhn1esSdPnoY/ARcAM9rmlwEuKClzFMvYQN71wKpt86sA1/eY8X3glPGmDL+Lj7mmlJrXr7aySAOKvGa8/B4zL+lm2RTKy1qHucs3onVYdDuTO7Oh/bjoOmwgz21hn3meBjf5PqdmBqmTMge4p5pfoVpWUuYoljF33qeAy6rrjwLYEfhwjxl1j4hOKCJa16f+IiKOBE4mnZr1H6SbqddxR5X1fODTETGT/s8YWj4iNpD0O4CIWB9Yfgrn5a7D3OVrIrP0Oiy9ncmd2cR+XHod5s5zW5inrbEBcOfUzCBPp6XpzFEsY9Y8ScdGxBnAM0kdv/8n6c4eM86pu/6lOGTM/MHtq62ZuSewC/A5SX+LiDVIN7PvxwHAzyPid6TPZF1g3ymcl7sOc5eviczS67DodqaBzCb249LrMHee28I8bY0NgAdEMjMAImJ1FndaLuq10zKIzFEsYwN5LyH9owPwC0mn1czZEPg4aYClWa3lkp7aT/kmg+qow8bV7G8lPTiV83JronwjWIdFtzNNZeZUeh2WXn9Q/n5X+n5snfnIqZm1bAv8W/VYQK1OS8OZo1jGbHkR8SlgG+CkatE7IuJZkt5fI+444GOkwSd2Bd5A/SOcY8v5YuDpLNnp/WiO7H5FxDKkG7q3Ovg/j4gjJS2cinm5NVG+UavDSrHtTMOZOZVeh0XXX+n73STZj60DHzk1s06dllcDF9fstDSSOYplbCDvSmALSYuq+emkQTE2q5F1qaStI+IqSZtWyy6RNLdO2dpyjwCWA55HGlHyFaSjBvv0k5tLRBxNGpxkfrVoL+ARSW+ainm5NVG+EazDotuZpjJzKr0OS68/KH+/K30/tgkMe0QmT548DX8CrgSmtc1PB64sKXMUy9hQXvtovavWzQN+TRpQ4wfAfsBu9Djy73hlHPNzBeD8fnNzTcAV3SybKnml19+I1mHR7UxTmaNUh6XXX1Wmove70vdjT+NPvs+pmbWs3PZ4pUIzR7GMOfM+SRpk47iImE+6n97Ha2YdQBr58B3A9sCbSKf29uuB6uc/I2JNYCGwRobcXB6prrcFICI2AB6Zwnm5NVG+UatDKLudaTIzp9LrsPT6K32/mwz7sXXga07NDBZ3WtpHBnxvYZmjWMaseZJOjoifk04Xgxqj9bZZS9KFwH2k06WIiJfVLVubH0XEysBngQWka62OzpCby4HAuWNGgOynU156Xm5NlG/U6rDodqbBzJxKr8PS6w/K3+9K349tHL7m1MwAqIaWb3Vaco3WmDVzFMuYMy8i9gB+Junv1fzKwHMl/aBG1gJJW41ZdqmkreuWr8M6ZgKzWuUtRVWujarZ65VnRMli83JronwjWIfFtjNNZuZUeh2WXn9Q/n5X+n5s4xj2ecWePHka/gTsAazUNr8ysHtJmaNYxgbyLu+w7LIeM14IfB64Czi0bTqaNGBHv7+LbwNWbptfBdi/39xcU+7ylZ5Xev2NaB0W3c40lTlKdVh6/VVlKnq/K30/9jT+5COnZkZEXC5pizHLLpO0ZSmZo1jGBvKu1JiRedtH2+0yY0tgK+BDQPvtXe4jHZX9c52yteVn/5xzmgSf8UjVXxOZo1aHk+Ezya30Oiy9/qD8bZ4MdWid+ZpTMwM6Do7Wb/uQO3MUy5g775KIOBT4cjX/X6RBkbom6TLStVAnAQ8DT66euknSw32UrWV6RISqb04j3e5m2Qy5ueQuX+l5uTVRvlGrw9LbmaYycyq9DkuvPyh/vyt9P7ZxeLReM4Oq0xIRG1bT5+mx0zKAzFEsY+68twMPAd+spgeA/WtmzQVuAr4OHAPcEBHb91G2ljOBb0XEvIiYB5xcLStF7vKVnpdbE+UbtToc2y4cSt52pt+8pjJzKr0OS68/KH+/K30/tnH4tF6zERYRJ0jaKyIOIt1PcufqqbOBj0m6f9iZTZSxLXt54IPAvDGZ/5yieXOBDwDrsfhbeI091bfLrEuA/5R0bTX/NOAESXPrlK0tdxrwFpbc5qMlFXELgKp8+7Lk72Ht8pWel1sT5RvBOmy1C1nawtx5TWXm1KF8PwE+nrEOi8prQun7Xen7sY3PnVOzERYR15Ia7jOA55GGW3+0UZB0z7AzmyhjW3a2ztokybse+G/gamBRa7mkW2tkdbp+9THLapZzWdIIiyKNsLiw30wzs04iYjVJd41KnlnpSjt/3cwG6wjgHGAD4JK25a0O4AYFZDZRxpaT6NBZm8J5d0s6LUMOwIKIOAI4sZp/LXBZv6ER8VxgPvB70mf8pIjYW9J5/WY3JSLOkLTrqOTVLMMc4H3A2sDpkk5ue+4rkno+vTx3ZhNlzCkiVgcOJrUFHyKdpv8y4LfAOyX9cZh5TWXmFBGrdlh8YaSB3qLGl6dF5zUhInaRdGb1eCXgEGBb0t+pA3rtSJeeZ4PnI6dmRkR8VdJbS85sqIy/lLTDCOXNA15N6uw/er83SafUyJoFvANole984HBJ/+qzjJcCr5F0fTX/VOBkZbx/as1ybTXeU8CPJK0xlfJyi4jvATcCFwBvBBaSPucHo8M9c4eR2UQZc4qIM4EfA8sDryF9efUNYHdgZ0kvHWZeU5k5RcQiYOyZImsDt5POSunpy87S85rQvi9ExNHAncDXSF9CPEfS7lMpzwbPnVMzG1k5O2uTJO9EYGPgGhYfiZWkN/aQcZyk19dZf5f5jZ0u3I+IeAT4BamzN9Z2kmZPpbzcYsxtHSLiA8CLgJcAZ9fsnGbNbKKMOUXbbTAi4jZJ67Q995jbZgw6r6nMnCLiPcDzgQMlXVUtu0XS+lMxrwljOn9j95k6v4dF59ng+bReMxtlbyB11pahrbMG1Or8TYK8bSRtVPO9LU13Ei+pvu1uP134kglePyjXAW+RdOPYJyLiD1MwL7eZETFN0iIASR+PiDuA80gDnZWQ2UQZc2q/w8LxEzw3rLymMrORdEhEfAv4fLVfHEzbGAZTLa8hT4yId5O+CJsTsfh2LdT7jEvPswFz59TMRlmOztpkyvt1RGyiaoTdmpZrXf/U6UlJC/rIBngr8DbSKcOQThf+Sp+ZOXyY8f+xefsUzMvtNGAn4KetBZKOi4g7gcMLyWyijDmdGhErSPqHpINaCyPiycANBeQ1lZmVpNuBV0bES0gjuC43lfMa8DVgxerxfODxwN3V9caXT8E8GzCf1mtmIysijgU+22dnbTLlXQdsCNxCOk046HH034i4D7iYzp1TSdqpZtkOJF1benud95uZ9SoiZgMbSLpmFPLMJgMfOTWzUbYdcHlE1O6sTbK8XWq+r91NdTugS7Em8JuI+D3pZunflvTnBtbTl4h4HOko5fak0+1+CXxU0l+mYl5uTZTPdVje9k62OoyIrHVYWl4TqjIeTBoUL9fvYbF5Njg+cmpmIysi1u20XDXu+zkZ8nJoH/CkgewAdgReRRrd8wpSR/UUSfc1sc5eRcTZpGsQ26+Jfa6kncd/1+TNy62J8rkOy9te12FZeU0ofZsnQx1aZ+6cmplZ1yLiBZJ+0sXrvifp5X2sZzqwM/ApYCNJRVx3FRFXS3rGmGVXSdp0Kubl1kT5XIflba/rsKy8JpS+zZOhDq0zj1plZmZd66ZjWql9P76I2BT4KPBl0unM76ub1YCfRMSrImJaNe0JnDWF83Jronyuw/K213VYVl4TSt/myVCH1oGPnJqZWXbRdq+5Ll//FNLpvK8CHgG+CXxT0u8aKmJPIg0EJdJ1v8uTyggwHfiHpDlTKS+3JsrnOixve12HZeU1ofRtngx1aBNz59TMzLKr0Tm9mXR96TclXd1cyczMzKxUHq3XzMya0PE+qOORtGFXoRG/kfSsekWqLyI2lvTbiOjY4e71/q6l5+XWRPlch0sqYXtdh2XlNaH0bZ4MdWgT85FTMzPrWkTMkXTvOM+tI+m26nFXAyfVWH9jowUvZb1HSdo3Is5tW/zoH1D1eHud0vNya6J8rkOgsO11HZaV14TSt3ky1KEthSRPnjx58uSpqwlY0Pb4nPGeG8T6h7T9ewJzqscfBL4PbDVV80qvP9dhmdvrOiwrz3VYZh166jx5tF4zM+sB5WyQAAAFtklEQVRF++m6q07w3FR1kKR7I2IHYCfgaOCrUzgvtybK5zosb3tdh2XlNaH0bZ4MdWgduHNqZma90DiPO803Ydgd4NbIjy8Gvibpx8CyUzgvtybK5zosb3tdh2XlNaH0bZ4MdWgduHNqZma9eGJEvDsi3tP2uDX/hBwriIh1I2Ln6vHsiFix7em9cqyjD3dExJHAfwCnR8RM+vtbWnpebk2Uz3VY3va6DsvKa0Lp2zwZ6tA68IBIZmbWtYg4eKLnJX2kz/w3A/sCq0raMNL9T4+QNK+f3FwiYjlgF+AqSTdGxBrApqo5+FPpebk1UT7XYXnb6zosK68JpW/zZKhD68ydUzMzyyIilpd0f58ZlwPbAheqGpU3Iq6StGmOMpqZmVm5fHjbzMx6EhFrRcTciFi2mn9iRHwCuDFD/IOSHmpb1wwGcy2rmZmZDZk7p2Zm1rWIeBdwOXA4cEFEvAm4DpgNbJ1hFb+IiPcDsyPi+cB3gNMy5JqZmVnhfFqvmZl1LSKuBXaQdE9ErAPcAGwv6dJM+dOAfYAXkEbmPQs4Wv5jZWZmNuW5c2pmZl2LiAWStmqbv0LS5pmypwPHS3ptjjwzMzObXGYMuwBmZjaprB0Rh7XNr9E+L+kddYMlPVLdRmbZ9utOzczMbDS4c2pmZr04cMx8ltN52/wO+FVE/BB4dORfSYdmXo+ZmZkVxp1TMzPrxRXAFQ1eA3pzNU0DVmxoHWZmZlYgX3NqZmZdi4hLgA1IR0x/DfwK+I2k+4ZaMDMzM5v03Dk1M7OeRMRywLbAs6tpG+BO4FeS9u8z+1w63NdU0k795JqZmVn53Dk1M7NaImJ5YDtge+A/gWmSNugzs/1eqbOAlwMPS/qffnLNzMysfO6cmplZ1yLiNaSjpVsADwIXAxeSTu29s6F1XiRp2yayzczMrBweEMnMzHpxJHA9cARwnqQbcoZHxKpts9OArYGVcq7DzMzMyuQjp2Zm1rWImA5szuLrTTcC/gj8hnT09Gd95t9CuuY0gIeBW4CPSvplP7lmZmZWPndOzcystohYDXgl8C5gfUnT+8ybJelfY5bNlPRgP7lmZmZWPp/Wa2ZmXYuIzVh81PTZwLKkW8ocTrqtTL9+DWw1ZtlvOiwzMzOzKcadUzMz68VxpE7oGcBBkm7LERoRqwNrAbMjYkvSab0Ac4DlcqzDzMzMyubTes3MrGcRMQt4cjV709hTcWvk7Q28HphLGgG41Tm9F5gv6ZR+8s3MzKx87pyamVnXImIG8AngjcCtpE7kk4BjgQ9IWthn/sslfa/vgpqZmdmkM23YBTAzs0nls8CqpMGPtpa0FbAhsDLwuQz5W0fEyq2ZiFglIj6WIdfMzMwK5yOnZmbWtYi4EXiqxvzxqG4x81tJT+kz/zJJW45ZtqDqBJuZmdkU5iOnZmbWC43tmFYLHyHdn7Rf0yNiZmsmImYDMyd4vZmZmU0R7pyamVkvro2I/xy7MCJeB/w2Q/5JwDkRsU9E7AOcDczPkGtmZmaF82m9ZmbWtYh4EvBd4AHg0mrxXGA2sIekOzKsY1dgXjV7tqSz+s00MzOz8rlzamZmXWtd/xkR84BNqsXXSjpnmOUyMzOzyc+dUzMz61qnAYsy5f5S0g4RcR9LXrsapOtc5+Rep5mZmZXFnVMzM+taRNwOHDre85LGfc7MzMxsIjOGXQAzM5tUpgMrkI5oZhMRs4D9gCcDVwLHSHo45zrMzMysbD5yamZmXWvqnqMR8S1gIXA+sCtwq6R35l6PmZmZlctHTs3MrBdZj5i22UTSpgAR8XXgoobWY2ZmZoXyfU7NzKwX85b+kloWth74dF4zM7PR5NN6zcxs6CLiEeD+1izpvqn/xKP1mpmZjQx3Ts3MzMzMzGzofFqvmZmZmZmZDZ07p2ZmZmZmZjZ07pyamZmZmZnZ0LlzamZmZmZmZkP3/wHtPgX0hQM7vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "correlation_df = pd.DataFrame(disorder_corr.T, columns=columns_to_drop[2:], index=most_common_disorders)\n",
    "sns.heatmap(correlation_df)\n",
    "\n",
    "plt.title('Correlation between disorders and discarded categorical features')\n",
    "plt.ylim([len(most_common_disorders), 0])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st approach: Do nothing\n",
    "\n",
    "Some algorithms such as XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.875 precision 0.614 0.903 recall 0.403 0.956\n",
      "[[ 27  40]\n",
      " [ 17 372]]\n"
     ]
    }
   ],
   "source": [
    "def do_nothing(x, y):\n",
    "    return x, y\n",
    "\n",
    "run_binary_classification(processed, 1, xgb.XGBClassifier(), do_nothing, False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Study.Site</th>\n",
       "      <th>ACE_Score</th>\n",
       "      <th>APQ_P_OPD</th>\n",
       "      <th>APQ_P_Total</th>\n",
       "      <th>APQ_SR_OPD</th>\n",
       "      <th>APQ_SR_Total</th>\n",
       "      <th>ARI_P_Total_Score</th>\n",
       "      <th>ARI_S_Total_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>WISC_FSIQ</th>\n",
       "      <th>YSR_AB</th>\n",
       "      <th>YSR_AD</th>\n",
       "      <th>YSR_WD</th>\n",
       "      <th>YSR_RBB</th>\n",
       "      <th>YSR_SC</th>\n",
       "      <th>YSR_Ext</th>\n",
       "      <th>YSR_Int</th>\n",
       "      <th>YSR_Total</th>\n",
       "      <th>Anxiety Disorders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.048254</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>13.484941</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>17.203855</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12.379192</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9.153661</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1748</td>\n",
       "      <td>0</td>\n",
       "      <td>10.046885</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1749</td>\n",
       "      <td>1</td>\n",
       "      <td>18.881359</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1751</td>\n",
       "      <td>0</td>\n",
       "      <td>17.101756</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1752</td>\n",
       "      <td>1</td>\n",
       "      <td>6.517796</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1757</td>\n",
       "      <td>1</td>\n",
       "      <td>7.563084</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>653 rows Ã— 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex        Age  Study.Site  ACE_Score  APQ_P_OPD  APQ_P_Total  \\\n",
       "0       1   7.048254           1        NaN        NaN          NaN   \n",
       "6       0  13.484941           1        NaN       19.0        102.0   \n",
       "8       1  17.203855           1        NaN       23.0        104.0   \n",
       "9       1  12.379192           1        NaN        NaN          NaN   \n",
       "10      0   9.153661           1        NaN        NaN          NaN   \n",
       "...   ...        ...         ...        ...        ...          ...   \n",
       "1748    0  10.046885           3        NaN       20.0        110.0   \n",
       "1749    1  18.881359           3        5.0       16.0        122.0   \n",
       "1751    0  17.101756           3        NaN       15.0        111.0   \n",
       "1752    1   6.517796           1        NaN       14.0         91.0   \n",
       "1757    1   7.563084           3        NaN       21.0         96.0   \n",
       "\n",
       "      APQ_SR_OPD  APQ_SR_Total  ARI_P_Total_Score  ARI_S_Total_Score  ...  \\\n",
       "0            NaN           NaN                NaN                NaN  ...   \n",
       "6           17.0         113.0                NaN                NaN  ...   \n",
       "8           12.0         117.0                NaN                5.0  ...   \n",
       "9           18.0         146.0                NaN                NaN  ...   \n",
       "10          20.0         116.0                NaN                NaN  ...   \n",
       "...          ...           ...                ...                ...  ...   \n",
       "1748        18.0         115.0                0.0                NaN  ...   \n",
       "1749        17.0         118.0                0.0                0.0  ...   \n",
       "1751        15.0         143.0                6.0                8.0  ...   \n",
       "1752        19.0         133.0                1.0                2.0  ...   \n",
       "1757        19.0         116.0               10.0                8.0  ...   \n",
       "\n",
       "      WISC_FSIQ  YSR_AB  YSR_AD  YSR_WD  YSR_RBB  YSR_SC  YSR_Ext  YSR_Int  \\\n",
       "0           NaN     NaN     NaN     NaN      NaN     NaN      NaN      NaN   \n",
       "6           NaN     6.0     9.0     9.0      0.0     9.0      6.0     27.0   \n",
       "8           NaN     8.0     9.0     3.0     10.0    10.0     18.0     22.0   \n",
       "9           NaN    11.0    14.0     4.0      4.0     8.0     15.0     26.0   \n",
       "10          NaN     NaN     NaN     NaN      NaN     NaN      NaN      NaN   \n",
       "...         ...     ...     ...     ...      ...     ...      ...      ...   \n",
       "1748      109.0     NaN     NaN     NaN      NaN     NaN      NaN      NaN   \n",
       "1749        NaN     NaN     NaN     NaN      NaN     NaN      NaN      NaN   \n",
       "1751        NaN    16.0     1.0     2.0      6.0     4.0     22.0      7.0   \n",
       "1752      121.0     NaN     NaN     NaN      NaN     NaN      NaN      NaN   \n",
       "1757      129.0     NaN     NaN     NaN      NaN     NaN      NaN      NaN   \n",
       "\n",
       "      YSR_Total  Anxiety Disorders  \n",
       "0           NaN                  1  \n",
       "6          55.0                  1  \n",
       "8          62.0                  1  \n",
       "9          79.0                  1  \n",
       "10          NaN                  0  \n",
       "...         ...                ...  \n",
       "1748        NaN                  1  \n",
       "1749        NaN                  1  \n",
       "1751       43.0                  1  \n",
       "1752        NaN                  0  \n",
       "1757        NaN                  1  \n",
       "\n",
       "[653 rows x 274 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd approach: Fill all missing values with a dummy value \n",
    "\n",
    "(For this refer to @gvasilak 's notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd approach: Imputation Using (Mean/Median/Most_frequent) Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.897 precision 0.778 0.907 recall 0.418 0.979\n",
      "[[ 28  39]\n",
      " [  8 381]]\n"
     ]
    }
   ],
   "source": [
    "strategy = 'median'\n",
    "assert strategy in ['mean', 'median', 'most_frequent']\n",
    "\n",
    "def imputer(x, y, strategy):\n",
    "    if strategy == 'mean':\n",
    "        filling = x.mean()\n",
    "    elif strategy == 'median':\n",
    "        filling = x.median()\n",
    "    elif strategy == 'most_frequent':\n",
    "        filling = x.mode().iloc[0]\n",
    "    \n",
    "    return x.fillna(filling), y.fillna(filling)\n",
    "\n",
    "clf = RandomForestClassifier(class_weight='balanced', n_estimators=100)\n",
    "run_binary_classification(processed, 1, clf, imputer, False, None, strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th approach: Imputation Using k-NN\n",
    "\n",
    "The algorithm uses â€˜feature similarityâ€™ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping this many columns: 95\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.8\n",
    "\n",
    "columns_mask = pd.isnull(processed).sum() / processed.shape[0] > threshold\n",
    "\n",
    "print('Droping this many columns:', np.sum(columns_mask))\n",
    "\n",
    "dropped_columns = processed.columns[columns_mask]\n",
    "\n",
    "processed = processed.drop(columns=dropped_columns)\n",
    "processed_80 = processed_80.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping this many columns: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sotiris/.local/lib/python3.7/site-packages/missingpy/knnimpute.py:224: UserWarning: There are rows with more than 50.0% missing values. These rows are not included as donor neighbors.\n",
      "  .format(self.row_max_missing * 100))\n",
      "/home/sotiris/.local/lib/python3.7/site-packages/missingpy/knnimpute.py:282: UserWarning: There are rows with more than 50.0% missing values. The missing features in these rows are imputed with column means.\n",
      "  .format(self.row_max_missing * 100))\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "/home/sotiris/.local/lib/python3.7/site-packages/missingpy/utils.py:124: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/home/sotiris/.local/lib/python3.7/site-packages/missingpy/knnimpute.py:282: UserWarning: There are rows with more than 50.0% missing values. The missing features in these rows are imputed with column means.\n",
      "  .format(self.row_max_missing * 100))\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib64/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.879 precision 0.650 0.901 recall 0.388 0.964\n",
      "[[ 26  41]\n",
      " [ 14 375]]\n"
     ]
    }
   ],
   "source": [
    "def imputer_fun(x, y, my_imputer):\n",
    "    x_new = my_inputer.fit_transform(x)\n",
    "    y_new = my_inputer.transform(y)\n",
    "\n",
    "    x[x.columns] = x_new\n",
    "    y[y.columns] = y_new\n",
    "    return x, y\n",
    "\n",
    "my_inputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "\n",
    "run_binary_classification(processed_80, 1, clf, imputer_fun, False, 0.8, my_inputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def imputer_fun(x, y, my_imputer):\n",
    "    x_new = my_inputer.fit_transform(x)\n",
    "    y_new = my_inputer.transform(y)\n",
    "\n",
    "    x[x.columns] = x_new\n",
    "    y[y.columns] = y_new\n",
    "    return x, y\n",
    "\n",
    "my_inputer = MissForest(max_depth=4)\n",
    "\n",
    "run_binary_classification(processed_80, 1, clf, imputer_fun, False, 0.8, my_inputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th approach: MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can try using EM to complete the missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputer_em(x, y, disorder):\n",
    "    x_values = x.drop(columns=disorder).values\n",
    "    y_values = y.drop(columns=disorder).values\n",
    "    total_values = np.concatenate((x_values, y_values), axis=0)\n",
    "\n",
    "    total_values = impy.em(total_values)\n",
    "\n",
    "    x.loc[:, x.columns != disorder] = total_values[:x.shape[0]]\n",
    "    y.loc[:, y.columns != disorder] = total_values[len(total_values) - y.shape[0]:]\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "run_binary_classification(processed_80, 1, clf, imputer_em, True, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular methods is using MICE. What is the difference between MICE and other multiple imputers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fancyimpute import MICE as MICE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6th approach: Multiple imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoimpute.imputations import SingleImputer, MultipleImputer\n",
    "\n",
    "def imputer_fun(x, y, my_imputer):\n",
    "    x_new = my_inputer.fit_transform(x)\n",
    "    y_new = my_inputer.transform(y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "\n",
    "si = SingleImputer() # imputation methods, passing through the data once\n",
    "mi = MultipleImputer() # imputation methods, passing through the data multiple times\n",
    "\n",
    "run_binary_classification(processed, 1, clf, imputer_fun, False, None, mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7th approach: Add features indicating if the value os misisng or not\n",
    "\n",
    "For this part we should take into account which exact features can have Nan values\n",
    "\n",
    "The features that do not take any null values are 'Sex', 'Age', 'Study.Site' and the disorder we are currently checking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.820 precision 0.739 0.833 recall 0.405 0.954\n",
      "[[ 51  75]\n",
      " [ 18 374]]\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(x, cols):\n",
    "    x1 = x.drop(columns=cols).isna().astype('int32')\n",
    "    \n",
    "    # change column naming to enable the inner join\n",
    "    x1.columns = [col + '_existence' for col in x1.columns]\n",
    "    return x1.join(x)\n",
    "\n",
    "def data_and_existence_of_features(x, y, disorder):\n",
    "    cols = ['Sex', 'Age', 'Study.Site', disorder]\n",
    "    rest_of_columns = list(x.columns.values)\n",
    "    for c in cols:\n",
    "        rest_of_columns.remove(c)\n",
    "    \n",
    "    x_new = process_dataset(x, cols)\n",
    "    y_new = process_dataset(y, cols)\n",
    "    \n",
    "    x_new[rest_of_columns], y_new[rest_of_columns] = imputer(x_new[rest_of_columns], \n",
    "                                                             y_new[rest_of_columns], \n",
    "                                                             'most_frequent')\n",
    "    return x_new, y_new\n",
    "\n",
    "run_binary_classification(processed, 1, clf, data_and_existence_of_features, True, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existence of features\n",
    "\n",
    "To explore the severity of existence of a feature for the correct classification, we only try to predict based on the existence of the features only and not the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.720 precision 0.398 0.791 recall 0.294 0.857\n",
      "[[ 37  89]\n",
      " [ 56 336]]\n"
     ]
    }
   ],
   "source": [
    "def process_dataset_only_existence(x, cols):\n",
    "    x1 = x.drop(columns=cols).isna().astype('int32')\n",
    "    x2 = x[cols]\n",
    "    \n",
    "    return x1.join(x2)\n",
    "\n",
    "def only_existence_of_features(x, y, disorder):\n",
    "    cols = ['Sex', 'Age', 'Study.Site', disorder]\n",
    "    return process_dataset_only_existence(x, cols), process_dataset_only_existence(y, cols)\n",
    "\n",
    "run_binary_classification(processed, 1, clf, only_existence_of_features, True, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use an autoencoder as imputation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = processed\n",
    "\n",
    "check_disorder = most_common_disorders[0]\n",
    "\n",
    "# only include patients that have that particular disease vs patients that are healthy\n",
    "temp = dataset[(dataset[check_disorder] == 1) | (dataset[healthy_diagnosis] == 1)]\n",
    "\n",
    "pos = most_common_disorders.index(check_disorder)\n",
    "\n",
    "columns_to_drop = most_common_disorders[:pos] + most_common_disorders[(pos + 1):]\n",
    "temp = temp.drop(columns=columns_to_drop)\n",
    "\n",
    "train, test = train_test_split(temp, test_size=0.3, random_state=17)\n",
    "train_clear = train.drop(columns=[check_disorder])\n",
    "test_clear = test.drop(columns=[check_disorder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Study.Site</th>\n",
       "      <th>ACE_Score</th>\n",
       "      <th>APQ_P_OPD</th>\n",
       "      <th>APQ_P_Total</th>\n",
       "      <th>APQ_SR_OPD</th>\n",
       "      <th>APQ_SR_Total</th>\n",
       "      <th>ARI_P_Total_Score</th>\n",
       "      <th>ARI_S_Total_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>WISC_PSI</th>\n",
       "      <th>WISC_FSIQ</th>\n",
       "      <th>YSR_AB</th>\n",
       "      <th>YSR_AD</th>\n",
       "      <th>YSR_WD</th>\n",
       "      <th>YSR_RBB</th>\n",
       "      <th>YSR_SC</th>\n",
       "      <th>YSR_Ext</th>\n",
       "      <th>YSR_Int</th>\n",
       "      <th>YSR_Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>0</td>\n",
       "      <td>12.866301</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>7.784964</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>10.181040</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>6.333903</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1464</td>\n",
       "      <td>0</td>\n",
       "      <td>7.317134</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>1</td>\n",
       "      <td>10.695185</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>5.173511</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>5.797170</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1483</td>\n",
       "      <td>1</td>\n",
       "      <td>15.793634</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>1</td>\n",
       "      <td>7.700547</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1062 rows Ã— 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex        Age  Study.Site  ACE_Score  APQ_P_OPD  APQ_P_Total  \\\n",
       "1512    0  12.866301           3        NaN       16.0         90.0   \n",
       "150     0   7.784964           1        NaN       21.0         89.0   \n",
       "701     0  10.181040           1        NaN       22.0        104.0   \n",
       "860     0   6.333903           3        NaN       12.0         88.0   \n",
       "1464    0   7.317134           1        NaN       21.0        103.0   \n",
       "...   ...        ...         ...        ...        ...          ...   \n",
       "491     1  10.695185           2        NaN       18.0        102.0   \n",
       "1644    0   5.173511           1        NaN       20.0        105.0   \n",
       "168     1   5.797170           2        NaN        NaN          NaN   \n",
       "1483    1  15.793634           3        NaN       19.0        104.0   \n",
       "743     1   7.700547           1        NaN       15.0         85.0   \n",
       "\n",
       "      APQ_SR_OPD  APQ_SR_Total  ARI_P_Total_Score  ARI_S_Total_Score  ...  \\\n",
       "1512        13.0         107.0                0.0                0.0  ...   \n",
       "150         19.0         111.0                0.0                7.0  ...   \n",
       "701         22.0         112.0                3.0                2.0  ...   \n",
       "860          8.0         106.0                0.0                1.0  ...   \n",
       "1464        19.0         108.0                2.0                1.0  ...   \n",
       "...          ...           ...                ...                ...  ...   \n",
       "491         15.0         116.0                1.0                3.0  ...   \n",
       "1644         NaN           NaN                2.0                5.0  ...   \n",
       "168          NaN           NaN                0.0                0.0  ...   \n",
       "1483        25.0         141.0                0.0                3.0  ...   \n",
       "743          8.0          91.0                8.0                1.0  ...   \n",
       "\n",
       "      WISC_PSI  WISC_FSIQ  YSR_AB  YSR_AD  YSR_WD  YSR_RBB  YSR_SC  YSR_Ext  \\\n",
       "1512      75.0      100.0     4.0    13.0     7.0      0.0     6.0      4.0   \n",
       "150        NaN        NaN     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "701       83.0       73.0     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "860       83.0       95.0     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "1464     129.0      121.0     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "...        ...        ...     ...     ...     ...      ...     ...      ...   \n",
       "491       86.0       93.0     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "1644       NaN        NaN     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "168        NaN        NaN     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "1483      69.0       82.0    10.0     5.0     4.0      4.0     8.0     14.0   \n",
       "743      111.0      128.0     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "\n",
       "      YSR_Int  YSR_Total  \n",
       "1512     26.0       64.0  \n",
       "150       NaN        NaN  \n",
       "701       NaN        NaN  \n",
       "860       NaN        NaN  \n",
       "1464      NaN        NaN  \n",
       "...       ...        ...  \n",
       "491       NaN        NaN  \n",
       "1644      NaN        NaN  \n",
       "168       NaN        NaN  \n",
       "1483     17.0       56.0  \n",
       "743       NaN        NaN  \n",
       "\n",
       "[1062 rows x 273 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def get_batches(iterable, batch_size=64, do_shuffle=True):\n",
    "    if do_shuffle:\n",
    "        iterable = shuffle(iterable)\n",
    "\n",
    "    length = len(iterable)\n",
    "    for ndx in range(0, length, batch_size):\n",
    "        iterable_batch = iterable[ndx: min(ndx + batch_size, length)]\n",
    "        yield iterable_batch\n",
    "\n",
    "\n",
    "def get_reconstruction_loss(true, preds, mask):\n",
    "    loss = np.mean(((true - preds) ** 2) * mask, axis=1)\n",
    "    return np.mean(loss, axis=0)\n",
    "\n",
    "\n",
    "DEFAULT_LOG_PATH = './autoencoder'\n",
    "\n",
    "\n",
    "class Autoencoder:\n",
    "    training = None\n",
    "    input_ = None\n",
    "    input_mask = None\n",
    "    intermediate_representation = None\n",
    "    input_reconstructed = None\n",
    "    loss = None\n",
    "\n",
    "    def __init__(self,\n",
    "                 number_of_features,\n",
    "                 activation=tf.nn.relu,\n",
    "                 layers=None,\n",
    "                 dropout=None,\n",
    "                 regularization=0,\n",
    "                 masking=0.5):\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        if layers is None:\n",
    "            self.layers = [15, 15, 15]\n",
    "        else:\n",
    "            self.layers = layers\n",
    "\n",
    "        self.number_of_features = number_of_features\n",
    "\n",
    "        self.masking = masking\n",
    "        self.dropout = dropout\n",
    "\n",
    "        use_regularization = (regularization > 0)\n",
    "        self.use_regularization = use_regularization\n",
    "\n",
    "        if regularization == 0:\n",
    "            # set to small value to avoid tensorflow error\n",
    "            # use_regularization = False in this case and will not contribute towards the final loss\n",
    "            self.regularization = 0.1\n",
    "        else:\n",
    "            self.regularization = regularization\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self.training = tf.placeholder(tf.bool, shape=[], name='training')\n",
    "\n",
    "        # these do not contain the wanted prediction\n",
    "        self.input_ = tf.placeholder(tf.float32, shape=[None, self.number_of_features], name='input')\n",
    "\n",
    "        self.input_mask = tf.placeholder(tf.float32, shape=[None, self.number_of_features], name='input_mask')\n",
    "\n",
    "        self.intermediate_representation = self.encode(self.input_)\n",
    "\n",
    "        self.input_reconstructed = self.decode(self.intermediate_representation)\n",
    "\n",
    "        if self.input_mask is not None:\n",
    "            self.loss = tf.reduce_mean(((self.input_ - self.input_reconstructed) ** 2) * self.input_mask)\n",
    "        else:\n",
    "            self.loss = tf.reduce_mean((self.input_ - self.input_reconstructed) ** 2)\n",
    "\n",
    "        if self.use_regularization:\n",
    "            self.loss += tf.losses.get_regularization_loss()\n",
    "\n",
    "    def encode(self,\n",
    "               input_):\n",
    "\n",
    "        if self.masking > 0:\n",
    "            # mask randomly some of the inputs\n",
    "            input_ = tf.layers.dropout(input_, rate=self.masking, training=self.training)\n",
    "\n",
    "        x = input_\n",
    "        # important to use relu as a first layer to make all unobserved values set to 0?\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = tf.layers.dense(x, layer, use_bias=True, name='input_layer_1_' + str(i),\n",
    "                                activation=self.activation,\n",
    "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
    "            if self.dropout is not None:\n",
    "                x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self,\n",
    "               intermediate):\n",
    "\n",
    "        x = intermediate\n",
    "        for i, layer in enumerate(self.layers[::-1][:-1]):\n",
    "            x = tf.layers.dense(x, layer, use_bias=True, name='input_layer_2_' + str(i),\n",
    "                                activation=self.activation,\n",
    "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
    "            if self.dropout is not None:\n",
    "                x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
    "\n",
    "        x = tf.layers.dense(x, self.number_of_features, use_bias=True, name='input_layer_2_final',\n",
    "                            activation=self.activation,\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
    "\n",
    "        if self.dropout is not None:\n",
    "            x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reconstruct_with_session(self,\n",
    "                                 sess,\n",
    "                                 data):\n",
    "\n",
    "        # much faster to reconstruct the whole table and make predictions from it\n",
    "        data_reconstructed = np.zeros((data.shape[0], self.number_of_features))\n",
    "\n",
    "        for rows in get_batches(list(range(data.shape[0])), batch_size=1024, do_shuffle=False):\n",
    "            rows_features = [data[i, :] for i in rows]\n",
    "\n",
    "            ratings_reconstructed = sess.run(self.input_reconstructed,\n",
    "                                             feed_dict={\n",
    "                                                 self.input_: rows_features,\n",
    "                                                 self.training: False\n",
    "                                             })\n",
    "\n",
    "            data_reconstructed[rows] = ratings_reconstructed\n",
    "\n",
    "        return data_reconstructed\n",
    "\n",
    "    def reconstruct(self,\n",
    "                    data,\n",
    "                    log_path=None):\n",
    "\n",
    "        if log_path is None:\n",
    "            log_path = DEFAULT_LOG_PATH\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "            with tf.Session() as sess:\n",
    "                self.build_graph()\n",
    "\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(sess, tf.train.latest_checkpoint(log_path))\n",
    "\n",
    "                reconstructed = self.reconstruct_with_session(sess, data)\n",
    "\n",
    "                return reconstructed\n",
    "\n",
    "    def fit(self,\n",
    "            data,\n",
    "            data_mask,\n",
    "            test_data=None,\n",
    "            test_data_mask=None,\n",
    "            n_epochs=350,\n",
    "            decay_steps=None,\n",
    "            learning_rate=None,\n",
    "            decay=None,\n",
    "            log_path=None,\n",
    "            verbose=True,\n",
    "            print_every_epochs=10):\n",
    "\n",
    "        if decay_steps is None:\n",
    "            # empirical\n",
    "            decay_steps = data.shape[0] // 64 * 5\n",
    "\n",
    "        if learning_rate is None:\n",
    "            learning_rate = 0.001\n",
    "\n",
    "        if decay is None:\n",
    "            decay = 0.96\n",
    "\n",
    "        if log_path is None:\n",
    "            log_path = DEFAULT_LOG_PATH\n",
    "\n",
    "        validation = False\n",
    "        if test_data is not None and test_data_mask is not None:\n",
    "            validation = True\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "            with tf.Session() as sess:\n",
    "\n",
    "                self.build_graph()\n",
    "\n",
    "                global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "\n",
    "                learning_rate = tf.Variable(learning_rate, trainable=False, dtype=tf.float32, name=\"learning_rate\")\n",
    "                learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay)\n",
    "\n",
    "                # Gradients and update operation for training the model.\n",
    "                opt = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "                with tf.control_dependencies(update_ops):\n",
    "                    # Update all the trainable parameters\n",
    "                    train_step = opt.minimize(self.loss, global_step=global_step)\n",
    "\n",
    "                saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "                # writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "                # writer.flush()\n",
    "\n",
    "                # tf.summary.scalar('loss', self.loss)\n",
    "                # tf.summary.scalar('learning_rate', learning_rate)\n",
    "                # summaries_merged = tf.summary.merge_all()\n",
    "\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                for epoch in range(n_epochs):\n",
    "                    total_loss = 0\n",
    "                    for rows in get_batches(list(range(data.shape[0]))):\n",
    "                        rows_features = [data[i, :] for i in rows]\n",
    "                        rows_masks = [data_mask[i, :] for i in rows]\n",
    "\n",
    "                        # summaries_merged\n",
    "                        _, loss, step = sess.run([train_step, self.loss, global_step],\n",
    "                                                 feed_dict={\n",
    "                                                     self.input_: rows_features,\n",
    "                                                     self.input_mask: rows_masks,\n",
    "                                                     self.training: True\n",
    "                                                 })\n",
    "\n",
    "                        total_loss += loss * len(rows)\n",
    "\n",
    "                    # writer.flush()\n",
    "                    total_loss = total_loss / data.shape[0]\n",
    "\n",
    "                    if epoch % print_every_epochs == 0:\n",
    "                        if verbose and validation:\n",
    "                            reconstructed_train = self.reconstruct_with_session(sess, data)\n",
    "                            train_loss = get_reconstruction_loss(data, reconstructed_train, data_mask)\n",
    "\n",
    "                            reconstructed_test = self.reconstruct_with_session(sess, test_data)\n",
    "                            test_loss = get_reconstruction_loss(test_data, reconstructed_test, test_data_mask)\n",
    "\n",
    "                            print('At epoch {0:3d} train loss is {1:6.4f} and reconstruction losses are in train '\n",
    "                                  '{2:6.4f} and in test {3:6.4f}'.format(epoch, total_loss, train_loss, test_loss))\n",
    "\n",
    "                        saver.save(sess, os.path.join(log_path, \"model\"), global_step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_clear.values\n",
    "test_data = test_clear.values\n",
    "\n",
    "data_mask = ~np.isnan(data)\n",
    "test_data_mask = ~np.isnan(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[~data_mask] = -1\n",
    "test_data[~test_data_mask] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(data.shape[1], layers=[50, 25], masking=0.2, regularization=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch   0 train loss is 30060.1658 and reconstruction losses are in train 29552.0055 and in test 26843.7144\n",
      "At epoch   1 train loss is 29047.9410 and reconstruction losses are in train 26940.7035 and in test 24462.1138\n",
      "At epoch   2 train loss is 17574.7226 and reconstruction losses are in train 8713.6334 and in test 7972.1055\n",
      "At epoch   3 train loss is 10975.8029 and reconstruction losses are in train 6652.1310 and in test 6191.9976\n",
      "At epoch   4 train loss is 8714.0828 and reconstruction losses are in train 5784.6438 and in test 5411.3921\n",
      "At epoch   5 train loss is 6924.4988 and reconstruction losses are in train 3908.4255 and in test 3692.7302\n",
      "At epoch   6 train loss is 7171.8584 and reconstruction losses are in train 2958.3565 and in test 2814.4229\n",
      "At epoch   7 train loss is 7290.1025 and reconstruction losses are in train 2747.0842 and in test 2629.0613\n",
      "At epoch   8 train loss is 5466.8236 and reconstruction losses are in train 2222.2980 and in test 2141.3048\n",
      "At epoch   9 train loss is 4050.5629 and reconstruction losses are in train 1687.9439 and in test 1656.3005\n",
      "At epoch  10 train loss is 6978.7477 and reconstruction losses are in train 2210.7408 and in test 2136.6597\n",
      "At epoch  11 train loss is 5671.3795 and reconstruction losses are in train 2014.2452 and in test 1950.8328\n",
      "At epoch  12 train loss is 4646.7858 and reconstruction losses are in train 2171.0877 and in test 2085.8179\n",
      "At epoch  13 train loss is 6700.8561 and reconstruction losses are in train 2489.3366 and in test 2382.3938\n",
      "At epoch  14 train loss is 5910.5525 and reconstruction losses are in train 1925.2865 and in test 1858.4346\n",
      "At epoch  15 train loss is 5109.0102 and reconstruction losses are in train 1861.8544 and in test 1797.3018\n",
      "At epoch  16 train loss is 4640.3235 and reconstruction losses are in train 1960.9889 and in test 1890.0696\n",
      "At epoch  17 train loss is 5041.7740 and reconstruction losses are in train 1880.1785 and in test 1816.1919\n",
      "At epoch  18 train loss is 6701.9042 and reconstruction losses are in train 1915.2524 and in test 1847.0767\n",
      "At epoch  19 train loss is 4361.1619 and reconstruction losses are in train 1918.1616 and in test 1846.0150\n",
      "At epoch  20 train loss is 6531.4138 and reconstruction losses are in train 1848.5349 and in test 1783.4807\n",
      "At epoch  21 train loss is 5479.8011 and reconstruction losses are in train 2035.7117 and in test 1956.5844\n",
      "At epoch  22 train loss is 4248.8335 and reconstruction losses are in train 1900.3768 and in test 1828.4276\n",
      "At epoch  23 train loss is 5986.9911 and reconstruction losses are in train 1658.6590 and in test 1610.9153\n",
      "At epoch  24 train loss is 4328.7374 and reconstruction losses are in train 1825.8009 and in test 1763.4926\n",
      "At epoch  25 train loss is 4296.6157 and reconstruction losses are in train 1865.0259 and in test 1799.0101\n",
      "At epoch  26 train loss is 5842.3070 and reconstruction losses are in train 1882.0645 and in test 1821.2863\n",
      "At epoch  27 train loss is 4683.7919 and reconstruction losses are in train 1863.3954 and in test 1801.3367\n",
      "At epoch  28 train loss is 5766.5081 and reconstruction losses are in train 1855.1161 and in test 1793.4494\n",
      "At epoch  29 train loss is 7209.0707 and reconstruction losses are in train 1805.2693 and in test 1747.2068\n",
      "At epoch  30 train loss is 5019.8224 and reconstruction losses are in train 1879.9482 and in test 1818.2893\n",
      "At epoch  31 train loss is 3920.4605 and reconstruction losses are in train 1899.8810 and in test 1834.3729\n",
      "At epoch  32 train loss is 5093.3471 and reconstruction losses are in train 1662.6546 and in test 1619.6350\n",
      "At epoch  33 train loss is 4896.1140 and reconstruction losses are in train 1966.7130 and in test 1901.3226\n",
      "At epoch  34 train loss is 5308.3391 and reconstruction losses are in train 1813.3616 and in test 1762.5066\n",
      "At epoch  35 train loss is 4782.2768 and reconstruction losses are in train 1900.7912 and in test 1843.0444\n",
      "At epoch  36 train loss is 5256.2939 and reconstruction losses are in train 1762.5802 and in test 1717.1068\n",
      "At epoch  37 train loss is 4927.9014 and reconstruction losses are in train 1846.5771 and in test 1790.1183\n",
      "At epoch  38 train loss is 5273.4792 and reconstruction losses are in train 1915.1948 and in test 1850.4365\n",
      "At epoch  39 train loss is 5450.9838 and reconstruction losses are in train 1890.8915 and in test 1829.1109\n",
      "At epoch  40 train loss is 4580.7964 and reconstruction losses are in train 1972.1545 and in test 1903.3866\n",
      "At epoch  41 train loss is 6409.8733 and reconstruction losses are in train 1876.7221 and in test 1821.4252\n",
      "At epoch  42 train loss is 4321.2203 and reconstruction losses are in train 1906.6640 and in test 1847.9058\n",
      "At epoch  43 train loss is 5825.3489 and reconstruction losses are in train 1899.9969 and in test 1845.2798\n",
      "At epoch  44 train loss is 3406.7024 and reconstruction losses are in train 1954.9451 and in test 1894.7228\n",
      "At epoch  45 train loss is 5182.4684 and reconstruction losses are in train 1952.3449 and in test 1890.5943\n",
      "At epoch  46 train loss is 4856.7329 and reconstruction losses are in train 1884.3404 and in test 1827.9601\n",
      "At epoch  47 train loss is 4544.8886 and reconstruction losses are in train 1818.2401 and in test 1765.6343\n",
      "At epoch  48 train loss is 5929.0244 and reconstruction losses are in train 1813.0950 and in test 1756.9969\n",
      "At epoch  49 train loss is 6157.5910 and reconstruction losses are in train 1788.4743 and in test 1736.5590\n",
      "At epoch  50 train loss is 5946.2507 and reconstruction losses are in train 1860.4040 and in test 1803.7623\n",
      "At epoch  51 train loss is 5730.3193 and reconstruction losses are in train 1881.8876 and in test 1824.5019\n",
      "At epoch  52 train loss is 5609.5583 and reconstruction losses are in train 1807.1120 and in test 1755.8785\n",
      "At epoch  53 train loss is 5494.3110 and reconstruction losses are in train 1836.1557 and in test 1783.3175\n",
      "At epoch  54 train loss is 4777.6838 and reconstruction losses are in train 1861.0182 and in test 1803.2052\n",
      "At epoch  55 train loss is 4939.0785 and reconstruction losses are in train 1835.8122 and in test 1781.0151\n",
      "At epoch  56 train loss is 5327.6075 and reconstruction losses are in train 1799.9061 and in test 1749.1387\n",
      "At epoch  57 train loss is 5073.3818 and reconstruction losses are in train 1788.7345 and in test 1741.2529\n",
      "At epoch  58 train loss is 4423.9016 and reconstruction losses are in train 1868.8223 and in test 1815.4845\n",
      "At epoch  59 train loss is 7084.0093 and reconstruction losses are in train 1803.6915 and in test 1755.5727\n",
      "At epoch  60 train loss is 5685.0329 and reconstruction losses are in train 1847.0784 and in test 1793.9228\n",
      "At epoch  61 train loss is 5460.3350 and reconstruction losses are in train 1857.4505 and in test 1803.7659\n",
      "At epoch  62 train loss is 5342.0841 and reconstruction losses are in train 1800.5818 and in test 1753.3199\n",
      "At epoch  63 train loss is 5970.2096 and reconstruction losses are in train 1829.1476 and in test 1781.2394\n",
      "At epoch  64 train loss is 6524.5755 and reconstruction losses are in train 1784.5097 and in test 1743.8955\n",
      "At epoch  65 train loss is 6416.1912 and reconstruction losses are in train 1833.5695 and in test 1788.0580\n",
      "At epoch  66 train loss is 4575.1334 and reconstruction losses are in train 1879.6569 and in test 1829.8927\n",
      "At epoch  67 train loss is 3654.1568 and reconstruction losses are in train 1913.0824 and in test 1860.3211\n",
      "At epoch  68 train loss is 6342.0800 and reconstruction losses are in train 1893.5466 and in test 1841.5691\n",
      "At epoch  69 train loss is 5613.5164 and reconstruction losses are in train 1835.6631 and in test 1786.2110\n",
      "At epoch  70 train loss is 3520.8664 and reconstruction losses are in train 1865.9772 and in test 1813.0335\n",
      "At epoch  71 train loss is 4227.6812 and reconstruction losses are in train 1865.7436 and in test 1810.5923\n",
      "At epoch  72 train loss is 6833.3193 and reconstruction losses are in train 1814.7851 and in test 1765.7906\n",
      "At epoch  73 train loss is 6380.4348 and reconstruction losses are in train 1809.9631 and in test 1765.4474\n",
      "At epoch  74 train loss is 4645.6492 and reconstruction losses are in train 1884.7522 and in test 1833.8123\n",
      "At epoch  75 train loss is 6189.5285 and reconstruction losses are in train 1848.4526 and in test 1801.0863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch  76 train loss is 5250.6316 and reconstruction losses are in train 1875.3577 and in test 1826.0204\n",
      "At epoch  77 train loss is 5713.5769 and reconstruction losses are in train 1894.4347 and in test 1843.4181\n",
      "At epoch  78 train loss is 4707.8770 and reconstruction losses are in train 1888.2917 and in test 1839.9497\n",
      "At epoch  79 train loss is 6113.9203 and reconstruction losses are in train 1802.2945 and in test 1762.1906\n",
      "At epoch  80 train loss is 4800.9426 and reconstruction losses are in train 1862.2154 and in test 1816.4759\n",
      "At epoch  81 train loss is 4945.9267 and reconstruction losses are in train 1910.2515 and in test 1861.4554\n",
      "At epoch  82 train loss is 4414.2223 and reconstruction losses are in train 1896.8225 and in test 1848.6236\n",
      "At epoch  83 train loss is 6249.7122 and reconstruction losses are in train 1887.2048 and in test 1839.5927\n",
      "At epoch  84 train loss is 5618.8539 and reconstruction losses are in train 1877.0052 and in test 1828.2058\n",
      "At epoch  85 train loss is 4807.4470 and reconstruction losses are in train 1890.2435 and in test 1839.1138\n",
      "At epoch  86 train loss is 6991.3809 and reconstruction losses are in train 1853.3803 and in test 1804.8289\n",
      "At epoch  87 train loss is 6303.8145 and reconstruction losses are in train 1845.0169 and in test 1797.5421\n",
      "At epoch  88 train loss is 4588.9588 and reconstruction losses are in train 1906.2720 and in test 1855.9482\n",
      "At epoch  89 train loss is 5177.3296 and reconstruction losses are in train 1909.3796 and in test 1858.4436\n",
      "At epoch  90 train loss is 3792.4306 and reconstruction losses are in train 1899.5647 and in test 1848.0377\n",
      "At epoch  91 train loss is 4794.6249 and reconstruction losses are in train 1852.3021 and in test 1802.5510\n",
      "At epoch  92 train loss is 5142.0241 and reconstruction losses are in train 1832.2387 and in test 1785.1263\n",
      "At epoch  93 train loss is 5774.9806 and reconstruction losses are in train 1767.7264 and in test 1726.2491\n",
      "At epoch  94 train loss is 4210.1719 and reconstruction losses are in train 1841.2574 and in test 1792.3116\n",
      "At epoch  95 train loss is 6292.5584 and reconstruction losses are in train 1783.0574 and in test 1739.5438\n",
      "At epoch  96 train loss is 6681.0165 and reconstruction losses are in train 1822.6390 and in test 1775.8954\n",
      "At epoch  97 train loss is 5217.3333 and reconstruction losses are in train 1815.4263 and in test 1769.4817\n",
      "At epoch  98 train loss is 5725.2802 and reconstruction losses are in train 1801.0464 and in test 1757.4803\n",
      "At epoch  99 train loss is 7305.8401 and reconstruction losses are in train 1839.9453 and in test 1794.7531\n"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(data, data_mask, test_data=test_data, test_data_mask=test_data_mask, learning_rate=1,\n",
    "                print_every_epochs=1, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./autoencoder/model-99\n"
     ]
    }
   ],
   "source": [
    "reconstructed_data = autoencoder.reconstruct(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      , 12.866301,  3.      , ...,  4.      , 26.      ,\n",
       "        64.      ],\n",
       "       [ 0.      ,  7.784964,  1.      , ..., -1.      , -1.      ,\n",
       "        -1.      ],\n",
       "       [ 0.      , 10.18104 ,  1.      , ..., -1.      , -1.      ,\n",
       "        -1.      ],\n",
       "       ...,\n",
       "       [ 1.      ,  5.79717 ,  2.      , ..., -1.      , -1.      ,\n",
       "        -1.      ],\n",
       "       [ 1.      , 15.793634,  3.      , ..., 14.      , 17.      ,\n",
       "        56.      ],\n",
       "       [ 1.      ,  7.700547,  1.      , ..., -1.      , -1.      ,\n",
       "        -1.      ]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.copy()\n",
    "new_data[~data_mask] = reconstructed_data[~data_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 12.866301  ,  3.        , ...,  4.        ,\n",
       "        26.        , 64.        ],\n",
       "       [ 0.        ,  7.784964  ,  1.        , ...,  0.        ,\n",
       "        13.59239101, 43.98703766],\n",
       "       [ 0.        , 10.18104   ,  1.        , ...,  0.        ,\n",
       "        17.7919426 , 53.56770325],\n",
       "       ...,\n",
       "       [ 1.        ,  5.79717   ,  2.        , ...,  0.        ,\n",
       "         0.        ,  6.11753988],\n",
       "       [ 1.        , 15.793634  ,  3.        , ..., 14.        ,\n",
       "        17.        , 56.        ],\n",
       "       [ 1.        ,  7.700547  ,  1.        , ...,  0.        ,\n",
       "        13.49396229, 45.43925858]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  12,   3,  -1,  16,  90,  13, 107,   0,   0,  -1,   3,   0,\n",
       "        10,  20,  30,   1,   0,   1,  15,   3,   0,  -1,  -1,  -1,   3,\n",
       "         2,   7,   0,   1,  12,   1,   6,  19,  -1,  -1,  -1,  -1,  -1,\n",
       "        -1,  -1,  -1,  -1,  25,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   3,   2,  -1,  -1,  -1,\n",
       "        -1,  -1,  -1,  -1,  -1,  31,  18,  17,  19,  20,  -1,  70,  -1,\n",
       "        -1,  -1,   1,   1,   2,   1,  23, -48,  -1,  18,  10,   2,   5,\n",
       "         5,  29,  25,  -1,   0,   0,  -1,  23,   2,   5,   7,  15,   0,\n",
       "         3,  10,  14,  -1,  -1,  13,  17,  30,  20,  12,  35,  -1,   8,\n",
       "        25,  -1,  -1,  -1,  -1,  -1,   1,   2,  17,  -1,  -1,  -1,  -1,\n",
       "        -1,  -1,  -1,  -1,  21,  27,  25,  73,  -1,  -1,  -1,  -1,  -1,\n",
       "        -1,   0,   0,   0,   0,   0,   3,   3,   0,   1,   0,   7,   5,\n",
       "         8,   4,   0,   1,  18,   3,   6,   4,   4,   2,  10,   4,   3,\n",
       "         8,   7,   2,  37,   7,   4,   7,  43,   3,   1,  21,  22,  -1,\n",
       "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,   0,   0,\n",
       "       105,   0, 149,  35,  68,  -1,  -1,  -1,  -1,  -1,  -1,  17,  24,\n",
       "         1,  12,  14,   1,   8,   6,   9,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "        -1,  -1,  -1,  -1,  -1,   0,   2,  42,  31,  31,  65,  12,  19,\n",
       "        42,  56,  41,  34,  19,  27,  31,  33,  25,  25,  37,  22, 126,\n",
       "       108, 100, 110,  75, 100,   4,  13,   7,   0,   6,   4,  26,  64])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  12,   3,   0,  16,  90,  13, 107,   0,   0,  65,   3,   0,\n",
       "        10,  20,  30,   1,   0,   1,  15,   3,   0,   0,   0,   0,   3,\n",
       "         2,   7,   0,   1,  12,   1,   6,  19,   0,   0,   0,   0,  16,\n",
       "         0,   0,  36,  21,  25,   0,  20,  25,   0,   0,  19,  40,   0,\n",
       "         0,   0,   8,   3,   4,   0,   9,  83,   3,   2,   7,   0,   6,\n",
       "         2,   6,   6,   5,   0,  31,  18,  17,  19,  20,  74,  70,   0,\n",
       "         4,   3,   1,   1,   2,   1,  23, -48,  92,  18,  10,   2,   5,\n",
       "         5,  29,  25,   0,   0,   0,   0,  23,   2,   5,   7,  15,   0,\n",
       "         3,  10,  14,  10,   0,  13,  17,  30,  20,  12,  35,   0,   8,\n",
       "        25,   0,   0,   0,   0,   0,   1,   2,  17, 129,  84,   0,   0,\n",
       "         0,  90,   0, 130,  21,  27,  25,  73,  45,  72,   0,  73,   0,\n",
       "        43,   0,   0,   0,   0,   0,   3,   3,   0,   1,   0,   7,   5,\n",
       "         8,   4,   0,   1,  18,   3,   6,   4,   4,   2,  10,   4,   3,\n",
       "         8,   7,   2,  37,   7,   4,   7,  43,   3,   1,  21,  22,  30,\n",
       "        37,  34,   0,   0,   0,  65,  90,   0,  42,   0,   0,   0,   0,\n",
       "       105,   0, 149,  35,  68,  18,   0,   6,   0,   0,   0,  17,  24,\n",
       "         1,  12,  14,   1,   8,   6,   9,  36,  81,   7,   0,  51,  22,\n",
       "        40,  30,  53,   0,  13,  21,   0,   0,  35,  17,  45,   7,   9,\n",
       "        33,  38,   0,  69,  98,   0,   2,  42,  31,  31,  65,  12,  19,\n",
       "        42,  56,  41,  34,  19,  27,  31,  33,  25,  25,  37,  22, 126,\n",
       "       108, 100, 110,  75, 100,   4,  13,   7,   0,   6,   4,  26,  64])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
