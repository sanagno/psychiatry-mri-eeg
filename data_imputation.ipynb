{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-KN0N1dlHBz"
   },
   "source": [
    "## Imputations methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0tMrgNTlHB2"
   },
   "source": [
    "In this notebook we are going to explore different imputation methods\n",
    "\n",
    "For a more theoretical perspective consider http://www.stat.columbia.edu/~gelman/arm/missing.pdf\n",
    "\n",
    "We are dealing with data probably belonging to the category missing by design (some values are too expensive to obtain)\n",
    "\n",
    "Also look at https://www.paultwin.com/wp-content/uploads/Lodder_1140873_Paper_Imputation.pdf\n",
    "\n",
    "TODO: also take into account more complicated methods that directly tackle the problem of classification such as BoostClean https://arxiv.org/pdf/1711.01299.pdf\n",
    "TODO: also try EM imputation\n",
    "\n",
    "For a more detail description check the book \"Flexible imputationn of missing data\" https://stefvanbuuren.name/fimd/\n",
    "\n",
    "\n",
    "For discriminative models it is more elaborate, since that is not possible. There are a number of approaches. Gharamani and Jordan http://mlg.eng.cam.ac.uk/zoubin/papers/nips93.pdf describe a principled approach, where missing values are treated like hidden variables, and a variant of the EM algorithm is used to estimate them. In a similar fashion, Smola et al. http://www.gatsby.ucl.ac.uk/aistats/fullpapers/234.pdf describe a variant of the SVM algorithm which explicitly tackles the problem.\n",
    "\n",
    "In out case the missingness of values is not random but can be an indicator by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0em19iElHB3"
   },
   "source": [
    "First we need to define a little clearer the setting exactly. For startes we are going to deal with only the 5 most common diseases. Also for starters we are merely going to deal with classifying based on the first diagnosis given only. We are going to consider the multiclass classification problem first, but we are should also check the binary classification one to compare with established methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBAn7-9xlHB4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JdGIzy5FlHB9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 282 patients as their evaluation was incomplete.\n",
      "Removing 37 patients as their diagnoses were very uncommon.\n"
     ]
    }
   ],
   "source": [
    "behaviour_data = pd.read_csv('./DataScience2019_MRI/Behavioral/cleaned/HBNFinalSummaries.csv', low_memory=False)\n",
    "\n",
    "initial_size = behaviour_data.shape[0]\n",
    "behaviour_data = behaviour_data[behaviour_data['NoDX'].isin(['Yes', 'No'])]\n",
    "new_size = behaviour_data.shape[0]\n",
    "print('Removing', initial_size - new_size, 'patients as their evaluation was incomplete.')\n",
    "\n",
    "keep_most_common_diseases = 5\n",
    "healthy_diagnosis = 'No Diagnosis Given'\n",
    "\n",
    "# these disorders should also include the no diagnosis given option\n",
    "keep_most_common_diseases += 1\n",
    "\n",
    "category_columns = ['DX_' + str(i).zfill(2) + '_Cat' for i in range(1, 11)]\n",
    "\n",
    "# count for each disorder number of occurences\n",
    "disorder_counts = {}\n",
    "for val in behaviour_data[category_columns].values.reshape(-1):\n",
    "    if not pd.isnull(val):\n",
    "        if val in disorder_counts:\n",
    "            disorder_counts[val] += 1\n",
    "        else:\n",
    "            disorder_counts[val] = 1\n",
    "            \n",
    "# sort in descending order\n",
    "disorder_counts = sorted(disorder_counts.items(), key=lambda kv: -kv[1])\n",
    "\n",
    "most_common_disorders = [x[0] for x in disorder_counts[:keep_most_common_diseases]]\n",
    "\n",
    "# find users that have no diagnosis within these top diseases \n",
    "# filtering should cahnge anything as this should also happen at a later stage\n",
    "mask = None\n",
    "for col in category_columns:\n",
    "    mask_col = behaviour_data[col].isin(most_common_disorders)\n",
    "    if mask is None:\n",
    "        mask = mask_col\n",
    "    else:\n",
    "        mask = mask | mask_col\n",
    "    \n",
    "initial_size = behaviour_data.shape[0]\n",
    "behaviour_data = behaviour_data[mask]\n",
    "behaviour_data = behaviour_data.reset_index(drop=True)\n",
    "new_size = behaviour_data.shape[0]\n",
    "print('Removing', initial_size - new_size, 'patients as their diagnoses were very uncommon.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1777, 447)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviour_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dvPMM3uqlHCA",
    "outputId": "f6b00027-9ada-497c-9576-3230dc02b8bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neurodevelopmental Disorders',\n",
       " 'Anxiety Disorders',\n",
       " 'Disruptive',\n",
       " 'No Diagnosis Given',\n",
       " 'Depressive Disorders',\n",
       " 'Elimination Disorders']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_diagnosis_given = 'No Diagnosis Given'\n",
    "most_common_disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtO43CsSlHCE"
   },
   "outputs": [],
   "source": [
    "classes = np.zeros((len(most_common_disorders), behaviour_data.shape[0]), dtype=np.int32)\n",
    "\n",
    "\n",
    "df_disorders = behaviour_data[category_columns]\n",
    "\n",
    "for i, disorder in enumerate(most_common_disorders):\n",
    "    mask = df_disorders.select_dtypes(include=[object]). \\\n",
    "            applymap(lambda x: disorder in x if pd.notnull(x) else False)\n",
    "    \n",
    "    disorder_df = df_disorders[mask.any(axis=1)]\n",
    "    \n",
    "    np.add.at(classes[i], disorder_df.index.values, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xH0pCLrWlHCI"
   },
   "source": [
    "Now we can safely remove previous columns describing diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ILsKEZIlHCJ"
   },
   "outputs": [],
   "source": [
    "behaviour_data_columns = behaviour_data.columns.values.astype(np.str)\n",
    "\n",
    "columns_to_drop = behaviour_data_columns[\n",
    "    np.flatnonzero(np.core.defchararray.find(behaviour_data_columns, 'DX')!=-1)]\n",
    "\n",
    "behaviour_data = behaviour_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHYEGLbqlHCM"
   },
   "outputs": [],
   "source": [
    "for disorder, classification in zip(most_common_disorders, classes):\n",
    "    behaviour_data[disorder] = classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jx9v9TH7lHCP"
   },
   "source": [
    "It is also reasonable to assume that we need to drop columns with too many Nans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8yk7ujzlHCP"
   },
   "outputs": [],
   "source": [
    "# threshold = 0.8\n",
    "\n",
    "# columns_mask = pd.isnull(behaviour_data).sum() / behaviour_data.shape[0] > threshold\n",
    "\n",
    "# print('Droping this many columns:', np.sum(columns_mask))\n",
    "\n",
    "# dropped_columns = behaviour_data.columns[columns_mask]\n",
    "\n",
    "# behaviour_data = behaviour_data.drop(columns=dropped_columns)\n",
    "# behaviour_data = behaviour_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "jE7DVI7WlHCS",
    "outputId": "19ec83d3-d6c9-4acb-b1bb-ca60504ad3b7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (1760, 312)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anonymized.ID</th>\n",
       "      <th>EID</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Study.Site</th>\n",
       "      <th>ACE_Score</th>\n",
       "      <th>APQ_P_OPD</th>\n",
       "      <th>APQ_P_Total</th>\n",
       "      <th>APQ_SR_OPD</th>\n",
       "      <th>APQ_SR_Total</th>\n",
       "      <th>ARI_P_Total_Score</th>\n",
       "      <th>ARI_S_Total_Score</th>\n",
       "      <th>ASR_Total</th>\n",
       "      <th>ASSQ_Total</th>\n",
       "      <th>AUDIT_Total_Score</th>\n",
       "      <th>Barratt_Total_Edu</th>\n",
       "      <th>Barratt_Total_Occ</th>\n",
       "      <th>Barratt_Total</th>\n",
       "      <th>C3SR_AG</th>\n",
       "      <th>C3SR_FR</th>\n",
       "      <th>C3SR_HY</th>\n",
       "      <th>C3SR_IN</th>\n",
       "      <th>C3SR_NI</th>\n",
       "      <th>C3SR_PI</th>\n",
       "      <th>CAARS_HR_Raw</th>\n",
       "      <th>CAARS_IE_Raw</th>\n",
       "      <th>CAARS_ADHD_Raw</th>\n",
       "      <th>CBCL_AD</th>\n",
       "      <th>CBCL_WD</th>\n",
       "      <th>CBCL_SC</th>\n",
       "      <th>CBCL_RBB</th>\n",
       "      <th>CBCL_AB</th>\n",
       "      <th>CBCL_Int</th>\n",
       "      <th>CBCL_Ext</th>\n",
       "      <th>CBCL_C</th>\n",
       "      <th>CBCL_Total</th>\n",
       "      <th>CBCL_Pre_AB</th>\n",
       "      <th>CBCL_Pre_AD</th>\n",
       "      <th>CBCL_Pre_SC</th>\n",
       "      <th>CBCL_Pre_WD</th>\n",
       "      <th>...</th>\n",
       "      <th>WHODAS_P_Score</th>\n",
       "      <th>WHODAS_SR_Score</th>\n",
       "      <th>WIAT_Num_Raw</th>\n",
       "      <th>WIAT_Pseudo_Raw</th>\n",
       "      <th>WIAT_Spell_Raw</th>\n",
       "      <th>WIAT_Word_Raw</th>\n",
       "      <th>WIAT_LCRV_Raw</th>\n",
       "      <th>WIAT_LCODC_Raw</th>\n",
       "      <th>WIAT_RC_Raw</th>\n",
       "      <th>WIAT_MP_Raw</th>\n",
       "      <th>WISC_BD_Raw</th>\n",
       "      <th>WISC_Similarities_Raw</th>\n",
       "      <th>WISC_MR_Raw</th>\n",
       "      <th>WISC_DS_Raw</th>\n",
       "      <th>WISC_Coding_Raw</th>\n",
       "      <th>WISC_Vocab_Raw</th>\n",
       "      <th>WISC_FW_Raw</th>\n",
       "      <th>WISC_VP_Raw</th>\n",
       "      <th>WISC_PS_Raw</th>\n",
       "      <th>WISC_SS_Raw</th>\n",
       "      <th>WISC_VSI</th>\n",
       "      <th>WISC_VCI</th>\n",
       "      <th>WISC_FRI</th>\n",
       "      <th>WISC_WMI</th>\n",
       "      <th>WISC_PSI</th>\n",
       "      <th>WISC_FSIQ</th>\n",
       "      <th>YSR_AB</th>\n",
       "      <th>YSR_AD</th>\n",
       "      <th>YSR_WD</th>\n",
       "      <th>YSR_RBB</th>\n",
       "      <th>YSR_SC</th>\n",
       "      <th>YSR_Ext</th>\n",
       "      <th>YSR_Int</th>\n",
       "      <th>YSR_Total</th>\n",
       "      <th>Neurodevelopmental Disorders</th>\n",
       "      <th>No Diagnosis Given</th>\n",
       "      <th>Anxiety Disorders</th>\n",
       "      <th>Depressive Disorders</th>\n",
       "      <th>Disruptive</th>\n",
       "      <th>Trauma and Stressor Related Disorders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00078864</td>\n",
       "      <td>NDARYM832PX3</td>\n",
       "      <td>1</td>\n",
       "      <td>7.048254</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00078865</td>\n",
       "      <td>NDARNJ687DMC</td>\n",
       "      <td>1</td>\n",
       "      <td>6.348163</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A00078866</td>\n",
       "      <td>NDARRM363BXZ</td>\n",
       "      <td>0</td>\n",
       "      <td>10.052589</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A00078867</td>\n",
       "      <td>NDARUW586LLL</td>\n",
       "      <td>1</td>\n",
       "      <td>12.319415</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A00078868</td>\n",
       "      <td>NDARDC298NW4</td>\n",
       "      <td>0</td>\n",
       "      <td>13.901437</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Anonymized.ID  ... Trauma and Stressor Related Disorders\n",
       "0     A00078864  ...                                     0\n",
       "1     A00078865  ...                                     0\n",
       "2     A00078866  ...                                     0\n",
       "3     A00078867  ...                                     0\n",
       "4     A00078868  ...                                     0\n",
       "\n",
       "[5 rows x 312 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape', behaviour_data.shape)\n",
    "behaviour_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Brc4Ej64lHCW"
   },
   "source": [
    "We are going to explore some different imputation methods. More specifically:\n",
    "1. Do nothing\n",
    "2. Fill all missing values with a dummy value\n",
    "3. Imputation Using (Mean/Median/Most Frequent) Values\n",
    "4. Imputation Using k-NN, Randomforest\n",
    "5. MICE (TODO)\n",
    "6. Multiple imputer\n",
    "7. Add features based on whether the value exists or not\n",
    "\n",
    "For a comparison with a more naive baseline check out @gvasilako 's notebook for both multilabel and per class metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzardwD2lgs5"
   },
   "outputs": [],
   "source": [
    "!pip install missingpy impyute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAaEFpailHCX"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import xgboost as xgb\n",
    "from missingpy import KNNImputer, MissForest\n",
    "import impyute as impy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import UndefinedMetricWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31GgQrw-lHCa"
   },
   "outputs": [],
   "source": [
    "# fdx and mdx may contain 'No Diagnosis'\n",
    "# drop them for now but they may be important\n",
    "# they correspond to father's and mother's primary diagnosis\n",
    "columns_to_drop = ['Anonymized.ID', 'EID', 'mdx', 'fdx', 'fcodxm_1', 'fcodxm_2', 'fcodxm_3', 'mcodxm_1',\n",
    "                   'mcodxm_2', 'mcodxm_3', 'mcodxmdt', 'TOWRE_Total_Desc', 'Picture_Vocab_Raw',\n",
    "                   'sib1dx', 'sib1codxm_1', 'sib1codxm_2', 'sib1codxm_3',\n",
    "                   'sib2dx', 'sib2codxm_1', 'sib2codxm_2', 'sib2codxm_3',\n",
    "                   'sib3dx', 'sib3codxm_1', 'sib3codxm_2', 'sib3codxm_3',\n",
    "                   'sib4dx', 'sib4codxm_1', 'sib4codxm_2', 'sib4codxm_3',\n",
    "                   'sib5dx', 'sib5codxm_1', 'sib5codxm_2', 'sib5codxm_3']\n",
    "\n",
    "processed = behaviour_data.drop(columns=columns_to_drop)\n",
    "most_common_disorders = list(most_common_disorders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcWqyFoTlHCd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "    \n",
    "def run_binary_classification(dataset, diseases_to_run_for, clf, imputer, imputer_requires_disorder, \n",
    "                              drop_missing_threshold, *args):\n",
    "    \n",
    "    conf_matrices = []\n",
    "    for check_disorder in most_common_disorders[:diseases_to_run_for]:\n",
    "        \n",
    "        if check_disorder == no_diagnosis_given:\n",
    "            continue\n",
    "            \n",
    "        # only include patients that have that particular disease vs patients that are healthy\n",
    "        temp = dataset[(dataset[check_disorder] == 1) | (dataset[healthy_diagnosis] == 1)]\n",
    "        \n",
    "        pos = most_common_disorders.index(check_disorder)\n",
    "\n",
    "        columns_to_drop = most_common_disorders[:pos] + most_common_disorders[(pos + 1):]\n",
    "        temp = temp.drop(columns=columns_to_drop)\n",
    "        \n",
    "        if drop_missing_threshold is not None:\n",
    "            # drop features missing in drop_missing_threshold percent of the time or more\n",
    "            # The missingpy algorithms to work require that not columsn have more than 80% missing values\n",
    "\n",
    "            columns_mask = pd.isnull(temp).sum() / temp.shape[0] > drop_missing_threshold\n",
    "\n",
    "            print('Droping this many columns:', np.sum(columns_mask))\n",
    "\n",
    "            dropped_columns = temp.columns[columns_mask]\n",
    "\n",
    "            temp = temp.drop(columns=dropped_columns)\n",
    "            \n",
    "        # drop columns that have only Nan values\n",
    "        temp = temp.dropna(axis=1, how='all')\n",
    "\n",
    "        kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        kf.get_n_splits(temp)\n",
    "\n",
    "        preds = np.zeros(temp.shape[0])\n",
    "\n",
    "        for train_index, test_index in kf.split(temp):\n",
    "            train, test = temp.iloc[train_index], temp.iloc[test_index]   \n",
    "\n",
    "            all_columns = train.columns.values\n",
    "            filtered_columns = train.dropna(axis=1, how='all').columns.values\n",
    "\n",
    "            columns_to_drop = list(set(all_columns) - set(filtered_columns))         \n",
    "            train = train.drop(columns=columns_to_drop)\n",
    "            test = test.drop(columns=columns_to_drop)\n",
    "            \n",
    "            if imputer_requires_disorder:\n",
    "                train, test = imputer(train, test, check_disorder, *args)\n",
    "            else:\n",
    "                train, test = imputer(train, test, *args)\n",
    "            \n",
    "            clf.fit(train.drop(columns=[check_disorder]), train[check_disorder])\n",
    "            preds[test_index] = clf.predict(test.drop(columns=[check_disorder]))\n",
    "\n",
    "        print('================================= {0} ================================='.format(check_disorder))\n",
    "\n",
    "        y_true = temp[check_disorder]\n",
    "        precision, recall, _, _ = precision_recall_fscore_support(y_true, preds)\n",
    "        accuracy = accuracy_score(y_true, preds)\n",
    "\n",
    "        print('accuracy {:.3f} precision {:.3f} {:.3f} recall {:.3f} {:.3f}' \\\n",
    "              .format(accuracy, precision[0], precision[1], recall[0], recall[1]))\n",
    "\n",
    "        conf = confusion_matrix(y_true, preds)\n",
    "        print(conf)\n",
    "        \n",
    "        conf_matrices.append(conf)\n",
    "        \n",
    "    return conf_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LieoXkGolHCh"
   },
   "source": [
    "Before actually droping columns that cannot be incorporated for the next analysis as they are categorical values, we should check their correlation with the predicted classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "SeUKxklulHCi",
    "outputId": "e1fdbdee-f78c-44b9-d8db-51399b1ead1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "disorder_corr = np.zeros((len(columns_to_drop) - 2, len(most_common_disorders)))\n",
    "\n",
    "# the disorder to find the correlation for\n",
    "for index, disorder in enumerate(most_common_disorders):\n",
    "    dropped_columns_dataset = behaviour_data[columns_to_drop + [most_common_disorders[index]]]\n",
    "\n",
    "    # remove the anonymized id and the EID\n",
    "    for col in dropped_columns_dataset[2:-1]:\n",
    "        dropped_columns_dataset[col] = dropped_columns_dataset[col].astype('category').cat.codes\n",
    "\n",
    "    disorder_corr[:, index] = dropped_columns_dataset[dropped_columns_dataset.columns[2:]].corr()[most_common_disorders[index]][:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "HRam24hflHCl",
    "outputId": "daf23b69-4154-4d6f-9ceb-025fe901a35e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6cAAAGoCAYAAAC61ZOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhQM7vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "correlation_df = pd.DataFrame(disorder_corr.T, columns=columns_to_drop[2:], index=most_common_disorders)\n",
    "sns.heatmap(correlation_df)\n",
    "\n",
    "plt.title('Correlation between disorders and discarded categorical features')\n",
    "plt.ylim([len(most_common_disorders), 0])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwzRM_RyFXrX"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(class_weight='balanced', n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1L1Vx_ilHCo"
   },
   "source": [
    "### 1st approach: Do nothing\n",
    "\n",
    "Some algorithms such as XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uyrCNg6tlHCp",
    "outputId": "d6481be1-5cd9-4acb-9e6d-e4d96037a697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.875 precision 0.614 0.903 recall 0.403 0.956\n",
      "[[ 27  40]\n",
      " [ 17 372]]\n"
     ]
    }
   ],
   "source": [
    "def do_nothing(x, y):\n",
    "    return x, y\n",
    "\n",
    "run_binary_classification(processed, 1, xgb.XGBClassifier(), do_nothing, False, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIDOiLUxlHCw"
   },
   "source": [
    "### 2nd approach: Fill all missing values with a dummy value \n",
    "\n",
    "(For this refer to @gvasilak 's notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7t4j92kZlHCy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3R_gvdrlHC1"
   },
   "source": [
    "### 3rd approach: Imputation Using (Mean/Median/Most_frequent) Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQakmM9OlHC2"
   },
   "outputs": [],
   "source": [
    "strategy = 'mean'\n",
    "assert strategy in ['mean', 'median', 'most_frequent']\n",
    "\n",
    "def imputer(x, y, strategy):\n",
    "    if strategy == 'mean':\n",
    "        filling = x.mean()\n",
    "    elif strategy == 'median':\n",
    "        filling = x.median()\n",
    "    elif strategy == 'most_frequent':\n",
    "        filling = x.mode().iloc[0]\n",
    "    \n",
    "    return x.fillna(filling), y.fillna(filling)\n",
    "\n",
    "all_matrices_mean = []\n",
    "for drop_values in np.arange(1, 0, -0.1):\n",
    "    mean_matrices = run_binary_classification(processed, len(most_common_disorders), clf, imputer, False, drop_values, strategy)\n",
    "    all_matrices_mean.append(mean_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tBE2_AjlHC5"
   },
   "source": [
    "### 4th approach: Imputation Using k-NN\n",
    "\n",
    "The algorithm uses â€˜feature similarityâ€™ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAPzYZfulHC8",
    "outputId": "950adfbb-5a70-4b90-e51d-1d3cce89415d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping this many columns: 95\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.8\n",
    "\n",
    "columns_mask = pd.isnull(processed).sum() / processed.shape[0] > threshold\n",
    "\n",
    "print('Droping this many columns:', np.sum(columns_mask))\n",
    "\n",
    "dropped_columns = processed.columns[columns_mask]\n",
    "\n",
    "processed = processed.drop(columns=dropped_columns)\n",
    "processed_80 = processed_80.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VO0LR6XLlHDB",
    "outputId": "cfe3cb88-bf2d-495f-e8d3-956cfde08a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping this many columns: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sotiris/.local/lib/python3.7/site-packages/missingpy/knnimpute.py:224: UserWarning: There are rows with more than 50.0% missing values. These rows are not included as donor neighbors.\n",
      "  .format(self.row_max_missing * 100))\n",
      "/home/sotiris/.local/lib/python3.7/site-packages/missingpy/knnimpute.py:282: UserWarning: There are rows with more than 50.0% missing values. The missing features in these rows are imputed with column means.\n",
      "  .format(self.row_max_missing * 100))\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "/home/sotiris/.local/lib/python3.7/site-packages/missingpy/utils.py:124: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/home/sotiris/.local/lib/python3.7/site-packages/missingpy/knnimpute.py:282: UserWarning: There are rows with more than 50.0% missing values. The missing features in these rows are imputed with column means.\n",
      "  .format(self.row_max_missing * 100))\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib64/python3.7/site-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib64/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.879 precision 0.650 0.901 recall 0.388 0.964\n",
      "[[ 26  41]\n",
      " [ 14 375]]\n"
     ]
    }
   ],
   "source": [
    "def imputer_fun(x, y, my_imputer):\n",
    "    x_new = my_inputer.fit_transform(x)\n",
    "    y_new = my_inputer.transform(y)\n",
    "\n",
    "    x[x.columns] = x_new\n",
    "    y[y.columns] = y_new\n",
    "    return x, y\n",
    "\n",
    "my_inputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "\n",
    "run_binary_classification(processed_80, 1, clf, imputer_fun, False, 0.8, my_inputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCcM_O93lHDE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def imputer_fun(x, y, my_imputer):\n",
    "    x_new = my_inputer.fit_transform(x)\n",
    "    y_new = my_inputer.transform(y)\n",
    "\n",
    "    x[x.columns] = x_new\n",
    "    y[y.columns] = y_new\n",
    "    return x, y\n",
    "\n",
    "my_inputer = MissForest(max_depth=4)\n",
    "\n",
    "run_binary_classification(processed_80, 1, clf, imputer_fun, False, 0.8, my_inputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZDQU4LQlHDG"
   },
   "source": [
    "### 5th approach: MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZcoGwSxlHDH"
   },
   "source": [
    "First we can try using EM to complete the missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzNAu6qmlHDI"
   },
   "outputs": [],
   "source": [
    "def imputer_em(x, y, disorder):\n",
    "    x_values = x.drop(columns=disorder).values\n",
    "    y_values = y.drop(columns=disorder).values\n",
    "    total_values = np.concatenate((x_values, y_values), axis=0)\n",
    "\n",
    "    total_values = impy.em(total_values)\n",
    "\n",
    "    x.loc[:, x.columns != disorder] = total_values[:x.shape[0]]\n",
    "    y.loc[:, y.columns != disorder] = total_values[len(total_values) - y.shape[0]:]\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "run_binary_classification(processed_80, 1, clf, imputer_em, True, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5V-vn5klHDL"
   },
   "source": [
    "One of the most popular methods is using MICE. What is the difference between MICE and other multiple imputers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kazSYTcvlHDM"
   },
   "outputs": [],
   "source": [
    "from fancyimpute import MICE as MICE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HKHY2pflHDO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3idaXvnlHDR"
   },
   "source": [
    "## 6th approach: Multiple imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPKE9OUBlHDS"
   },
   "outputs": [],
   "source": [
    "from autoimpute.imputations import SingleImputer, MultipleImputer\n",
    "\n",
    "def imputer_fun(x, y, my_imputer):\n",
    "    x_new = my_inputer.fit_transform(x)\n",
    "    y_new = my_inputer.transform(y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "\n",
    "si = SingleImputer() # imputation methods, passing through the data once\n",
    "mi = MultipleImputer() # imputation methods, passing through the data multiple times\n",
    "\n",
    "run_binary_classification(processed, 1, clf, imputer_fun, False, None, mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOmAkwNTlHDU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-ePuS6clHDV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQ_4u13zlHDX"
   },
   "source": [
    "### 7th approach: Add features indicating if the value os misisng or not\n",
    "\n",
    "For this part we should take into account which exact features can have Nan values\n",
    "\n",
    "The features that do not take any null values are 'Sex', 'Age', 'Study.Site' and the disorder we are currently checking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixc9O6wjlHDY",
    "outputId": "21dc0c25-9269-4d76-87b3-0bb1eb413adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.820 precision 0.739 0.833 recall 0.405 0.954\n",
      "[[ 51  75]\n",
      " [ 18 374]]\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(x, cols):\n",
    "    x1 = x.drop(columns=cols).isna().astype('int32')\n",
    "    \n",
    "    # change column naming to enable the inner join\n",
    "    x1.columns = [col + '_existence' for col in x1.columns]\n",
    "    return x1.join(x)\n",
    "\n",
    "def data_and_existence_of_features(x, y, disorder):\n",
    "    cols = ['Sex', 'Age', 'Study.Site', disorder]\n",
    "    rest_of_columns = list(x.columns.values)\n",
    "    for c in cols:\n",
    "        rest_of_columns.remove(c)\n",
    "    \n",
    "    x_new = process_dataset(x, cols)\n",
    "    y_new = process_dataset(y, cols)\n",
    "    \n",
    "    x_new[rest_of_columns], y_new[rest_of_columns] = imputer(x_new[rest_of_columns], \n",
    "                                                             y_new[rest_of_columns], \n",
    "                                                             'most_frequent')\n",
    "    return x_new, y_new\n",
    "\n",
    "run_binary_classification(processed, 1, clf, data_and_existence_of_features, True, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gM7hJVI8lHDc"
   },
   "source": [
    "## Existence of features\n",
    "\n",
    "To explore the severity of existence of a feature for the correct classification, we only try to predict based on the existence of the features only and not the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eKPDx4MlHDd",
    "outputId": "d466ca54-6514-4257-8fa2-88e5b2937b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= Neurodevelopmental Disorders =================================\n",
      "accuracy 0.720 precision 0.398 0.791 recall 0.294 0.857\n",
      "[[ 37  89]\n",
      " [ 56 336]]\n"
     ]
    }
   ],
   "source": [
    "def process_dataset_only_existence(x, cols):\n",
    "    x1 = x.drop(columns=cols).isna().astype('int32')\n",
    "    x2 = x[cols]\n",
    "    \n",
    "    return x1.join(x2)\n",
    "\n",
    "def only_existence_of_features(x, y, disorder):\n",
    "    cols = ['Sex', 'Age', 'Study.Site', disorder]\n",
    "    return process_dataset_only_existence(x, cols), process_dataset_only_existence(y, cols)\n",
    "\n",
    "run_binary_classification(processed, 1, clf, only_existence_of_features, True, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_cjGYzxlHDg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbFQINOLlHDj"
   },
   "source": [
    "## Use an autoencoder as imputation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKv9t65iEjKv"
   },
   "source": [
    "Before using an autoencoder we should actually scale the data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kDm7aYvlHDn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "layers = [40, 20]\n",
    "learning_rate = 5\n",
    "masking=0\n",
    "dropout=0\n",
    "regularization = 10.\n",
    "n_epochs=300\n",
    "\n",
    "def imputer_autoencoder(x, y, disorder):\n",
    "    original_x_labels = x[disorder].values\n",
    "    original_y_labels = y[disorder].values\n",
    "\n",
    "    data = x.values\n",
    "    test_data = y.values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    test_data = scaler.fit_transform(test_data)\n",
    "\n",
    "    data_mask = ~np.isnan(data)\n",
    "    test_data_mask = ~np.isnan(test_data)\n",
    "\n",
    "    # fill all nan values with a dummy value\n",
    "    # when using relu activation any negative value should have the same effect\n",
    "    # also try using the mean value!!\n",
    "    data[~data_mask] = -1\n",
    "    test_data[~test_data_mask] = -1\n",
    "    \n",
    "    autoencoder = Autoencoder(data.shape[1], layers=layers, masking=masking, regularization=regularization, dropout=dropout)\n",
    "    autoencoder.fit(data, data_mask, learning_rate=learning_rate, print_every_epochs=1, n_epochs=n_epochs)\n",
    "    \n",
    "    reconstructed_data = autoencoder.reconstruct(data)\n",
    "    data[~data_mask] = reconstructed_data[~data_mask]\n",
    "    \n",
    "    reconstructed_test_data = autoencoder.reconstruct(test_data)\n",
    "    test_data[~test_data_mask] = reconstructed_test_data[~test_data_mask]\n",
    "    \n",
    "    x[x.columns] = data\n",
    "    y[y.columns] = test_data\n",
    "    x[disorder] = original_x_labels\n",
    "    y[disorder] = original_y_labels\n",
    "    return x, y\n",
    "\n",
    "all_matrices = []\n",
    "for drop_values in np.arange(1, 0, -0.1):\n",
    "# for _ in range(2):\n",
    "    print(drop_values)\n",
    "    conf_matrices = run_binary_classification(processed, len(most_common_disorders), clf, \n",
    "                                              imputer_autoencoder, True, drop_values)\n",
    "    \n",
    "    all_matrices.append(conf_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmHy-p0RlHDq"
   },
   "source": [
    "We can also make predictions based on solely the latent space. This does not seem to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXcg8PlVlHDt"
   },
   "outputs": [],
   "source": [
    "layers = [40, 20]\n",
    "learning_rate = 5\n",
    "masking=0\n",
    "dropout=0\n",
    "regularization = 10.\n",
    "n_epochs=300\n",
    "\n",
    "def imputer_autoencoder_latent(x, y, disorder):\n",
    "    original_x_labels = x[disorder].values\n",
    "    original_y_labels = y[disorder].values\n",
    "\n",
    "    data = x.values\n",
    "    test_data = y.values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    test_data = scaler.fit_transform(test_data)\n",
    "\n",
    "    data_mask = ~np.isnan(data)\n",
    "    test_data_mask = ~np.isnan(test_data)\n",
    "\n",
    "    # fill all nan values with a dummy value\n",
    "    # when using relu activation any negative value should have the same effect\n",
    "    # also try using the mean value!!\n",
    "    data[~data_mask] = -1\n",
    "    test_data[~test_data_mask] = -1\n",
    "    \n",
    "    autoencoder = Autoencoder(data.shape[1], layers=layers, masking=masking, regularization=regularization, dropout=dropout)\n",
    "    autoencoder.fit(data, data_mask, learning_rate=learning_rate, print_every_epochs=1, n_epochs=n_epochs)\n",
    "    \n",
    "    latent_data = autoencoder.get_latent_space(data)\n",
    "    latent_test_data = autoencoder.get_latent_space(test_data)\n",
    "    \n",
    "    data_new = np.concatenate((latent_data, np.expand_dims(original_x_labels, axis=1)), axis=1)\n",
    "    data_new = pd.DataFrame(data_new, columns=list(range(autoencoder.layers[-1])) + [disorder])    \n",
    "\n",
    "    data_test_new = np.concatenate((latent_test_data, np.expand_dims(original_y_labels, axis=1)), axis=1)\n",
    "    data_test_new = pd.DataFrame(data_test_new, columns=list(range(autoencoder.layers[-1])) + [disorder])    \n",
    "\n",
    "    return data_new, data_test_new\n",
    "\n",
    "all_matrices = []\n",
    "for drop_values in np.arange(1, 0, -0.1):\n",
    "    print(drop_values)\n",
    "    conf_matrices = run_binary_classification(processed, len(most_common_disorders), clf, \n",
    "                                              imputer_autoencoder_latent, True, drop_values)\n",
    "    \n",
    "    all_matrices.append(conf_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaxeTtJO4XHF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Py-Q50164X3h"
   },
   "source": [
    "## Generative model for imputation of missign values\n",
    "\n",
    "\n",
    "GAIN seems to be promising: http://medianetlab.ee.ucla.edu/papers/ICML_GAIN.pdf\n",
    "\n",
    "the introduction in this paper is excelent for different imputation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZaYmXRU_Nab"
   },
   "source": [
    "Here try to start also only with columns having values with a threshold regarding the nans in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4E--QT54XDg"
   },
   "outputs": [],
   "source": [
    "mask_nans = (np.sum(pd.isnull(processed)) > 0).values\n",
    "columns_with_nans = processed.columns[mask_nans]\n",
    "\n",
    "data_nans = processed[columns_with_nans].values\n",
    "data_nans = data_nans.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twLemwKD9UoN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. Mini batch size\n",
    "mb_size = 8\n",
    "\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "\n",
    "# 4. Loss Hyperparameters/ initial value = 10\n",
    "alpha = 10\n",
    "\n",
    "## SHOULD WE SPLIT INTO TRAIN AND TEST SET HERE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aop_bqb_9Uvq"
   },
   "outputs": [],
   "source": [
    "Dim = data_nans.shape[1]\n",
    "\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9WY9h3i9UzR"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_nans = scaler.fit_transform(data_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HU4cOElN9U2p"
   },
   "outputs": [],
   "source": [
    "missing = np.isnan(data_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHAff6v0CJnV"
   },
   "outputs": [],
   "source": [
    "data_nans[missing] = 0\n",
    "missing = 1 - missing.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yPeJjRLDPSq"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCQl_qdw9U5h"
   },
   "outputs": [],
   "source": [
    "# 1. Xavier Initialization Definition\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "    \n",
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C\n",
    "\n",
    "#%% GAIN Architecture   \n",
    "   \n",
    "#%% 1. Input Placeholders\n",
    "# 1.1. Data Vector\n",
    "X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "# 1.2. Mask Vector \n",
    "M = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "# 1.3. Hint vector\n",
    "H = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "# 1.4. X with missing values\n",
    "New_X = tf.placeholder(tf.float32, shape = [None, Dim])\n",
    "\n",
    "#%% 2. Discriminator\n",
    "D_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Hint as inputs\n",
    "D_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "D_W3 = tf.Variable(xavier_init([H_Dim2, Dim]))\n",
    "D_b3 = tf.Variable(tf.zeros(shape = [Dim]))       # Output is multi-variate\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "#%% 3. Generator\n",
    "G_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]))     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "G_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]))\n",
    "\n",
    "G_W3 = tf.Variable(xavier_init([H_Dim2, Dim]))\n",
    "G_b3 = tf.Variable(tf.zeros(shape = [Dim]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RBNQWWy-9t1"
   },
   "outputs": [],
   "source": [
    "#%% GAIN Function\n",
    "\n",
    "#%% 1. Generator\n",
    "def generator(new_x,m):\n",
    "    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
    "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) # [0,1] normalized Output\n",
    "    \n",
    "    return G_prob\n",
    "    \n",
    "#%% 2. Discriminator\n",
    "def discriminator(new_x, h):\n",
    "    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "    \n",
    "    return D_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6Z4D0SO-9yr"
   },
   "outputs": [],
   "source": [
    "#%% 3. Other functions\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx\n",
    "\n",
    "#%% Structure\n",
    "# Generator\n",
    "G_sample = generator(New_X,M)\n",
    "\n",
    "# Combine with original data\n",
    "Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "# Discriminator\n",
    "D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "#%% Loss\n",
    "D_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "G_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "MSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "\n",
    "D_loss = D_loss1\n",
    "G_loss = G_loss1 + alpha * MSE_train_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dET4vO9G-92y"
   },
   "outputs": [],
   "source": [
    "#%% Solver\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0__8nio-95q"
   },
   "outputs": [],
   "source": [
    "#%% Iterations\n",
    "#%% Start Iterations\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_batches(iterable, batch_size=16, do_shuffle=True):\n",
    "    if do_shuffle:\n",
    "        iterable = shuffle(iterable)\n",
    "\n",
    "    length = len(iterable)\n",
    "    for ndx in range(0, length, batch_size):\n",
    "        iterable_batch = iterable[ndx: min(ndx + batch_size, length)]\n",
    "        yield iterable_batch\n",
    "\n",
    "print_every_epochs = 20\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_epochs = 300\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for mb_idx in get_batches(range(data_nans.shape[0]), batch_size=mb_size):\n",
    "        #%% Inputs\n",
    "        X_mb = data_nans[mb_idx,:]  \n",
    "        \n",
    "        Z_mb = sample_Z(len(mb_idx), Dim) \n",
    "        M_mb = missing[mb_idx,:]  \n",
    "        H_mb1 = sample_M(len(mb_idx), Dim, 1-p_hint)\n",
    "        H_mb = M_mb * H_mb1\n",
    "\n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "        _, G_loss_curr, MSE_train_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss],\n",
    "                                                                        feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "            \n",
    "    #%% Intermediate Losses\n",
    "    if epoch % print_every_epochs == 0:\n",
    "        print('Epoch: {:3d} train_loss {:.4}'.format(epoch, np.sqrt(MSE_train_loss_curr)))\n",
    "\n",
    "data_reconstructed = np.zeros(data_nans.shape)\n",
    "\n",
    "# reconstruct\n",
    "for mb_idx in get_batches(range(data_nans.shape[0]), batch_size=32):\n",
    "    #%% Inputs\n",
    "    X_mb = data_nans[mb_idx,:]  \n",
    "    \n",
    "    Z_mb = sample_Z(len(mb_idx), Dim) \n",
    "    M_mb = missing[mb_idx,:]  \n",
    "    \n",
    "    reconstructed = sess.run(G_sample, feed_dict={New_X: X_mb, M: M_mb})\n",
    "    data_reconstructed[mb_idx] = reconstructed * (1 - M_mb) + X_mb * M_mb\n",
    "\n",
    "# reconstruct data\n",
    "data_reconstructed_transformed = scaler.inverse_transform(data_reconstructed)\n",
    "\n",
    "processed_gan = processed.copy()\n",
    "processed_gan[columns_with_nans] = data_reconstructed_transformed\n",
    "\n",
    "def do_nothing(x, y):\n",
    "    return x, y\n",
    "\n",
    "conf_matrices_gans = run_binary_classification(processed_gan, len(most_common_disorders), clf, do_nothing, False, None)\n",
    "\n",
    "conf_matrices_gans = np.array(conf_matrices_gans)\n",
    "\n",
    "for matrix in conf_matrices_gans:\n",
    "    print(matrix, matrix[0][0] + matrix[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-xG4yxGlHDw"
   },
   "source": [
    "## temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzft_4b_lHDx"
   },
   "source": [
    "Compare autoencoder confusion matrices with that of the mean imputation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "BPtbYFfLlHDy",
    "outputId": "05ede2f3-4e32-48b9-f067-48b8de6fda24",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  84  151]\n",
      " [  27 1256]] 1340\n",
      "[[162  73]\n",
      " [ 42 376]] 538\n",
      "[[214  21]\n",
      " [ 25 145]] 359\n",
      "[[205  30]\n",
      " [ 33 232]] 437\n",
      "[[225  10]\n",
      " [ 43  32]] 257\n"
     ]
    }
   ],
   "source": [
    "all_matrices_mean = np.array(all_matrices_mean)\n",
    "\n",
    "for matrix in all_matrices_mean[0]:\n",
    "    print(matrix, matrix[0][0] + matrix[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBtJqd-VcSZ9"
   },
   "source": [
    "#### Autoencoder with scaling\n",
    "\n",
    "layers = [40, 20]\n",
    "learning_rate = 5\n",
    "masking=0\n",
    "dropout=0\n",
    "regularization = 10.\n",
    "n_epochs=300\n",
    "\n",
    "\n",
    "[[array([[  82,  153],\n",
    "         [  23, 1260]]), array([[161,  74],\n",
    "         [ 47, 371]]), array([[211,  24],\n",
    "         [ 24, 146]]), array([[202,  33],\n",
    "         [ 32, 233]]), array([[224,  11],\n",
    "         [ 46,  29]])], [array([[  83,  152],\n",
    "         [  28, 1255]]), array([[163,  72],\n",
    "         [ 50, 368]]), array([[208,  27],\n",
    "         [ 22, 148]]), array([[200,  35],\n",
    "         [ 33, 232]]), array([[223,  12],\n",
    "         [ 44,  31]])], [array([[  85,  150],\n",
    "         [  24, 1259]]), array([[162,  73],\n",
    "         [ 50, 368]]), array([[210,  25],\n",
    "         [ 21, 149]]), array([[199,  36],\n",
    "         [ 31, 234]]), array([[224,  11],\n",
    "         [ 43,  32]])], [array([[  84,  151],\n",
    "         [  22, 1261]]), array([[162,  73],\n",
    "         [ 50, 368]]), array([[208,  27],\n",
    "         [ 24, 146]]), array([[198,  37],\n",
    "         [ 34, 231]]), array([[226,   9],\n",
    "         [ 43,  32]])], [array([[  80,  155],\n",
    "         [  24, 1259]]), array([[157,  78],\n",
    "         [ 53, 365]]), array([[211,  24],\n",
    "         [ 21, 149]]), array([[197,  38],\n",
    "         [ 39, 226]]), array([[224,  11],\n",
    "         [ 43,  32]])], [array([[  82,  153],\n",
    "         [  28, 1255]]), array([[163,  72],\n",
    "         [ 49, 369]]), array([[207,  28],\n",
    "         [ 23, 147]]), array([[205,  30],\n",
    "         [ 31, 234]]), array([[225,  10],\n",
    "         [ 42,  33]])], [array([[  82,  153],\n",
    "         [  25, 1258]]), array([[158,  77],\n",
    "         [ 51, 367]]), array([[210,  25],\n",
    "         [ 29, 141]]), array([[196,  39],\n",
    "         [ 32, 233]]), array([[224,  11],\n",
    "         [ 45,  30]])], [array([[  84,  151],\n",
    "         [  28, 1255]]), array([[161,  74],\n",
    "         [ 55, 363]]), array([[207,  28],\n",
    "         [ 25, 145]]), array([[200,  35],\n",
    "         [ 36, 229]]), array([[222,  13],\n",
    "         [ 41,  34]])], [array([[  78,  157],\n",
    "         [  23, 1260]]), array([[156,  79],\n",
    "         [ 56, 362]]), array([[206,  29],\n",
    "         [ 31, 139]]), array([[201,  34],\n",
    "         [ 39, 226]]), array([[221,  14],\n",
    "         [ 38,  37]])], [array([[  75,  160],\n",
    "         [ 32, 1251]]), array([[143,  92],\n",
    "         [ 56, 362]]), array([[201,  34],\n",
    "         [ 46, 124]]), array([[199,  36],\n",
    "         [ 34, 231]]), array([[220,  15],\n",
    "         [ 51,  24]])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "pmXyHmxzmmVU",
    "outputId": "c1805a0c-f9f2-495c-b30e-a503c039dfab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.8532739726016502\n",
      "0.9 0.8513920187951879\n",
      "0.8 0.8550480579776772\n",
      "0.7000000000000001 0.8524009970615187\n",
      "0.6000000000000001 0.8484328939008092\n",
      "0.5000000000000001 0.8559595360434548\n",
      "0.40000000000000013 0.8461487153222731\n",
      "0.30000000000000016 0.8474948340441173\n",
      "0.20000000000000018 0.8425589419138639\n"
     ]
    }
   ],
   "source": [
    "for i, drop_values in enumerate(np.arange(1, 0.1, -0.1)):\n",
    "    # print(drop_values)\n",
    "    accuracies = []\n",
    "    total = 0\n",
    "    for matrix in all_matrices_auto[i]:\n",
    "        # print(matrix, matrix[0][0] + matrix[1][1])\n",
    "        accuracies.append((matrix[0][0] + matrix[1][1]) / np.sum(matrix))\n",
    "        total += 1\n",
    "    \n",
    "    print(drop_values, np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "67KxbpJa4I7w",
    "outputId": "537912a9-4662-475a-e5cd-48ea7ae9fdea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.8592164397543737\n",
      "0.9 0.8540288265485346\n",
      "0.8 0.8550816573026061\n",
      "0.7000000000000001 0.8517290100885427\n",
      "0.6000000000000001 0.8515769918854869\n",
      "0.5000000000000001 0.8509142688790089\n",
      "0.40000000000000013 0.845968761758078\n",
      "0.30000000000000016 0.8476523128052138\n",
      "0.20000000000000018 0.8458511641203451\n"
     ]
    }
   ],
   "source": [
    "for i, drop_values in enumerate(np.arange(1, 0.1, -0.1)):\n",
    "    # print(drop_values)\n",
    "    accuracies = []\n",
    "    total = 0\n",
    "    for matrix in all_matrices_mean[i]:\n",
    "        # print(matrix, matrix[0][0] + matrix[1][1])\n",
    "        accuracies.append((matrix[0][0] + matrix[1][1]) / np.sum(matrix))\n",
    "        total += 1\n",
    "    \n",
    "    print(drop_values, np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUROeF6acr2n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "d1L1Vx_ilHCo",
    "EIDOiLUxlHCw",
    "F3R_gvdrlHC1",
    "5tBE2_AjlHC5",
    "HZDQU4LQlHDG",
    "V3idaXvnlHDR",
    "NQ_4u13zlHDX",
    "gM7hJVI8lHDc",
    "NbFQINOLlHDj",
    "l-xG4yxGlHDw"
   ],
   "name": "data_imputation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
