{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": true,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "338px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "combined_features.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "o8iz9CsV-fRj"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QgFzePe-fQu",
        "colab_type": "text"
      },
      "source": [
        "# Combine features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-z0pu1P-fQx",
        "colab_type": "text"
      },
      "source": [
        "At this stage we are going to combine the behavioural, eeg and mri data.\n",
        "\n",
        "We are then gonna train a linear model with the purpose to both reconstruct the original data based on a latent space but also based on the predictions for the multi-label problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv-wzl38-fQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 5, 10\n",
        "rcParams['font.size'] = 12\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPozvjKt-jJB",
        "colab_type": "code",
        "outputId": "5916f7dd-2023-455b-ab02-4c50815cf42b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_7f1DfK-fQ2",
        "colab_type": "text"
      },
      "source": [
        "## Combine features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9aMfy8w-fQ3",
        "colab_type": "text"
      },
      "source": [
        "### Load behavioural data and preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn0sedol-fQ5",
        "colab_type": "code",
        "outputId": "d9b0c9a3-f5bb-45d2-9ea6-51945385a606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "base_dir = '/gdrive/My Drive/Colab Notebooks/DSLab/data'\n",
        "\n",
        "behaviour_data = pd.read_csv(os.path.join(base_dir, 'HBNFinalSummaries.csv'), low_memory=False)\n",
        "\n",
        "initial_size = behaviour_data.shape[0]\n",
        "behaviour_data = behaviour_data[behaviour_data['NoDX'].isin(['Yes', 'No'])]\n",
        "new_size = behaviour_data.shape[0]\n",
        "print('Removing', initial_size - new_size,\n",
        "      'patients as their evaluation was incomplete.')\n",
        "\n",
        "keep_most_common_diseases = 5\n",
        "healthy_diagnosis = 'No Diagnosis Given'\n",
        "\n",
        "# these disorders should also include the no diagnosis given option\n",
        "keep_most_common_diseases += 1\n",
        "\n",
        "category_columns = ['DX_' + str(i).zfill(2) + '_Cat' for i in range(1, 11)]\n",
        "\n",
        "# count for each disorder number of occurences\n",
        "disorder_counts = {}\n",
        "for val in behaviour_data[category_columns].values.reshape(-1):\n",
        "    if not pd.isnull(val):\n",
        "        if val in disorder_counts:\n",
        "            disorder_counts[val] += 1\n",
        "        else:\n",
        "            disorder_counts[val] = 1\n",
        "\n",
        "# sort in descending order\n",
        "disorder_counts = sorted(disorder_counts.items(), key=lambda kv: -kv[1])\n",
        "\n",
        "most_common_disorders = [x[0]\n",
        "                         for x in disorder_counts[:keep_most_common_diseases]]\n",
        "\n",
        "# find users that have no diagnosis within these top diseases\n",
        "# filtering should cahnge anything as this should also happen at a later stage\n",
        "mask = None\n",
        "for col in category_columns:\n",
        "    mask_col = behaviour_data[col].isin(most_common_disorders)\n",
        "    if mask is None:\n",
        "        mask = mask_col\n",
        "    else:\n",
        "        mask = mask | mask_col\n",
        "\n",
        "initial_size = behaviour_data.shape[0]\n",
        "behaviour_data = behaviour_data[mask]\n",
        "behaviour_data = behaviour_data.reset_index(drop=True)\n",
        "new_size = behaviour_data.shape[0]\n",
        "print('Removing', initial_size - new_size,\n",
        "      'patients as their diagnoses were very uncommon.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing 282 patients as their evaluation was incomplete.\n",
            "Removing 37 patients as their diagnoses were very uncommon.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mew1LmLf-fQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_diagnosis_given = 'No Diagnosis Given'\n",
        "\n",
        "if no_diagnosis_given in most_common_disorders:\n",
        "    no_diag_index = most_common_disorders.index(no_diagnosis_given)\n",
        "    most_common_disorders = most_common_disorders[:no_diag_index] + \\\n",
        "        most_common_disorders[no_diag_index + 1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbNxdZwW_Osl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diagnoses_to_ids = {disorder: i for i, disorder in enumerate(most_common_disorders)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va2jTwN0_Owe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "order_of_disorders = []\n",
        "for k in range(behaviour_data.shape[0]):\n",
        "    i = 0\n",
        "    disorder = behaviour_data.iloc[k][category_columns[i]]\n",
        "    disorders_patient = []\n",
        "    while disorder != no_diagnosis_given and not pd.isnull(disorder):\n",
        "        if disorder in diagnoses_to_ids:\n",
        "            if diagnoses_to_ids[disorder] not in disorders_patient:\n",
        "                disorders_patient.append(diagnoses_to_ids[disorder])\n",
        "        i += 1\n",
        "        if i == len(category_columns):\n",
        "            break\n",
        "        disorder = behaviour_data.iloc[k][category_columns[i]]\n",
        "\n",
        "    order_of_disorders.append(disorders_patient)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62KTq9bsD-JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_order = np.max([len(x) for x in order_of_disorders])\n",
        "\n",
        "# pad with a new token denoting the pad token\n",
        "pad_token = len(most_common_disorders)\n",
        "bod_token = len(most_common_disorders) + 1\n",
        "eod_token = len(most_common_disorders) + 2\n",
        "\n",
        "order_of_disorders = [[bod_token] + x + [eod_token] + [pad_token] * (max_len_order - len(x)) for x in order_of_disorders]\n",
        "\n",
        "order_of_disorders = np.array(order_of_disorders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOCqpdBw_mQN",
        "colab_type": "text"
      },
      "source": [
        "Many diseases are also present many times. What should we do with them? I have only considered the first presence of a disorder for a patient! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ0Jqd1O-fRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = np.zeros((len(most_common_disorders),\n",
        "                    behaviour_data.shape[0]), dtype=np.int32)\n",
        "\n",
        "\n",
        "df_disorders = behaviour_data[category_columns]\n",
        "\n",
        "for i, disorder in enumerate(most_common_disorders):\n",
        "    mask = df_disorders.select_dtypes(include=[object]). \\\n",
        "        applymap(lambda x: disorder in x if pd.notnull(x) else False)\n",
        "\n",
        "    disorder_df = df_disorders[mask.any(axis=1)]\n",
        "\n",
        "    np.add.at(classes[i], disorder_df.index.values, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS7UZNBj-fRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "behaviour_data_columns = behaviour_data.columns.values.astype(np.str)\n",
        "\n",
        "columns_to_drop = behaviour_data_columns[\n",
        "    np.flatnonzero(np.core.defchararray.find(behaviour_data_columns, 'DX') != -1)]\n",
        "\n",
        "behaviour_data = behaviour_data.drop(columns=columns_to_drop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHs-5BDa-fRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for disorder, classification in zip(most_common_disorders, classes):\n",
        "    behaviour_data[disorder] = classification\n",
        "\n",
        "behaviour_data['order_diagnoses'] = list(order_of_disorders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVmvhBv-fRP",
        "colab_type": "code",
        "outputId": "f86eac30-bfcf-40c0-f353-6fd2caedbcd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "behaviour_data.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1777, 312)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tRsTXJq-fRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined_df = behaviour_data.set_index('EID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu5vFci--fRW",
        "colab_type": "text"
      },
      "source": [
        "### Load mri and add to dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYyM0gS6-fRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fa_per_tract = pd.read_csv('DataScience2019_MRI/MRI/DTI/FAPerTract.csv', low_memory=False)\n",
        "\n",
        "# Remove \"/\" from the end some IDs \n",
        "fa_per_tract['ID'] = fa_per_tract['ID'].apply(lambda x: x[:-1] if \"/\" in x else x)\n",
        "\n",
        "# join with behavioural data\n",
        "combined_df = combined_df.join(fa_per_tract.set_index('ID'), how='inner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSVR86ej-fRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base_dir = 'DataScience2019_MRI/MRI/structuralMRI'\n",
        "\n",
        "# column ScanSite already exists in the behavioural data\n",
        "cort_thick_l = pd.read_csv(os.path.join(base_dir,\n",
        "    'CorticalThicknessLHROI.csv'), low_memory=False).drop(columns=['ScanSite'])\n",
        "cort_thick_r = pd.read_csv(os.path.join(base_dir,\n",
        "    'CorticalThicknessRHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "cort_vol_l = pd.read_csv(os.path.join(base_dir,\n",
        "    'CorticalVolumeLHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "cort_vol_r = pd.read_csv(os.path.join(base_dir,\n",
        "    'CorticalVolumeRHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "sub_cort_vol_l = pd.read_csv(os.path.join(base_dir,\n",
        "    'SubCorticalVolumeLHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "sub_cort_vol_r = pd.read_csv(os.path.join(base_dir,\n",
        "    'SubCorticalVolumeRHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "glob_thick = pd.read_csv(os.path.join(base_dir,\n",
        "    'GlobalCorticalThickness.csv'), low_memory=False).drop(columns=['ScanSite'])\n",
        "\n",
        "# Join tables \n",
        "struct_mri = pd.merge(cort_thick_l, cort_thick_r, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, cort_vol_l, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, cort_vol_r, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, sub_cort_vol_l, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, sub_cort_vol_r, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, glob_thick, on='ID', how='inner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy8hgIJZ-fRg",
        "colab_type": "code",
        "outputId": "6929cdb6-ab77-4f21-9468-8eb6d481c304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "combined_df = combined_df.join(struct_mri.set_index('ID'), how='inner')\n",
        "combined_df.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1053, 685)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8iz9CsV-fRj",
        "colab_type": "text"
      },
      "source": [
        "### Load EEG  and add to dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nApUpzud-fRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = 'DataScience2019_MRI/EEG'\n",
        "\n",
        "eeg_mic = pd.read_csv(os.path.join(base_dir, \"RestingEEG_Microstates.csv\"))\n",
        "eeg_psd = pd.read_csv(os.path.join(base_dir, \"RestingEEG_PSD_Average.csv\"))\n",
        "eeg_spectro = pd.read_csv(os.path.join(base_dir, \"RestingEEG_Spectro_Average.csv\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaRnpuIs-fRp",
        "colab_type": "code",
        "outputId": "75e44322-7684-43a6-f3a5-bcaf3ea2a87f",
        "colab": {}
      },
      "source": [
        "combined_df = combined_df.join(eeg_mic.set_index('id'), how='inner')\n",
        "combined_df = combined_df.join(eeg_psd.set_index('id'), how='inner')\n",
        "combined_df = combined_df.join(eeg_spectro.set_index('id'), how='inner')\n",
        "\n",
        "combined_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(950, 735)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsbzEqsi-fRv",
        "colab_type": "text"
      },
      "source": [
        "### Some final preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNavca52-fRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fdx and mdx may contain 'No Diagnosis'\n",
        "# drop them for now but they may be important\n",
        "# they correspond to father's and mother's primary diagnosis\n",
        "columns_to_drop = ['Anonymized.ID', 'mdx', 'fdx', 'fcodxm_1', 'fcodxm_2', 'fcodxm_3', 'mcodxm_1',\n",
        "                   'mcodxm_2', 'mcodxm_3', 'mcodxmdt', 'TOWRE_Total_Desc', 'Picture_Vocab_Raw',\n",
        "                   'sib1dx', 'sib1codxm_1', 'sib1codxm_2', 'sib1codxm_3',\n",
        "                   'sib2dx', 'sib2codxm_1', 'sib2codxm_2', 'sib2codxm_3',\n",
        "                   'sib3dx', 'sib3codxm_1', 'sib3codxm_2', 'sib3codxm_3',\n",
        "                   'sib4dx', 'sib4codxm_1', 'sib4codxm_2', 'sib4codxm_3',\n",
        "                   'sib5dx', 'sib5codxm_1', 'sib5codxm_2', 'sib5codxm_3']\n",
        "\n",
        "combined_df = combined_df.drop(columns=columns_to_drop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLNGh4D8-fR2",
        "colab_type": "code",
        "outputId": "6a0111aa-126c-4227-8bf4-90594f2d98cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "combined_df.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1053, 653)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzSFBnK0-fR5",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl_l1rVj-5_U",
        "colab_type": "text"
      },
      "source": [
        "For this time we have only considered the behavioural data and the structural mri as these datasets have the most ids in common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8K6bYxjLF8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert combined_df.shape == (1053, 653) # if you only include the behavioural and the structural"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga9mupwM-fR6",
        "colab_type": "code",
        "outputId": "f8d0f3e7-b468-4fba-fefc-95afc52aefa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def mean_imputer(x, y):\n",
        "    return np.where(np.isnan(x), np.ma.array(x, mask=np.isnan(x)).mean(axis=0), x),\\\n",
        "            np.where(np.isnan(y), np.ma.array(x, mask=np.isnan(x)).mean(axis=0), y)  \n",
        "\n",
        "# we are goint to remove some behavioral data based on their null values\n",
        "drop_missing_threshold = 0.5\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=17, shuffle=True)\n",
        "kf.get_n_splits(combined_df)\n",
        "\n",
        "# preds = np.zeros(combined_df.shape[0], len(most_common_disorders))\n",
        "\n",
        "for train_index, test_index in kf.split(combined_df):\n",
        "    train, test = combined_df.iloc[train_index], combined_df.iloc[test_index]\n",
        "    # for testing purposes\n",
        "    break\n",
        "\n",
        "columns_mask = pd.isnull(train).sum() / train.shape[0] >= drop_missing_threshold\n",
        "\n",
        "print('Droping this many columns:', np.sum(columns_mask))\n",
        "\n",
        "dropped_columns = train.columns[columns_mask]\n",
        "\n",
        "train = train.drop(columns=dropped_columns)\n",
        "test = test.drop(columns=dropped_columns)\n",
        "\n",
        "# classes\n",
        "train_classes = train[most_common_disorders].values\n",
        "test_classes = test[most_common_disorders].values\n",
        "\n",
        "# orders\n",
        "train_orders = np.stack(train['order_diagnoses'].values)\n",
        "test_orders = np.stack(test['order_diagnoses'].values)\n",
        "\n",
        "# keep only features\n",
        "train = train.drop(columns=most_common_disorders + ['order_diagnoses'])\n",
        "test = test.drop(columns=most_common_disorders + ['order_diagnoses'])\n",
        "\n",
        "train_mask = pd.isnull(train).values.astype(np.float32)\n",
        "test_mask = pd.isnull(test).values.astype(np.float32)\n",
        "\n",
        "# deal with numpy because of some weird SettingWithCopyWarning I cannot figure out\n",
        "# impute based on the mean values\n",
        "train, test = mean_imputer(train.values, test.values)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train = scaler.fit_transform(train)\n",
        "test = scaler.transform(test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Droping this many columns: 130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2wYRkLt-fR9",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IyHKw7YDmOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "81649fd6-6a3d-479b-f2a6-b2d2dad10802"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "\n",
        "def get_batches(iterable, batch_size=64, do_shuffle=True):\n",
        "    if do_shuffle:\n",
        "        iterable = shuffle(iterable)\n",
        "\n",
        "    length = len(iterable)\n",
        "    for ndx in range(0, length, batch_size):\n",
        "        iterable_batch = iterable[ndx: min(ndx + batch_size, length)]\n",
        "        yield iterable_batch\n",
        "\n",
        "\n",
        "def get_reconstruction_loss(true, predictions, mask):\n",
        "    loss = np.mean(((true - predictions) ** 2) * mask, axis=1)\n",
        "    return np.mean(loss, axis=0)\n",
        "\n",
        "\n",
        "def multi_label_accuracy(true, predictions):\n",
        "    if not issubclass(predictions.dtype.type, np.integer):\n",
        "        predictions = before_softmax_to_predictions(predictions)\n",
        "\n",
        "    return 1 - np.sum((true - predictions) ** 2) / (true.shape[0] * true.shape[1])\n",
        "\n",
        "\n",
        "def before_softmax_to_predictions(predictions):\n",
        "    return (predictions >= 0).astype(np.int16)\n",
        "\n",
        "\n",
        "def f1_per_class(true, predictions):\n",
        "    if not issubclass(predictions.dtype.type, np.integer):\n",
        "        predictions = before_softmax_to_predictions(predictions)\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        f1_scores = list()\n",
        "        for i in range(true.shape[1]):\n",
        "            f1_scores.append(f1_score(true[:, i], predictions[:, i], average='macro'))\n",
        "\n",
        "    return f1_scores\n",
        "\n",
        "\n",
        "DEFAULT_LOG_PATH = './autoencoder_predict'\n",
        "\n",
        "\n",
        "class AutoencodePredict:\n",
        "    training = None\n",
        "    input_ = None\n",
        "    input_mask = None\n",
        "    intermediate_representation = None\n",
        "    input_reconstructed = None\n",
        "    reconstruction_loss = None\n",
        "    regularization_loss = None\n",
        "    prediction_loss = None\n",
        "    true_predictions = None\n",
        "    predictions = None\n",
        "    total_loss = None\n",
        "    pos_weights = None\n",
        "    class_weights = None\n",
        "\n",
        "    def __init__(self,\n",
        "                 number_of_features,\n",
        "                 num_classes,\n",
        "                 alpha=1,           # parameter showing the significance of the prediction loss\n",
        "                 activation=tf.nn.relu,\n",
        "                 layers=None,\n",
        "                 prediction_layers=None,\n",
        "                 dropout=None,\n",
        "                 regularization=0,\n",
        "                 masking=0.5):\n",
        "\n",
        "        self.activation = activation\n",
        "        self.num_classes = num_classes\n",
        "        self.alpha = alpha\n",
        "\n",
        "        if layers is None:\n",
        "            self.layers = [50, 15]\n",
        "        else:\n",
        "            self.layers = layers\n",
        "\n",
        "        if prediction_layers is None:\n",
        "            self.prediction_layers = [25, 15]\n",
        "        else:\n",
        "            self.prediction_layers = prediction_layers\n",
        "\n",
        "        self.number_of_features = number_of_features\n",
        "\n",
        "        self.masking = masking\n",
        "        self.dropout = dropout\n",
        "\n",
        "        use_regularization = (regularization > 0)\n",
        "        self.use_regularization = use_regularization\n",
        "\n",
        "        if regularization == 0:\n",
        "            # set to small value to avoid tensorflow error\n",
        "            # use_regularization = False in this case and will not contribute towards the final loss\n",
        "            self.regularization = 0.1\n",
        "        else:\n",
        "            self.regularization = regularization\n",
        "\n",
        "    def build_graph(self):\n",
        "\n",
        "        self.training = tf.placeholder(tf.bool, shape=[], name='training')\n",
        "\n",
        "        self.input_ = tf.placeholder(tf.float32, shape=[None, self.number_of_features], name='input_data')\n",
        "        self.input_mask = tf.placeholder(tf.float32, shape=[None, self.number_of_features], name='input_mask')\n",
        "        self.true_predictions = tf.placeholder(tf.float32, shape=[None, self.num_classes], name='input_predictions')\n",
        "\n",
        "        # placeholders used to balance the loss for individual classes and predictions\n",
        "        self.pos_weights = tf.placeholder(tf.float32, shape=[5], name='pos_weights')\n",
        "        self.class_weights = tf.placeholder(tf.float32, shape=[5], name='class_weights')\n",
        "\n",
        "        self.intermediate_representation = self.encode(self.input_)\n",
        "\n",
        "        self.input_reconstructed = self.decode(self.intermediate_representation)\n",
        "\n",
        "        self.predictions = self.predict_classes(self.intermediate_representation)\n",
        "\n",
        "        if self.input_mask is not None:\n",
        "            self.reconstruction_loss = tf.reduce_mean(((self.input_ - self.input_reconstructed) ** 2) * self.input_mask)\n",
        "        else:\n",
        "            self.reconstruction_loss = tf.reduce_mean((self.input_ - self.input_reconstructed) ** 2)\n",
        "\n",
        "        # self.prediction_loss = tf.reduce_mean(\n",
        "        #     tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_predictions,\n",
        "        #                                             logits=self.predictions)) * self.alpha\n",
        "        def my_loss(labels, logits, pos_weight, class_weight):\n",
        "            return tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(labels=labels,\n",
        "                                                                           logits=logits,\n",
        "                                                                           pos_weight=pos_weight)) * class_weight\n",
        "\n",
        "        loss_per_class = tf.map_fn(\n",
        "            lambda x: my_loss(x[0], x[1], x[2], x[3]),\n",
        "            (tf.transpose(self.true_predictions), tf.transpose(self.predictions), self.pos_weights, self.class_weights),\n",
        "            dtype=tf.float32)\n",
        "\n",
        "        self.prediction_loss = tf.reduce_mean(loss_per_class)\n",
        "\n",
        "        if self.use_regularization:\n",
        "            self.regularization_loss = tf.losses.get_regularization_loss()\n",
        "\n",
        "            # TODO CHANGE TO ORIGINAL\n",
        "            self.total_loss = self.reconstruction_loss + self.regularization_loss + self.prediction_loss\n",
        "            # self.total_loss = self.prediction_loss\n",
        "        else:\n",
        "            self.total_loss = self.reconstruction_loss + self.prediction_loss\n",
        "            # self.total_loss = self.prediction_loss\n",
        "\n",
        "    def predict_classes(self, intermediate):\n",
        "\n",
        "        x = intermediate\n",
        "        for i, layer in enumerate(self.prediction_layers):\n",
        "            x = tf.layers.dense(x, layer, use_bias=True, name='predict_layer_' + str(i),\n",
        "                                activation=self.activation,\n",
        "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "            if self.dropout is not None:\n",
        "                x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
        "\n",
        "        x = tf.layers.dense(x, self.num_classes, use_bias=True, name='predict_layer_final', activation=None,\n",
        "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "        return x\n",
        "\n",
        "    def encode(self,\n",
        "               input_):\n",
        "\n",
        "        if self.masking > 0:\n",
        "            # mask randomly some of the inputs\n",
        "            input_ = tf.layers.dropout(input_, rate=self.masking, training=self.training)\n",
        "\n",
        "        x = input_\n",
        "        # important to use relu as a first layer to make all unobserved values set to 0?\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = tf.layers.dense(x, layer, use_bias=True, name='input_layer_1_' + str(i),\n",
        "                                activation=self.activation,\n",
        "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "            if self.dropout is not None:\n",
        "                x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def decode(self,\n",
        "               intermediate):\n",
        "\n",
        "        x = intermediate\n",
        "        for i, layer in enumerate(self.layers[::-1][1:]):\n",
        "            x = tf.layers.dense(x, layer, use_bias=True, name='input_layer_2_' + str(i),\n",
        "                                activation=self.activation,\n",
        "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "            if self.dropout is not None:\n",
        "                x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
        "\n",
        "        x = tf.layers.dense(x, self.number_of_features, use_bias=True, name='input_layer_2_final',\n",
        "                            activation=self.activation,\n",
        "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def reconstruct(self,\n",
        "                    data,\n",
        "                    log_path=None):\n",
        "\n",
        "        if log_path is None:\n",
        "            log_path = DEFAULT_LOG_PATH\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "                self.build_graph()\n",
        "\n",
        "                saver = tf.train.Saver()\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(log_path))\n",
        "\n",
        "                data_reconstructed = np.zeros((data.shape[0], self.number_of_features))\n",
        "\n",
        "                for rows in get_batches(list(range(data.shape[0])), batch_size=64, do_shuffle=False):\n",
        "                    rows_features = [data[i, :] for i in rows]\n",
        "\n",
        "                    rows_reconstructed = sess.run(self.input_reconstructed,\n",
        "                                                  feed_dict={\n",
        "                                                      self.input_: rows_features,\n",
        "                                                      self.training: False\n",
        "                                                  })\n",
        "\n",
        "                    data_reconstructed[rows] = rows_reconstructed\n",
        "\n",
        "                return data_reconstructed\n",
        "\n",
        "    def get_latent_space(self,\n",
        "                         data,\n",
        "                         log_path=None):\n",
        "\n",
        "        if log_path is None:\n",
        "            log_path = DEFAULT_LOG_PATH\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "                self.build_graph()\n",
        "\n",
        "                saver = tf.train.Saver()\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(log_path))\n",
        "\n",
        "                data_latent = np.zeros((data.shape[0], self.layers[-1]))\n",
        "\n",
        "                for rows in get_batches(list(range(data.shape[0])), batch_size=64, do_shuffle=False):\n",
        "                    rows_features = [data[i, :] for i in rows]\n",
        "\n",
        "                    rows_latent = sess.run(self.intermediate_representation,\n",
        "                                           feed_dict={\n",
        "                                               self.input_: rows_features,\n",
        "                                               self.training: False\n",
        "                                           })\n",
        "\n",
        "                    data_latent[rows] = rows_latent\n",
        "\n",
        "                return data_latent\n",
        "\n",
        "    def predict_with_sess(self, sess, data):\n",
        "        predictions = np.zeros((data.shape[0], self.num_classes))\n",
        "\n",
        "        for rows in get_batches(list(range(data.shape[0])), batch_size=64, do_shuffle=False):\n",
        "            rows_features = [data[i, :] for i in rows]\n",
        "\n",
        "            rows_predictions = sess.run(self.predictions,\n",
        "                                        feed_dict={\n",
        "                                            self.input_: rows_features,\n",
        "                                            self.training: False\n",
        "                                        })\n",
        "\n",
        "            predictions[rows] = rows_predictions\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict(self,\n",
        "                data,\n",
        "                log_path=None,\n",
        "                make_integer=True):\n",
        "\n",
        "        if log_path is None:\n",
        "            log_path = DEFAULT_LOG_PATH\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "                self.build_graph()\n",
        "\n",
        "                saver = tf.train.Saver()\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(log_path))\n",
        "\n",
        "                if make_integer:\n",
        "                    return before_softmax_to_predictions(self.predict_with_sess(sess, data))\n",
        "                else:\n",
        "                    return self.predict_with_sess(sess, data)\n",
        "\n",
        "    def fit(self,\n",
        "            data,\n",
        "            data_mask,\n",
        "            data_labels,\n",
        "            test_data=None,\n",
        "            test_data_mask=None,\n",
        "            test_data_labels=None,\n",
        "            pos_weights=None,\n",
        "            class_weights=None,\n",
        "            n_epochs=350,\n",
        "            decay_steps=None,\n",
        "            learning_rate=None,\n",
        "            decay=None,\n",
        "            log_path=None,\n",
        "            verbose=True,\n",
        "            print_every_epochs=10):\n",
        "\n",
        "        if pos_weights is None:\n",
        "            pos_weights = [1] * self.num_classes\n",
        "\n",
        "        if class_weights is None:\n",
        "            class_weights = [1] * self.num_classes\n",
        "\n",
        "        if decay_steps is None:\n",
        "            # empirical\n",
        "            decay_steps = data.shape[0] // 64 * 5\n",
        "\n",
        "        if learning_rate is None:\n",
        "            learning_rate = 0.001\n",
        "\n",
        "        if decay is None:\n",
        "            decay = 0.96\n",
        "\n",
        "        if log_path is None:\n",
        "            log_path = DEFAULT_LOG_PATH\n",
        "\n",
        "        validation = False\n",
        "        if test_data is not None and test_data_mask is not None and test_data_labels is not None:\n",
        "            validation = True\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "\n",
        "                self.build_graph()\n",
        "\n",
        "                global_step = tf.Variable(1, name='global_step', trainable=False)\n",
        "\n",
        "                learning_rate = tf.Variable(learning_rate, trainable=False, dtype=tf.float32, name=\"learning_rate\")\n",
        "                learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay)\n",
        "\n",
        "                # Gradients and update operation for training the model.\n",
        "                opt = tf.train.AdamOptimizer(learning_rate)\n",
        "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "                with tf.control_dependencies(update_ops):\n",
        "                    # Update all the trainable parameters\n",
        "                    train_step = opt.minimize(self.total_loss, global_step=global_step)\n",
        "\n",
        "                saver = tf.train.Saver(max_to_keep=3)\n",
        "\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "\n",
        "                for epoch in range(n_epochs):\n",
        "                    reconstruction_loss = 0\n",
        "\n",
        "                    if self.use_regularization:\n",
        "                        regularization_loss = 0\n",
        "\n",
        "                    prediction_loss = 0\n",
        "                    for rows in get_batches(list(range(data.shape[0])), batch_size=64):\n",
        "                        rows_features = data[rows]\n",
        "                        rows_masks = data_mask[rows]\n",
        "                        rows_predictions = data_labels[rows]\n",
        "\n",
        "                        if self.use_regularization:\n",
        "                            _, rec_loss, reg_loss, pred_loss, step = sess.run(\n",
        "                                [train_step, self.reconstruction_loss, self.regularization_loss, self.prediction_loss,\n",
        "                                 global_step],\n",
        "                                feed_dict={\n",
        "                                    self.input_: rows_features,\n",
        "                                    self.input_mask: rows_masks,\n",
        "                                    self.true_predictions: rows_predictions,\n",
        "                                    self.pos_weights: pos_weights,\n",
        "                                    self.class_weights: class_weights,\n",
        "                                    self.training: True\n",
        "                                })\n",
        "                        else:\n",
        "                            _, rec_loss, pred_loss, step = sess.run(\n",
        "                                [train_step, self.reconstruction_loss, self.prediction_loss,\n",
        "                                 global_step],\n",
        "                                feed_dict={\n",
        "                                    self.input_: rows_features,\n",
        "                                    self.input_mask: rows_masks,\n",
        "                                    self.true_predictions: rows_predictions,\n",
        "                                    self.pos_weights: pos_weights,\n",
        "                                    self.class_weights: class_weights,\n",
        "                                    self.training: True\n",
        "                                })\n",
        "\n",
        "                        reconstruction_loss += rec_loss\n",
        "\n",
        "                        if self.use_regularization:\n",
        "                            regularization_loss += reg_loss\n",
        "\n",
        "                        prediction_loss += pred_loss\n",
        "\n",
        "                    if epoch % print_every_epochs == 0:\n",
        "                        if verbose and validation:\n",
        "                            predictions_train = self.predict_with_sess(sess, data)\n",
        "                            train_accuracy = multi_label_accuracy(data_labels, predictions_train)\n",
        "\n",
        "                            predictions_test = self.predict_with_sess(sess, test_data)\n",
        "                            test_accuracy = multi_label_accuracy(test_data_labels, predictions_test)\n",
        "\n",
        "                            if self.use_regularization:\n",
        "                                print('At epoch {:4d} rec_loss: {:8.4f} reg_loss: {:8.4f} pred_loss: {:8.4f} train_'\n",
        "                                      'acc: {:.4f} test_acc {:.4f}'.format(epoch, reconstruction_loss,\n",
        "                                                                           regularization_loss, prediction_loss,\n",
        "                                                                           train_accuracy, test_accuracy))\n",
        "                            else:\n",
        "                                print('At epoch {:4d} rec_loss: {:8.4f} pred_loss: {:8.4f} train_'\n",
        "                                      'acc: {:.4f} test_acc {:.4f}'.format(epoch, reconstruction_loss,\n",
        "                                                                           prediction_loss,\n",
        "                                                                           train_accuracy, test_accuracy))\n",
        "\n",
        "                            f1_scores = f1_per_class(data_labels, predictions_train)\n",
        "                            print('\\ttrain f1_scores: ', end='')\n",
        "                            for sc in f1_scores:\n",
        "                                print('{:.3f} '.format(sc), end='')\n",
        "\n",
        "                            f1_scores = f1_per_class(test_data_labels, predictions_test)\n",
        "                            print('test f1_scores: ', end='')\n",
        "                            for sc in f1_scores:\n",
        "                                print('{:.3f} '.format(sc), end='')\n",
        "                            print()\n",
        "\n",
        "                        saver.save(sess, os.path.join(log_path, \"model\"), global_step=epoch)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m600XyjQB99y",
        "colab_type": "code",
        "outputId": "2a1d597b-8759-4ed1-9f38-ace9e4929beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(842, 517)\n",
            "(211, 517)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_nBWfFY-fSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha=2e-3          # parameter showing the significance of the prediction loss\n",
        "activation=tf.nn.relu\n",
        "layers=[50, 15]\n",
        "prediction_layers=[100, 25]\n",
        "dropout=0.1\n",
        "regularization=5e-5\n",
        "masking=0.15\n",
        "\n",
        "model = AutoencodePredict(train.shape[1], len(most_common_disorders), alpha=alpha, activation=activation, layers=layers,\n",
        "                          prediction_layers=prediction_layers, dropout=dropout, regularization=regularization, \n",
        "                          masking=masking)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9d5bqQg-fSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "8f7ade0c-8d89-465a-ad66-3aa05407dadb"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "# pos_weights = [1, 1, 1, 1, 1]\n",
        "# set larger weight for instances of disorders that are very infrequent\n",
        "\n",
        "def smoothing_fun(a, beta=1):\n",
        "    return [min(i, 1) for i in a] + beta * np.log([max(i - 1, 1) for i in a])\n",
        "\n",
        "pos_weights = smoothing_fun(1 / (np.sum(train_classes, axis=0) / train_classes.shape[0]))\n",
        "class_weights = [1, 1, 1, 1, 1]\n",
        "\n",
        "model.fit(train, train_mask, train_classes, test_data=test, test_data_mask=test_mask, test_data_labels=test_classes,\n",
        "          n_epochs=2000, print_every_epochs=200, pos_weights=pos_weights, class_weights=class_weights)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-90f86fcf2fb6>:173: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-19-90f86fcf2fb6>:180: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "At epoch    0 rec_loss:   0.1296 reg_loss:   0.1007 pred_loss:  11.6234 train_acc: 0.8316 test_acc 0.8199\n",
            "\ttrain f1_scores: 0.418 0.432 0.478 0.472 0.489 test f1_scores: 0.422 0.420 0.456 0.463 0.485 \n",
            "At epoch  200 rec_loss:   0.0149 reg_loss:   0.0487 pred_loss:   7.4592 train_acc: 0.8565 test_acc 0.8417\n",
            "\ttrain f1_scores: 0.603 0.747 0.762 0.786 0.489 test f1_scores: 0.637 0.741 0.733 0.743 0.485 \n",
            "At epoch  400 rec_loss:   0.0151 reg_loss:   0.0474 pred_loss:   6.9499 train_acc: 0.8575 test_acc 0.8398\n",
            "\ttrain f1_scores: 0.609 0.736 0.779 0.803 0.489 test f1_scores: 0.630 0.716 0.754 0.749 0.485 \n",
            "At epoch  600 rec_loss:   0.0145 reg_loss:   0.0468 pred_loss:   7.1938 train_acc: 0.8594 test_acc 0.8332\n",
            "\ttrain f1_scores: 0.628 0.743 0.779 0.800 0.489 test f1_scores: 0.626 0.716 0.732 0.737 0.485 \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "At epoch  800 rec_loss:   0.0147 reg_loss:   0.0465 pred_loss:   7.0934 train_acc: 0.8594 test_acc 0.8351\n",
            "\ttrain f1_scores: 0.627 0.741 0.777 0.804 0.489 test f1_scores: 0.626 0.716 0.743 0.737 0.485 \n",
            "At epoch 1000 rec_loss:   0.0140 reg_loss:   0.0464 pred_loss:   7.0736 train_acc: 0.8594 test_acc 0.8332\n",
            "\ttrain f1_scores: 0.627 0.742 0.776 0.805 0.489 test f1_scores: 0.626 0.716 0.738 0.731 0.485 \n",
            "At epoch 1200 rec_loss:   0.0140 reg_loss:   0.0464 pred_loss:   6.7900 train_acc: 0.8594 test_acc 0.8332\n",
            "\ttrain f1_scores: 0.627 0.742 0.776 0.805 0.489 test f1_scores: 0.626 0.716 0.738 0.731 0.485 \n",
            "At epoch 1400 rec_loss:   0.0145 reg_loss:   0.0464 pred_loss:   6.9795 train_acc: 0.8594 test_acc 0.8332\n",
            "\ttrain f1_scores: 0.627 0.742 0.776 0.805 0.489 test f1_scores: 0.626 0.716 0.738 0.731 0.485 \n",
            "At epoch 1600 rec_loss:   0.0149 reg_loss:   0.0464 pred_loss:   6.7861 train_acc: 0.8594 test_acc 0.8332\n",
            "\ttrain f1_scores: 0.627 0.742 0.776 0.805 0.489 test f1_scores: 0.626 0.716 0.738 0.731 0.485 \n",
            "At epoch 1800 rec_loss:   0.0146 reg_loss:   0.0464 pred_loss:   6.8632 train_acc: 0.8594 test_acc 0.8332\n",
            "\ttrain f1_scores: 0.627 0.742 0.776 0.805 0.489 test f1_scores: 0.626 0.716 0.738 0.731 0.485 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hnadzuPhVKV",
        "colab_type": "text"
      },
      "source": [
        "Using only behavioural:\n",
        "\n",
        "At epoch 2000 rec_loss:   0.0943 reg_loss:   0.1133 pred_loss:  11.0274 train_acc: 0.8756 test_acc 0.8522<br>\n",
        "train f1_scores: 0.736 0.757 0.784 0.826 0.673 test f1_scores: 0.719 0.719 0.746 0.716 0.523<br>\n",
        "\n",
        "Total recall: 0.7884615384615384<br>\n",
        "Total precision: 0.6923076923076923<br>\n",
        "<br>\n",
        "\n",
        "Behavioural + structural mri:\n",
        "\n",
        "At epoch 2000 rec_loss:   0.0151 reg_loss:   0.0533 pred_loss:   7.0952 train_acc: 0.8646 test_acc 0.8313\n",
        "train f1_scores: 0.713 0.725 0.795 0.816 0.739 test f1_scores: 0.694 0.708 0.705 0.731 0.545 \n",
        "\n",
        "Total recall: 0.7758007117437722<br>\n",
        "Total precision: 0.6546546546546547<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "85959b7a-ac45-4f2f-8e77-2d53f32494fc",
        "id": "k0nKmgnUN0om",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds = model.predict(test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./autoencoder_predict/model-1800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyclf4nqMPDa",
        "colab_type": "code",
        "outputId": "d05c69f9-00e2-4f84-a1c5-45a42170589d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "multi_label_accuracy(test_classes, preds)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.833175355450237"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on2_tbCpozAw",
        "colab_type": "code",
        "outputId": "a92722a2-603d-419c-8657-2da8d9c7eceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# total recall\n",
        "\n",
        "indices = np.where(test_classes == 1)\n",
        "total = len(indices[0])\n",
        "found = 0\n",
        "for i, j in zip(indices[0], indices[1]):\n",
        "    found += preds[i][j]\n",
        "\n",
        "print('Total recall:', found / total)\n",
        "print('Total precision:', found / np.sum(preds))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total recall: 0.8078291814946619\n",
            "Total precision: 0.6504297994269341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrvHRVAFVCCw",
        "colab_type": "code",
        "outputId": "558db9bc-1d8c-4fdd-dbd4-908adf73ff1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(test.shape)\n",
        "print(np.sum(test_classes, axis=0))\n",
        "print(np.sum(preds, axis=0))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(211, 517)\n",
            "[154  58  28  29  12]\n",
            "[187  76  46  40   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP1E2ESyBlQZ",
        "colab_type": "text"
      },
      "source": [
        "## Order of diagnoses from the latent space!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7-g8BxGBorr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a95ad6d-38be-4fbb-eeb4-273d5736f274"
      },
      "source": [
        "train_latent_space = model.get_latent_space(train)\n",
        "test_latent_space = model.get_latent_space(test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./autoencoder_predict/model-1800\n",
            "INFO:tensorflow:Restoring parameters from ./autoencoder_predict/model-1800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XRfDfKxCwLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_correct(out1, out2):\n",
        "    # out2 is the prediction\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "    for o1, o2 in zip(out1.astype(np.int32), out2.astype(np.int32)):\n",
        "        for i, token in enumerate(o2):\n",
        "            total_tokens += 1\n",
        "\n",
        "            if o1[i] == o2[i]:\n",
        "                correct_tokens += 1\n",
        "\n",
        "            if o2[i] == eod_token:\n",
        "                break\n",
        "\n",
        "    return correct_tokens / total_tokens\n",
        "\n",
        "class DisorderGenerator:\n",
        "    def __init__(self, hidden_size, embed_size, rnn_size, vocab_size, max_generator_size):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.rnn_size = rnn_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_generator_size = max_generator_size\n",
        "\n",
        "    def build_graph(self):\n",
        "        self.ordered_disorders = tf.placeholder(tf.int32, [None, self.max_generator_size], name='ordered_disorders')\n",
        "        self.initial_states = tf.placeholder(tf.float32, [None, self.hidden_size], name='initial_states')\n",
        "\n",
        "        self.embedding = tf.Variable(tf.random_uniform([self.vocab_size, self.hidden_size], -1, 1), name='embedding')\n",
        "        self.embeded_ordered_disorders = tf.nn.embedding_lookup(self.embedding, self.ordered_disorders) # (batch_size, max_len, hidden_size)\n",
        "\n",
        "        self.W_proj = tf.Variable(tf.random_uniform([self.hidden_size, self.rnn_size]))\n",
        "        self.b_proj = tf.Variable(tf.zeros(shape=[self.rnn_size]))\n",
        "\n",
        "        self.proj_states = tf.nn.relu(tf.matmul(self.initial_states, self.W_proj) + self.b_proj, name='vocab_projection')\n",
        "\n",
        "        self.gru = tf.keras.layers.GRU(self.rnn_size,\n",
        "                                       return_sequences=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        rnn_states = self.gru(self.embeded_ordered_disorders, initial_state=self.proj_states)\n",
        "\n",
        "        self.W_vocab = tf.Variable(tf.random_uniform([rnn_size, vocab_size]))\n",
        "        self.b_vocab = tf.Variable(tf.zeros(shape=[vocab_size]))\n",
        "\n",
        "        logits = tf.nn.relu(tf.matmul(rnn_states, self.W_vocab) + self.b_vocab, name='vocab_projection')\n",
        "\n",
        "        mask = tf.cast(tf.not_equal(self.ordered_disorders[:, 1:], pad_token), tf.float32)\n",
        "\n",
        "        # transforms outputs to the required shape (batch_size, sentence_length - 1, vocabulary_size)\n",
        "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[:, :-1],\n",
        "                                                                    labels=self.ordered_disorders[:, 1:]) * mask\n",
        "\n",
        "\n",
        "        loss_per_batch_sample = tf.reduce_sum(cross_entropy, axis=1) / tf.reduce_sum(mask, axis=1)\n",
        "\n",
        "        self.loss = tf.reduce_mean(loss_per_batch_sample)\n",
        "\n",
        "    def sequential_generation_graph(self):\n",
        "        # sequential generation\n",
        "\n",
        "        sequences = [] # (batch_size, max_generator_size)\n",
        "        last_token = tf.tile(tf.Variable([bod_token], trainable=False), [tf.shape(self.proj_states)[0]])\n",
        "        last_proj_states = self.proj_states\n",
        "\n",
        "        for _ in range(self.max_generator_size - 1):\n",
        "            step_rnn_states = self.gru(tf.expand_dims(tf.nn.embedding_lookup(self.embedding, last_token), 1), \n",
        "                                initial_state=last_proj_states)\n",
        "\n",
        "            last_proj_states = tf.reshape(step_rnn_states, [-1, self.rnn_size], name='reshape_hidden')\n",
        "\n",
        "            logits = tf.nn.relu(tf.matmul(step_rnn_states, self.W_vocab) + self.b_vocab)\n",
        "            \n",
        "            # reshape to known shape\n",
        "            logits = tf.reshape(logits, [-1, self.vocab_size], name='reshape_logits')\n",
        "            last_token = tf.argmax(logits, axis=1)\n",
        "            \n",
        "            sequences.append(last_token)\n",
        "\n",
        "        self.sequences = tf.transpose(sequences)\n",
        "\n",
        "    def predict_with_sess(self, sess, data):\n",
        "        predictions = np.zeros((data.shape[0], self.max_generator_size - 1))\n",
        "\n",
        "        for rows in get_batches(list(range(data.shape[0])), batch_size=64):\n",
        "            rows_features = data[rows]\n",
        "\n",
        "            preds = sess.run(self.sequences, feed_dict={self.initial_states: rows_features})\n",
        "            predictions[rows] = preds\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict(self, data):\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "\n",
        "                self.build_graph()\n",
        "                self.sequential_generation_graph()\n",
        "\n",
        "                return self.predict_with_sess(sess, data)\n",
        "\n",
        "    def fit(self, data, data_labels, test_data, test_data_labels, n_epochs=500, print_every_epoch=10):\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "\n",
        "                self.build_graph()\n",
        "                self.sequential_generation_graph()\n",
        "\n",
        "                global_step = tf.Variable(1, name='global_step', trainable=False)\n",
        "\n",
        "                optimizer = tf.train.AdamOptimizer()\n",
        "                gvs = optimizer.compute_gradients(self.loss)\n",
        "                capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs]\n",
        "                train_step = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
        "\n",
        "                saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "\n",
        "                for epoch in range(n_epochs):\n",
        "                    total_loss = 0\n",
        "                    \n",
        "                    for rows in get_batches(list(range(data.shape[0])), batch_size=64):\n",
        "                        rows_features = data[rows]\n",
        "                        rows_predictions = data_labels[rows]\n",
        "\n",
        "                        _, loss_, step = sess.run(\n",
        "                            [train_step, self.loss, global_step],\n",
        "                            feed_dict={\n",
        "                                self.initial_states: rows_features,\n",
        "                                self.ordered_disorders: rows_predictions\n",
        "                            })\n",
        "\n",
        "                        total_loss += loss_\n",
        "\n",
        "                    if epoch % print_every_epoch == 0:\n",
        "                        print('At epoch {:4d} loss is {:8.4f}. '.format(epoch, total_loss), end='')\n",
        "                        # train\n",
        "                        predictions = self.predict_with_sess(sess, data)\n",
        "                        print('Total correct tokens train {:.4f}'.format(count_correct(train_orders[:, 1:], predictions)), end='')\n",
        "\n",
        "                        # test\n",
        "                        predictions = self.predict_with_sess(sess, test_data)\n",
        "                        print(' and test {:.4f}'.format(count_correct(test_orders[:, 1:], predictions)))\n",
        "\n",
        "                        saver.save(sess, os.path.join('gru', \"model\"), global_step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnxIt52eFZwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 15\n",
        "embed_size = 12\n",
        "rnn_size = 64\n",
        "max_generator_size = max_len_order + 2 # plus two for the bod and oed token\n",
        "\n",
        "# three extra tokens for padding and bod (beggining of diagnosis) and eod (end of diagnosis)\n",
        "vocab_size = len(most_common_disorders) + 3\n",
        "\n",
        "disorder_generator = DisorderGenerator(hidden_size, embed_size, rnn_size, vocab_size, max_generator_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcyEkQ1FfFre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "disorder_generator.fit(train_latent_space, train_orders, test_latent_space, test_orders, n_epochs=500, print_every_epoch=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcXJinCtgFrz",
        "colab_type": "text"
      },
      "source": [
        "Careful about the above metric. Looks kind of like a precision score. Look more into sequence generation metrics.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30egWoUSgFxA",
        "colab_type": "text"
      },
      "source": [
        "# TEMP (obsolete)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zha4S2xlFekf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9VudQSqFehT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNfdqX2zFpA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ordered_disorders = tf.placeholder(tf.int32, [None, max_generator_size], name='ordered_disorders')\n",
        "initial_states = tf.placeholder(tf.float32, [None, hidden_size], name='initial_states')\n",
        "\n",
        "embedding = tf.Variable(tf.random_uniform([vocab_size, hidden_size], -1, 1), name='embedding')\n",
        "embeded_ordered_disorders = tf.nn.embedding_lookup(embedding, ordered_disorders) # (batch_size, max_len, hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dNQ4hJja2Xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_proj = tf.Variable(tf.random_uniform([hidden_size, rnn_size]))\n",
        "b_proj = tf.Variable(tf.zeros(shape=[rnn_size]))\n",
        "\n",
        "proj_states = tf.nn.relu(tf.matmul(initial_states, W_proj) + b_proj, name='vocab_projection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEvkVv8PIc_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru = tf.keras.layers.GRU(rnn_size,\n",
        "                          return_sequences=True,\n",
        "                          recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVaf5bfYIdEm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70e08c3e-b8c7-4da6-cd76-5a45fb356677"
      },
      "source": [
        "rnn_states = gru(embeded_ordered_disorders, initial_state=proj_states)\n",
        "rnn_states.shape"
      ],
      "execution_count": 863,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(None), Dimension(6), Dimension(64)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 863
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll5ZOepQIdId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_vocab = tf.Variable(tf.random_uniform([rnn_size, vocab_size]))\n",
        "b_vocab = tf.Variable(tf.zeros(shape=[vocab_size]))\n",
        "\n",
        "logits = tf.nn.relu(tf.matmul(rnn_states, W_vocab) + b_vocab, name='vocab_projection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCspfJNIKfWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = tf.cast(tf.not_equal(ordered_disorders[:, 1:], pad_token), tf.float32)\n",
        "\n",
        "# transforms outputs to the required shape (batch_size, sentence_length - 1, vocabulary_size)\n",
        "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[:, :-1],\n",
        "                                                               labels=ordered_disorders[:, 1:]) * mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0vWIfanKfhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_per_batch_sample = tf.reduce_sum(cross_entropy, axis=1) / tf.reduce_sum(mask, axis=1)\n",
        "\n",
        "loss = tf.reduce_mean(loss_per_batch_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s2xUSycFoyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sequential generation\n",
        "\n",
        "sequences = [] # (batch_size, max_generator_size)\n",
        "last_token = tf.tile(tf.Variable([bod_token], trainable=False), [tf.shape(initial_states)[0]])\n",
        "last_proj_states = proj_states\n",
        "\n",
        "for _ in range(max_generator_size - 1):\n",
        "    step_rnn_states = gru(tf.expand_dims(tf.nn.embedding_lookup(embedding, last_token), 1), \n",
        "                          initial_state=last_proj_states)\n",
        "\n",
        "    last_proj_states = tf.reshape(step_rnn_states, [-1, rnn_size], name='reshape_hidden')\n",
        "\n",
        "    logits = tf.nn.relu(tf.matmul(step_rnn_states, W_vocab) + b_vocab)\n",
        "    \n",
        "    # reshape to known shape\n",
        "    logits = tf.reshape(logits, [-1, vocab_size], name='reshape_logits')\n",
        "    last_token = tf.argmax(logits, axis=1)\n",
        "    \n",
        "    sequences.append(last_token)\n",
        "\n",
        "sequences = tf.transpose(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTAzhArHKfkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "gvs = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs]\n",
        "train_step = optimizer.apply_gradients(capped_gvs, global_step=global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVH_QgecKfnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhoVHIzFFo7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.run(tf.global_variables_initializer())\n",
        "# sess.run(inputs, feed_dict={ordered_disorders: train_orders[:5]}).shape\n",
        "saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ_naQ_gFo3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "078e8492-3905-439b-aae2-8b49c215bfcc"
      },
      "source": [
        "n_epochs = 500\n",
        "print_every_epoch = 5\n",
        "\n",
        "def count_correct(out1, out2):\n",
        "    # out2 is the prediction\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "    for o1, o2 in zip(out1.astype(np.int32), out2.astype(np.int32)):\n",
        "        for i, token in enumerate(o2):\n",
        "            total_tokens += 1\n",
        "\n",
        "            if o1[i] == o2[i]:\n",
        "                correct_tokens += 1\n",
        "\n",
        "            if o2[i] == eod_token:\n",
        "                break\n",
        "\n",
        "    return correct_tokens / total_tokens\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0\n",
        "    \n",
        "    for rows in get_batches(list(range(data.shape[0])), batch_size=64):\n",
        "        rows_features = train_latent_space[rows]\n",
        "        rows_predictions = train_orders[rows]\n",
        "\n",
        "        _, loss_, step = sess.run(\n",
        "            [train_step, loss, global_step],\n",
        "            feed_dict={\n",
        "                initial_states: rows_features,\n",
        "                ordered_disorders: rows_predictions\n",
        "            })\n",
        "\n",
        "        total_loss += loss_\n",
        "\n",
        "    if epoch % print_every_epoch == 0:\n",
        "        print('At epoch {:4d} loss if {:8.4f}. '.format(epoch, total_loss), end='')\n",
        "        # train\n",
        "        predictions = np.zeros((train.shape[0], max_generator_size - 1))\n",
        "\n",
        "        for rows in get_batches(list(range(train.shape[0])), batch_size=64):\n",
        "            rows_features = train_latent_space[rows]\n",
        "\n",
        "            preds = sess.run(sequences, feed_dict={initial_states: rows_features})\n",
        "            predictions[rows] = preds\n",
        "\n",
        "        print('Total correct tokens train {:.4f}'.format(count_correct(train_orders[:, 1:], predictions)), end='')\n",
        "\n",
        "        # test\n",
        "        predictions = np.zeros((test.shape[0], max_generator_size - 1))\n",
        "\n",
        "        for rows in get_batches(list(range(test.shape[0])), batch_size=64):\n",
        "            rows_features = test_latent_space[rows]\n",
        "\n",
        "            preds = sess.run(sequences, feed_dict={initial_states: rows_features})\n",
        "            predictions[rows] = preds\n",
        "\n",
        "        print(' and test {:.4f}'.format(count_correct(test_orders[:, 1:], predictions)))\n",
        "\n",
        "        saver.save(sess, os.path.join('gru', \"model\"), global_step=epoch)"
      ],
      "execution_count": 870,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At epoch    0 loss if  19.0931. Total correct tokens train 0.6122 and test 0.5614\n",
            "At epoch    5 loss if  10.3284. Total correct tokens train 0.6366 and test 0.5959\n",
            "At epoch   10 loss if   9.6682. Total correct tokens train 0.6469 and test 0.6117\n",
            "At epoch   15 loss if   9.3193. Total correct tokens train 0.6569 and test 0.5869\n",
            "At epoch   20 loss if   8.9020. Total correct tokens train 0.6300 and test 0.5892\n",
            "At epoch   25 loss if   8.9208. Total correct tokens train 0.6658 and test 0.6299\n",
            "At epoch   30 loss if   8.8503. Total correct tokens train 0.6573 and test 0.6005\n",
            "At epoch   35 loss if   8.5652. Total correct tokens train 0.6537 and test 0.5884\n",
            "At epoch   40 loss if   8.3465. Total correct tokens train 0.6766 and test 0.6179\n",
            "At epoch   45 loss if   8.1621. Total correct tokens train 0.6687 and test 0.6128\n",
            "At epoch   50 loss if   8.1415. Total correct tokens train 0.6586 and test 0.5934\n",
            "At epoch   55 loss if   8.6767. Total correct tokens train 0.6589 and test 0.5976\n",
            "At epoch   60 loss if   8.2493. Total correct tokens train 0.6673 and test 0.6034\n",
            "At epoch   65 loss if   8.0294. Total correct tokens train 0.6701 and test 0.6170\n",
            "At epoch   70 loss if   8.1606. Total correct tokens train 0.6708 and test 0.6131\n",
            "At epoch   75 loss if   8.0367. Total correct tokens train 0.6702 and test 0.6015\n",
            "At epoch   80 loss if   7.9145. Total correct tokens train 0.6694 and test 0.6098\n",
            "At epoch   85 loss if   8.1566. Total correct tokens train 0.6754 and test 0.6163\n",
            "At epoch   90 loss if   8.0488. Total correct tokens train 0.6697 and test 0.6262\n",
            "At epoch   95 loss if   8.1611. Total correct tokens train 0.6553 and test 0.6020\n",
            "At epoch  100 loss if   8.1682. Total correct tokens train 0.6484 and test 0.6076\n",
            "At epoch  105 loss if   7.7541. Total correct tokens train 0.6800 and test 0.6186\n",
            "At epoch  110 loss if   8.1501. Total correct tokens train 0.6749 and test 0.6195\n",
            "At epoch  115 loss if   7.7493. Total correct tokens train 0.6804 and test 0.6169\n",
            "At epoch  120 loss if   7.8282. Total correct tokens train 0.6863 and test 0.6024\n",
            "At epoch  125 loss if   8.0722. Total correct tokens train 0.6715 and test 0.6130\n",
            "At epoch  130 loss if   7.9164. Total correct tokens train 0.6698 and test 0.6181\n",
            "At epoch  135 loss if   8.0158. Total correct tokens train 0.6787 and test 0.6058\n",
            "At epoch  140 loss if   7.9066. Total correct tokens train 0.6817 and test 0.6024\n",
            "At epoch  145 loss if   7.9662. Total correct tokens train 0.6875 and test 0.5912\n",
            "At epoch  150 loss if   7.9620. Total correct tokens train 0.6886 and test 0.5923\n",
            "At epoch  155 loss if   7.7605. Total correct tokens train 0.6746 and test 0.5971\n",
            "At epoch  160 loss if   7.7618. Total correct tokens train 0.6733 and test 0.5817\n",
            "At epoch  165 loss if   7.7348. Total correct tokens train 0.6772 and test 0.6047\n",
            "At epoch  170 loss if   7.7271. Total correct tokens train 0.6869 and test 0.5981\n",
            "At epoch  175 loss if   7.6429. Total correct tokens train 0.6872 and test 0.5912\n",
            "At epoch  180 loss if   7.6020. Total correct tokens train 0.6841 and test 0.5953\n",
            "At epoch  185 loss if   7.7574. Total correct tokens train 0.6706 and test 0.5857\n",
            "At epoch  190 loss if   7.9252. Total correct tokens train 0.6848 and test 0.5781\n",
            "At epoch  195 loss if   7.3450. Total correct tokens train 0.6850 and test 0.5932\n",
            "At epoch  200 loss if   7.5196. Total correct tokens train 0.6929 and test 0.6014\n",
            "At epoch  205 loss if   7.4039. Total correct tokens train 0.6914 and test 0.5981\n",
            "At epoch  210 loss if   7.8007. Total correct tokens train 0.6828 and test 0.6019\n",
            "At epoch  215 loss if   7.6670. Total correct tokens train 0.6722 and test 0.5896\n",
            "At epoch  220 loss if   7.2852. Total correct tokens train 0.6925 and test 0.5858\n",
            "At epoch  225 loss if   7.2717. Total correct tokens train 0.6977 and test 0.5714\n",
            "At epoch  230 loss if   7.5587. Total correct tokens train 0.6875 and test 0.6010\n",
            "At epoch  235 loss if   7.5731. Total correct tokens train 0.6720 and test 0.5721\n",
            "At epoch  240 loss if   7.3073. Total correct tokens train 0.6952 and test 0.5763\n",
            "At epoch  245 loss if   7.2969. Total correct tokens train 0.6984 and test 0.5957\n",
            "At epoch  250 loss if   7.1601. Total correct tokens train 0.6907 and test 0.5913\n",
            "At epoch  255 loss if   7.2491. Total correct tokens train 0.6975 and test 0.5891\n",
            "At epoch  260 loss if   7.3660. Total correct tokens train 0.6882 and test 0.5995\n",
            "At epoch  265 loss if   7.3169. Total correct tokens train 0.6724 and test 0.5611\n",
            "At epoch  270 loss if   7.1367. Total correct tokens train 0.6980 and test 0.5811\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-870-2208cf2fed37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             feed_dict={\n\u001b[1;32m     30\u001b[0m                 \u001b[0minitial_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrows_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mordered_disorders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrows_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             })\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiAhs_0JQV3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtigxYPNQV7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a1f55362-a96c-4b57-c77e-bdc0697cea7c"
      },
      "source": [
        "test_orders[:, 1:]"
      ],
      "execution_count": 762,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7, 5, 5, 5, 5],\n",
              "       [0, 2, 7, 5, 5],\n",
              "       [7, 5, 5, 5, 5],\n",
              "       ...,\n",
              "       [1, 0, 7, 5, 5],\n",
              "       [3, 7, 5, 5, 5],\n",
              "       [1, 0, 7, 5, 5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 762
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO1Edev8FeXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtWUKiIGFeTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mOFhuFpFePS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7CmyuFLFZpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}