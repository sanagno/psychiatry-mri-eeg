{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": true,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "338px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "combined_features.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "o8iz9CsV-fRj"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QgFzePe-fQu",
        "colab_type": "text"
      },
      "source": [
        "# Combine features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-z0pu1P-fQx",
        "colab_type": "text"
      },
      "source": [
        "At this stage we are going to combine the behavioural, eeg and mri data.\n",
        "\n",
        "We are then gonna train a linear model with the purpose to both reconstruct the original data based on a latent space but also based on the predictions for the multi-label problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv-wzl38-fQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 5, 10\n",
        "rcParams['font.size'] = 12\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPozvjKt-jJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "78f6f129-8b27-43a3-e6d8-3d663883ec29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_7f1DfK-fQ2",
        "colab_type": "text"
      },
      "source": [
        "## Combine features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9aMfy8w-fQ3",
        "colab_type": "text"
      },
      "source": [
        "### Load behavioural data and preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn0sedol-fQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05d210ad-1a62-461d-eb0e-c08834007eca"
      },
      "source": [
        "base_dir = '/gdrive/My Drive/Colab Notebooks/DSLab/data'\n",
        "\n",
        "behaviour_data = pd.read_csv(os.path.join(base_dir, 'HBNFinalSummaries.csv'), low_memory=False)\n",
        "\n",
        "initial_size = behaviour_data.shape[0]\n",
        "behaviour_data = behaviour_data[behaviour_data['NoDX'].isin(['Yes', 'No'])]\n",
        "new_size = behaviour_data.shape[0]\n",
        "print('Removing', initial_size - new_size,\n",
        "      'patients as their evaluation was incomplete.')\n",
        "\n",
        "keep_most_common_diseases = 5\n",
        "healthy_diagnosis = 'No Diagnosis Given'\n",
        "\n",
        "# these disorders should also include the no diagnosis given option\n",
        "keep_most_common_diseases += 1\n",
        "\n",
        "category_columns = ['DX_' + str(i).zfill(2) + '_Cat' for i in range(1, 11)]\n",
        "\n",
        "# count for each disorder number of occurences\n",
        "disorder_counts = {}\n",
        "for val in behaviour_data[category_columns].values.reshape(-1):\n",
        "    if not pd.isnull(val):\n",
        "        if val in disorder_counts:\n",
        "            disorder_counts[val] += 1\n",
        "        else:\n",
        "            disorder_counts[val] = 1\n",
        "\n",
        "# sort in descending order\n",
        "disorder_counts = sorted(disorder_counts.items(), key=lambda kv: -kv[1])\n",
        "\n",
        "most_common_disorders = [x[0]\n",
        "                         for x in disorder_counts[:keep_most_common_diseases]]\n",
        "\n",
        "# find users that have no diagnosis within these top diseases\n",
        "# filtering should cahnge anything as this should also happen at a later stage\n",
        "mask = None\n",
        "for col in category_columns:\n",
        "    mask_col = behaviour_data[col].isin(most_common_disorders)\n",
        "    if mask is None:\n",
        "        mask = mask_col\n",
        "    else:\n",
        "        mask = mask | mask_col\n",
        "\n",
        "initial_size = behaviour_data.shape[0]\n",
        "behaviour_data = behaviour_data[mask]\n",
        "behaviour_data = behaviour_data.reset_index(drop=True)\n",
        "new_size = behaviour_data.shape[0]\n",
        "print('Removing', initial_size - new_size,\n",
        "      'patients as their diagnoses were very uncommon.')"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing 282 patients as their evaluation was incomplete.\n",
            "Removing 37 patients as their diagnoses were very uncommon.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mew1LmLf-fQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_diagnosis_given = 'No Diagnosis Given'\n",
        "\n",
        "if no_diagnosis_given in most_common_disorders:\n",
        "    no_diag_index = most_common_disorders.index(no_diagnosis_given)\n",
        "    most_common_disorders = most_common_disorders[:no_diag_index] + \\\n",
        "        most_common_disorders[no_diag_index + 1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ0Jqd1O-fRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = np.zeros((len(most_common_disorders),\n",
        "                    behaviour_data.shape[0]), dtype=np.int32)\n",
        "\n",
        "\n",
        "df_disorders = behaviour_data[category_columns]\n",
        "\n",
        "for i, disorder in enumerate(most_common_disorders):\n",
        "    mask = df_disorders.select_dtypes(include=[object]). \\\n",
        "        applymap(lambda x: disorder in x if pd.notnull(x) else False)\n",
        "\n",
        "    disorder_df = df_disorders[mask.any(axis=1)]\n",
        "\n",
        "    np.add.at(classes[i], disorder_df.index.values, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS7UZNBj-fRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "behaviour_data_columns = behaviour_data.columns.values.astype(np.str)\n",
        "\n",
        "columns_to_drop = behaviour_data_columns[\n",
        "    np.flatnonzero(np.core.defchararray.find(behaviour_data_columns, 'DX') != -1)]\n",
        "\n",
        "behaviour_data = behaviour_data.drop(columns=columns_to_drop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHs-5BDa-fRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for disorder, classification in zip(most_common_disorders, classes):\n",
        "    behaviour_data[disorder] = classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVmvhBv-fRP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9f1a877-a247-4fbb-8a04-97ef5d2ccab9"
      },
      "source": [
        "behaviour_data.shape"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1777, 311)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tRsTXJq-fRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined_df = behaviour_data.set_index('EID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu5vFci--fRW",
        "colab_type": "text"
      },
      "source": [
        "### Load mri and add to dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYyM0gS6-fRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fa_per_tract = pd.read_csv('DataScience2019_MRI/MRI/DTI/FAPerTract.csv', low_memory=False)\n",
        "\n",
        "# Remove \"/\" from the end some IDs \n",
        "fa_per_tract['ID'] = fa_per_tract['ID'].apply(lambda x: x[:-1] if \"/\" in x else x)\n",
        "\n",
        "# join with behavioural data\n",
        "combined_df = combined_df.join(fa_per_tract.set_index('ID'), how='inner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSVR86ej-fRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base_dir = 'DataScience2019_MRI/MRI/structuralMRI'\n",
        "\n",
        "# column ScanSite already exists in the behavioural data\n",
        "cort_thick_l = pd.read_csv(os.path.join(base_dir,\n",
        "    'CorticalThicknessLHROI.csv'), low_memory=False).drop(columns=['ScanSite'])\n",
        "cort_thick_r = pd.read_csv(os.path.join(base_dir,\n",
        "    'CorticalThicknessRHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "cort_vol_l = pd.read_csv(os.path.join(base_dir,\n",
        "    'CorticalVolumeLHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "cort_vol_r = pd.read_csv(os.path.join(base_dir,\n",
        "    'CorticalVolumeRHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "sub_cort_vol_l = pd.read_csv(os.path.join(base_dir,\n",
        "    'SubCorticalVolumeLHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "sub_cort_vol_r = pd.read_csv(os.path.join(base_dir,\n",
        "    'SubCorticalVolumeRHROI.csv'), low_memory=False).drop(columns=['eTIV', 'ScanSite'])\n",
        "glob_thick = pd.read_csv(os.path.join(base_dir,\n",
        "    'GlobalCorticalThickness.csv'), low_memory=False).drop(columns=['ScanSite'])\n",
        "\n",
        "# Join tables \n",
        "struct_mri = pd.merge(cort_thick_l, cort_thick_r, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, cort_vol_l, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, cort_vol_r, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, sub_cort_vol_l, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, sub_cort_vol_r, on='ID', how='inner')\n",
        "struct_mri = pd.merge(struct_mri, glob_thick, on='ID', how='inner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy8hgIJZ-fRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdaf40f9-f6bb-443e-d6c2-d6df0827e55e"
      },
      "source": [
        "combined_df = combined_df.join(struct_mri.set_index('ID'), how='inner')\n",
        "combined_df.shape"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1053, 684)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8iz9CsV-fRj",
        "colab_type": "text"
      },
      "source": [
        "### Load EEG  and add to dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nApUpzud-fRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = 'DataScience2019_MRI/EEG'\n",
        "\n",
        "eeg_mic = pd.read_csv(os.path.join(base_dir, \"RestingEEG_Microstates.csv\"))\n",
        "eeg_psd = pd.read_csv(os.path.join(base_dir, \"RestingEEG_PSD_Average.csv\"))\n",
        "eeg_spectro = pd.read_csv(os.path.join(base_dir, \"RestingEEG_Spectro_Average.csv\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaRnpuIs-fRp",
        "colab_type": "code",
        "colab": {},
        "outputId": "75e44322-7684-43a6-f3a5-bcaf3ea2a87f"
      },
      "source": [
        "combined_df = combined_df.join(eeg_mic.set_index('id'), how='inner')\n",
        "combined_df = combined_df.join(eeg_psd.set_index('id'), how='inner')\n",
        "combined_df = combined_df.join(eeg_spectro.set_index('id'), how='inner')\n",
        "\n",
        "combined_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(950, 735)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsbzEqsi-fRv",
        "colab_type": "text"
      },
      "source": [
        "### Some final preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNavca52-fRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fdx and mdx may contain 'No Diagnosis'\n",
        "# drop them for now but they may be important\n",
        "# they correspond to father's and mother's primary diagnosis\n",
        "columns_to_drop = ['Anonymized.ID', 'mdx', 'fdx', 'fcodxm_1', 'fcodxm_2', 'fcodxm_3', 'mcodxm_1',\n",
        "                   'mcodxm_2', 'mcodxm_3', 'mcodxmdt', 'TOWRE_Total_Desc', 'Picture_Vocab_Raw',\n",
        "                   'sib1dx', 'sib1codxm_1', 'sib1codxm_2', 'sib1codxm_3',\n",
        "                   'sib2dx', 'sib2codxm_1', 'sib2codxm_2', 'sib2codxm_3',\n",
        "                   'sib3dx', 'sib3codxm_1', 'sib3codxm_2', 'sib3codxm_3',\n",
        "                   'sib4dx', 'sib4codxm_1', 'sib4codxm_2', 'sib4codxm_3',\n",
        "                   'sib5dx', 'sib5codxm_1', 'sib5codxm_2', 'sib5codxm_3']\n",
        "\n",
        "combined_df = combined_df.drop(columns=columns_to_drop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLNGh4D8-fR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "150ce481-b6ea-43e1-d775-b35ccf0f8cad"
      },
      "source": [
        "combined_df.shape"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1053, 652)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzSFBnK0-fR5",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl_l1rVj-5_U",
        "colab_type": "text"
      },
      "source": [
        "For this time we have only considered the behavioural data and the structural mri as these datasets have the most ids in common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8K6bYxjLF8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert combined_df.shape == (1053, 652)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga9mupwM-fR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83f31d3b-2d80-4c9b-d708-739cbcb23e39"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def mean_imputer(x, y):\n",
        "    return np.where(np.isnan(x), np.ma.array(x, mask=np.isnan(x)).mean(axis=0), x),\\\n",
        "            np.where(np.isnan(y), np.ma.array(x, mask=np.isnan(x)).mean(axis=0), y)  \n",
        "\n",
        "# we are goint to remove some behavioral data based on their null values\n",
        "drop_missing_threshold = 0.5\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=17, shuffle=True)\n",
        "kf.get_n_splits(combined_df)\n",
        "\n",
        "# preds = np.zeros(combined_df.shape[0], len(most_common_disorders))\n",
        "\n",
        "for train_index, test_index in kf.split(combined_df):\n",
        "    train, test = combined_df.iloc[train_index], combined_df.iloc[test_index]\n",
        "    # for testing purposes\n",
        "    break\n",
        "\n",
        "columns_mask = pd.isnull(train).sum() / train.shape[0] >= drop_missing_threshold\n",
        "\n",
        "print('Droping this many columns:', np.sum(columns_mask))\n",
        "\n",
        "dropped_columns = train.columns[columns_mask]\n",
        "\n",
        "train = train.drop(columns=dropped_columns)\n",
        "test = test.drop(columns=dropped_columns)\n",
        "\n",
        "# classes\n",
        "train_classes = train[most_common_disorders].values\n",
        "test_classes = test[most_common_disorders].values\n",
        "\n",
        "# keep only features\n",
        "train = train.drop(columns=most_common_disorders)\n",
        "test = test.drop(columns=most_common_disorders)\n",
        "\n",
        "train_mask = pd.isnull(train).values.astype(np.float32)\n",
        "test_mask = pd.isnull(test).values.astype(np.float32)\n",
        "\n",
        "# deal with numpy because of some weird SettingWithCopyWarning I cannot figure out\n",
        "# impute based on the mean values\n",
        "train, test = mean_imputer(train.values, test.values)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train = scaler.fit_transform(train)\n",
        "test = scaler.transform(test)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Droping this many columns: 130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2wYRkLt-fR9",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IyHKw7YDmOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "\n",
        "def get_batches(iterable, batch_size=64, do_shuffle=True):\n",
        "    if do_shuffle:\n",
        "        iterable = shuffle(iterable)\n",
        "\n",
        "    length = len(iterable)\n",
        "    for ndx in range(0, length, batch_size):\n",
        "        iterable_batch = iterable[ndx: min(ndx + batch_size, length)]\n",
        "        yield iterable_batch\n",
        "\n",
        "\n",
        "def get_reconstruction_loss(true, predictions, mask):\n",
        "    loss = np.mean(((true - predictions) ** 2) * mask, axis=1)\n",
        "    return np.mean(loss, axis=0)\n",
        "\n",
        "\n",
        "def multi_label_accuracy(true, predictions):\n",
        "    if not issubclass(predictions.dtype.type, np.integer):\n",
        "        predictions = before_softmax_to_predictions(predictions)\n",
        "\n",
        "    return 1 - np.sum((true - predictions) ** 2) / (true.shape[0] * true.shape[1])\n",
        "\n",
        "\n",
        "def before_softmax_to_predictions(predictions):\n",
        "    return (predictions >= 0).astype(np.int16)\n",
        "\n",
        "\n",
        "def f1_per_class(true, predictions):\n",
        "    if not issubclass(predictions.dtype.type, np.integer):\n",
        "        predictions = before_softmax_to_predictions(predictions)\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        f1_scores = list()\n",
        "        for i in range(true.shape[1]):\n",
        "            f1_scores.append(f1_score(true[:, i], predictions[:, i], average='macro'))\n",
        "\n",
        "    return f1_scores\n",
        "\n",
        "\n",
        "DEFAULT_LOG_PATH = './autoencoder_predict'\n",
        "\n",
        "\n",
        "class AutoencodePredict:\n",
        "    training = None\n",
        "    input_ = None\n",
        "    input_mask = None\n",
        "    intermediate_representation = None\n",
        "    input_reconstructed = None\n",
        "    reconstruction_loss = None\n",
        "    regularization_loss = None\n",
        "    prediction_loss = None\n",
        "    true_predictions = None\n",
        "    predictions = None\n",
        "    total_loss = None\n",
        "    pos_weights = None\n",
        "    class_weights = None\n",
        "\n",
        "    def __init__(self,\n",
        "                 number_of_features,\n",
        "                 num_classes,\n",
        "                 alpha=1,           # parameter showing the significance of the prediction loss\n",
        "                 activation=tf.nn.relu,\n",
        "                 layers=None,\n",
        "                 prediction_layers=None,\n",
        "                 dropout=None,\n",
        "                 regularization=0,\n",
        "                 masking=0.5):\n",
        "\n",
        "        self.activation = activation\n",
        "        self.num_classes = num_classes\n",
        "        self.alpha = alpha\n",
        "\n",
        "        if layers is None:\n",
        "            self.layers = [50, 15]\n",
        "        else:\n",
        "            self.layers = layers\n",
        "\n",
        "        if prediction_layers is None:\n",
        "            self.prediction_layers = [25, 15]\n",
        "        else:\n",
        "            self.prediction_layers = prediction_layers\n",
        "\n",
        "        self.number_of_features = number_of_features\n",
        "\n",
        "        self.masking = masking\n",
        "        self.dropout = dropout\n",
        "\n",
        "        use_regularization = (regularization > 0)\n",
        "        self.use_regularization = use_regularization\n",
        "\n",
        "        if regularization == 0:\n",
        "            # set to small value to avoid tensorflow error\n",
        "            # use_regularization = False in this case and will not contribute towards the final loss\n",
        "            self.regularization = 0.1\n",
        "        else:\n",
        "            self.regularization = regularization\n",
        "\n",
        "    def build_graph(self):\n",
        "\n",
        "        self.training = tf.placeholder(tf.bool, shape=[], name='training')\n",
        "\n",
        "        self.input_ = tf.placeholder(tf.float32, shape=[None, self.number_of_features], name='input_data')\n",
        "        self.input_mask = tf.placeholder(tf.float32, shape=[None, self.number_of_features], name='input_mask')\n",
        "        self.true_predictions = tf.placeholder(tf.float32, shape=[None, self.num_classes], name='input_predictions')\n",
        "\n",
        "        # placeholders used to balance the loss for individual classes and predictions\n",
        "        self.pos_weights = tf.placeholder(tf.float32, shape=[5], name='pos_weights')\n",
        "        self.class_weights = tf.placeholder(tf.float32, shape=[5], name='class_weights')\n",
        "\n",
        "        self.intermediate_representation = self.encode(self.input_)\n",
        "\n",
        "        self.input_reconstructed = self.decode(self.intermediate_representation)\n",
        "\n",
        "        self.predictions = self.predict_classes(self.intermediate_representation)\n",
        "\n",
        "        if self.input_mask is not None:\n",
        "            self.reconstruction_loss = tf.reduce_mean(((self.input_ - self.input_reconstructed) ** 2) * self.input_mask)\n",
        "        else:\n",
        "            self.reconstruction_loss = tf.reduce_mean((self.input_ - self.input_reconstructed) ** 2)\n",
        "\n",
        "        # self.prediction_loss = tf.reduce_mean(\n",
        "        #     tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_predictions,\n",
        "        #                                             logits=self.predictions)) * self.alpha\n",
        "        def my_loss(labels, logits, pos_weight, class_weight):\n",
        "            return tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(labels=labels,\n",
        "                                                                           logits=logits,\n",
        "                                                                           pos_weight=pos_weight)) * class_weight\n",
        "\n",
        "        loss_per_class = tf.map_fn(\n",
        "            lambda x: my_loss(x[0], x[1], x[2], x[3]),\n",
        "            (tf.transpose(self.true_predictions), tf.transpose(self.predictions), self.pos_weights, self.class_weights),\n",
        "            dtype=tf.float32)\n",
        "\n",
        "        self.prediction_loss = tf.reduce_mean(loss_per_class)\n",
        "\n",
        "        if self.use_regularization:\n",
        "            self.regularization_loss = tf.losses.get_regularization_loss()\n",
        "\n",
        "            # TODO CHANGE TO ORIGINAL\n",
        "            self.total_loss = self.reconstruction_loss + self.regularization_loss + self.prediction_loss\n",
        "            # self.total_loss = self.prediction_loss\n",
        "        else:\n",
        "            self.total_loss = self.reconstruction_loss + self.prediction_loss\n",
        "            # self.total_loss = self.prediction_loss\n",
        "\n",
        "    def predict_classes(self, intermediate):\n",
        "\n",
        "        x = intermediate\n",
        "        for i, layer in enumerate(self.prediction_layers):\n",
        "            x = tf.layers.dense(x, layer, use_bias=True, name='predict_layer_' + str(i),\n",
        "                                activation=self.activation,\n",
        "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "            if self.dropout is not None:\n",
        "                x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
        "\n",
        "        x = tf.layers.dense(x, self.num_classes, use_bias=True, name='predict_layer_final', activation=None,\n",
        "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "        return x\n",
        "\n",
        "    def encode(self,\n",
        "               input_):\n",
        "\n",
        "        if self.masking > 0:\n",
        "            # mask randomly some of the inputs\n",
        "            input_ = tf.layers.dropout(input_, rate=self.masking, training=self.training)\n",
        "\n",
        "        x = input_\n",
        "        # important to use relu as a first layer to make all unobserved values set to 0?\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = tf.layers.dense(x, layer, use_bias=True, name='input_layer_1_' + str(i),\n",
        "                                activation=self.activation,\n",
        "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "            if self.dropout is not None:\n",
        "                x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def decode(self,\n",
        "               intermediate):\n",
        "\n",
        "        x = intermediate\n",
        "        for i, layer in enumerate(self.layers[::-1][1:]):\n",
        "            x = tf.layers.dense(x, layer, use_bias=True, name='input_layer_2_' + str(i),\n",
        "                                activation=self.activation,\n",
        "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "            if self.dropout is not None:\n",
        "                x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
        "\n",
        "        x = tf.layers.dense(x, self.number_of_features, use_bias=True, name='input_layer_2_final',\n",
        "                            activation=self.activation,\n",
        "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(self.regularization))\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            x = tf.layers.dropout(x, rate=self.dropout, training=self.training)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def reconstruct(self,\n",
        "                    data,\n",
        "                    log_path=None):\n",
        "\n",
        "        if log_path is None:\n",
        "            log_path = DEFAULT_LOG_PATH\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "                self.build_graph()\n",
        "\n",
        "                saver = tf.train.Saver()\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(log_path))\n",
        "\n",
        "                data_reconstructed = np.zeros((data.shape[0], self.number_of_features))\n",
        "\n",
        "                for rows in get_batches(list(range(data.shape[0])), batch_size=64, do_shuffle=False):\n",
        "                    rows_features = [data[i, :] for i in rows]\n",
        "\n",
        "                    rows_reconstructed = sess.run(self.input_reconstructed,\n",
        "                                                  feed_dict={\n",
        "                                                      self.input_: rows_features,\n",
        "                                                      self.training: False\n",
        "                                                  })\n",
        "\n",
        "                    data_reconstructed[rows] = rows_reconstructed\n",
        "\n",
        "                return data_reconstructed\n",
        "\n",
        "    def get_latent_space(self,\n",
        "                         data,\n",
        "                         log_path=None):\n",
        "\n",
        "        if log_path is None:\n",
        "            log_path = DEFAULT_LOG_PATH\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "                self.build_graph()\n",
        "\n",
        "                saver = tf.train.Saver()\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(log_path))\n",
        "\n",
        "                data_latent = np.zeros((data.shape[0], self.layers[-1]))\n",
        "\n",
        "                for rows in get_batches(list(range(data.shape[0])), batch_size=64, do_shuffle=False):\n",
        "                    rows_features = [data[i, :] for i in rows]\n",
        "\n",
        "                    rows_latent = sess.run(self.input_reconstructed,\n",
        "                                           feed_dict={\n",
        "                                               self.input_: rows_features,\n",
        "                                               self.training: False\n",
        "                                           })\n",
        "\n",
        "                    data_latent[rows] = rows_latent\n",
        "\n",
        "                return data_latent\n",
        "\n",
        "    def predict_with_sess(self, sess, data):\n",
        "        predictions = np.zeros((data.shape[0], self.num_classes))\n",
        "\n",
        "        for rows in get_batches(list(range(data.shape[0])), batch_size=64, do_shuffle=False):\n",
        "            rows_features = [data[i, :] for i in rows]\n",
        "\n",
        "            rows_predictions = sess.run(self.predictions,\n",
        "                                        feed_dict={\n",
        "                                            self.input_: rows_features,\n",
        "                                            self.training: False\n",
        "                                        })\n",
        "\n",
        "            predictions[rows] = rows_predictions\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict(self,\n",
        "                data,\n",
        "                log_path=None,\n",
        "                make_integer=True):\n",
        "\n",
        "        if log_path is None:\n",
        "            log_path = DEFAULT_LOG_PATH\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "                self.build_graph()\n",
        "\n",
        "                saver = tf.train.Saver()\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(log_path))\n",
        "\n",
        "                if make_integer:\n",
        "                    return before_softmax_to_predictions(self.predict_with_sess(sess, data))\n",
        "                else:\n",
        "                    return self.predict_with_sess(sess, data)\n",
        "\n",
        "    def fit(self,\n",
        "            data,\n",
        "            data_mask,\n",
        "            data_labels,\n",
        "            test_data=None,\n",
        "            test_data_mask=None,\n",
        "            test_data_labels=None,\n",
        "            pos_weights=None,\n",
        "            class_weights=None,\n",
        "            n_epochs=350,\n",
        "            decay_steps=None,\n",
        "            learning_rate=None,\n",
        "            decay=None,\n",
        "            log_path=None,\n",
        "            verbose=True,\n",
        "            print_every_epochs=10):\n",
        "\n",
        "        if pos_weights is None:\n",
        "            pos_weights = [1] * self.num_classes\n",
        "\n",
        "        if class_weights is None:\n",
        "            class_weights = [1] * self.num_classes\n",
        "\n",
        "        if decay_steps is None:\n",
        "            # empirical\n",
        "            decay_steps = data.shape[0] // 64 * 5\n",
        "\n",
        "        if learning_rate is None:\n",
        "            learning_rate = 0.001\n",
        "\n",
        "        if decay is None:\n",
        "            decay = 0.96\n",
        "\n",
        "        if log_path is None:\n",
        "            log_path = DEFAULT_LOG_PATH\n",
        "\n",
        "        validation = False\n",
        "        if test_data is not None and test_data_mask is not None and test_data_labels is not None:\n",
        "            validation = True\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            with tf.Session() as sess:\n",
        "\n",
        "                self.build_graph()\n",
        "\n",
        "                global_step = tf.Variable(1, name='global_step', trainable=False)\n",
        "\n",
        "                learning_rate = tf.Variable(learning_rate, trainable=False, dtype=tf.float32, name=\"learning_rate\")\n",
        "                learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay)\n",
        "\n",
        "                # Gradients and update operation for training the model.\n",
        "                opt = tf.train.AdamOptimizer(learning_rate)\n",
        "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "                with tf.control_dependencies(update_ops):\n",
        "                    # Update all the trainable parameters\n",
        "                    train_step = opt.minimize(self.total_loss, global_step=global_step)\n",
        "\n",
        "                saver = tf.train.Saver(max_to_keep=3)\n",
        "\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "\n",
        "                for epoch in range(n_epochs):\n",
        "                    reconstruction_loss = 0\n",
        "\n",
        "                    if self.use_regularization:\n",
        "                        regularization_loss = 0\n",
        "\n",
        "                    prediction_loss = 0\n",
        "                    for rows in get_batches(list(range(data.shape[0])), batch_size=data.shape[0]):\n",
        "                        rows_features = data[rows]\n",
        "                        rows_masks = data_mask[rows]\n",
        "                        rows_predictions = data_labels[rows]\n",
        "\n",
        "                        if self.use_regularization:\n",
        "                            _, rec_loss, reg_loss, pred_loss, step = sess.run(\n",
        "                                [train_step, self.reconstruction_loss, self.regularization_loss, self.prediction_loss,\n",
        "                                 global_step],\n",
        "                                feed_dict={\n",
        "                                    self.input_: rows_features,\n",
        "                                    self.input_mask: rows_masks,\n",
        "                                    self.true_predictions: rows_predictions,\n",
        "                                    self.pos_weights: pos_weights,\n",
        "                                    self.class_weights: class_weights,\n",
        "                                    self.training: True\n",
        "                                })\n",
        "                        else:\n",
        "                            _, rec_loss, pred_loss, step = sess.run(\n",
        "                                [train_step, self.reconstruction_loss, self.prediction_loss,\n",
        "                                 global_step],\n",
        "                                feed_dict={\n",
        "                                    self.input_: rows_features,\n",
        "                                    self.input_mask: rows_masks,\n",
        "                                    self.true_predictions: rows_predictions,\n",
        "                                    self.pos_weights: pos_weights,\n",
        "                                    self.class_weights: class_weights,\n",
        "                                    self.training: True\n",
        "                                })\n",
        "\n",
        "                        reconstruction_loss += rec_loss\n",
        "\n",
        "                        if self.use_regularization:\n",
        "                            regularization_loss += reg_loss\n",
        "\n",
        "                        prediction_loss += pred_loss\n",
        "\n",
        "                    if epoch % print_every_epochs == 0:\n",
        "                        if verbose and validation:\n",
        "                            predictions_train = self.predict_with_sess(sess, data)\n",
        "                            train_accuracy = multi_label_accuracy(data_labels, predictions_train)\n",
        "\n",
        "                            predictions_test = self.predict_with_sess(sess, test_data)\n",
        "                            test_accuracy = multi_label_accuracy(test_data_labels, predictions_test)\n",
        "\n",
        "                            if self.use_regularization:\n",
        "                                print('At epoch {:4d} rec_loss: {:8.4f} reg_loss: {:8.4f} pred_loss: {:8.4f} train_'\n",
        "                                      'acc: {:.4f} test_acc {:.4f}'.format(epoch, reconstruction_loss,\n",
        "                                                                           regularization_loss, prediction_loss,\n",
        "                                                                           train_accuracy, test_accuracy))\n",
        "                            else:\n",
        "                                print('At epoch {:4d} rec_loss: {:8.4f} pred_loss: {:8.4f} train_'\n",
        "                                      'acc: {:.4f} test_acc {:.4f}'.format(epoch, reconstruction_loss,\n",
        "                                                                           prediction_loss,\n",
        "                                                                           train_accuracy, test_accuracy))\n",
        "\n",
        "                            f1_scores = f1_per_class(data_labels, predictions_train)\n",
        "                            print('train f1_scores: ', end='')\n",
        "                            for sc in f1_scores:\n",
        "                                print('{:.3f} '.format(sc), end='')\n",
        "\n",
        "                            f1_scores = f1_per_class(test_data_labels, predictions_test)\n",
        "                            print('test f1_scores: ', end='')\n",
        "                            for sc in f1_scores:\n",
        "                                print('{:.3f} '.format(sc), end='')\n",
        "                            print()\n",
        "\n",
        "                        saver.save(sess, os.path.join(log_path, \"model\"), global_step=epoch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m600XyjQB99y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f3c55ed7-98e4-48a5-fccb-578a0a2731b0"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(842, 517)\n",
            "(211, 517)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_nBWfFY-fSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha=2e-3          # parameter showing the significance of the prediction loss\n",
        "activation=tf.nn.relu\n",
        "layers=[50, 15]\n",
        "prediction_layers=[100, 25]\n",
        "dropout=0.1\n",
        "regularization=5e-5\n",
        "masking=0.15\n",
        "\n",
        "model = AutoencodePredict(train.shape[1], len(most_common_disorders), alpha=alpha, activation=activation, layers=layers,\n",
        "                          prediction_layers=prediction_layers, dropout=dropout, regularization=regularization, \n",
        "                          masking=masking)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9d5bqQg-fSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 3e-3\n",
        "# pos_weights = [1, 1, 1, 1, 1]\n",
        "# set larger weight for instances of disorders that are very infrequent\n",
        "\n",
        "def smoothing_fun(a, beta=1):\n",
        "    return [min(i, 1) for i in a] + beta * np.log([max(i - 1, 1) for i in a])\n",
        "\n",
        "pos_weights = smoothing_fun(1 / (np.sum(train_classes, axis=0) / train_classes.shape[0]))\n",
        "class_weights = [1, 1, 1, 1, 1]\n",
        "\n",
        "model.fit(train, train_mask, train_classes, test_data=test, test_data_mask=test_mask, test_data_labels=test_classes,\n",
        "          n_epochs=15000, print_every_epochs=200, pos_weights=pos_weights, class_weights=class_weights    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hnadzuPhVKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKy6DpE2-fSM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef348da6-a7d7-4c6c-860e-415443e4e5b2"
      },
      "source": [
        "preds = model.predict(test)"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./autoencoder_predict/model-16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw8bnmf2MJJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c0066731-c7d6-488d-bdaa-4b39a9b9f32a"
      },
      "source": [
        "preds"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 1, 0]], dtype=int16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4UiZ_SQMKxL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d08a52c7-8afa-405d-c861-cb84c989fe74"
      },
      "source": [
        "test_classes"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyclf4nqMPDa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ee78b75-5399-41ba-b0b7-45407e8d0d99"
      },
      "source": [
        "multi_label_accuracy(test_classes, preds)"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8454976303317535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on2_tbCpozAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "307be09a-b80f-4b85-c993-b03fa575314e"
      },
      "source": [
        "# total recall\n",
        "\n",
        "indices = np.where(test_classes == 1)\n",
        "total = len(indices[0])\n",
        "found = 0\n",
        "for i, j in zip(indices[0], indices[1]):\n",
        "    found += preds[i][j]\n",
        "\n",
        "print('Total recall:', found / total)\n",
        "print('Total precision:', found / np.sum(preds))"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total recall: 0.7437722419928826\n",
            "Total precision: 0.6966666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrvHRVAFVCCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "689af5fd-3ed1-41e6-d079-e3a89e9776e7"
      },
      "source": [
        "print(test.shape)\n",
        "print(np.sum(test_classes, axis=0))\n",
        "print(np.sum(preds, axis=0))"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(211, 517)\n",
            "[154  58  28  29  12]\n",
            "[161  63  39  32   5]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}